@ARTICLE{Aach-Church-2001,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {John Aach and George M. Church},
  DOI          = {10.1093/bioinformatics},
  JOURNALTITLE = {Bioinformatics},
  LOCALFILE    = {article/Aach-Church-2001.pdf},
  PAGES        = {495–508},
  TITLE        = {Aligning gene expression time series with time warping algorithms},
  VOLUME       = {17},
  YEAR         = {2001},
}

@ARTICLE{Aitchison-1964-tolerance,
  ABSTRACT     = {In the theory of statistical tolerance regions, as usually presented in frequentist terms, there are inherent difficulties of formulation, development and interpretation. The present paper re-examines the basic problem from a Bayesian point of view and suggests that such an approach provides a set of widely applicable, mathematically tractable tools, often more tailored to the requirements of users than the corresponding frequentist tools. For the one-dimensional case, Bayesian intervals are quoted for a number of standard distributions and prior densities, and the customary feature of a Bayesian analysis–that special prior densities give rise to standard frequentist results–is briefly demonstrated. A problem which seems to be of greater practical significance, namely the selection of an optimum tolerance region from a set of possible tolerance regions, is also investigated and the overwhelming advantages of the Bayesian approach are indicated.},
  AUTHOR       = {J. Aitchison},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Aitchison-1964-tolerance.pdf},
  NUMBER       = {2},
  PAGES        = {161–175},
  TITLE        = {Two Papers on the Comparison of Bayesian and Frequentist Approaches to Statistical Problems of Prediction: Bayesian Tolerance Regions},
  URL          = {http://links.jstor.org/stable/2984416},
  VOLUME       = {26},
  YEAR         = {1964},
}

@REPORT{VandenAkker-etal-2010,
  ABSTRACT    = {In electrical power networks nowadays more and more customers are becoming power-producers, mainly because of the development of novel components for decentral power generation (solar panels, small wind turbines and heat pumps). This gives rise to the question how many units of each type (solar panel, small wind turbine or central heating power units) can be inserted into any transmission line in the network, such that under given distributions on the typical production and consumption over time, the maximum loads on the lines and components will not be exceeded. In this paper, we present a linear programming model for maximizing the amount of decentral power generation while respecting the load limitations of the network. We describe a prototype showing that for an example network the maximization problem can be solved efficiently. We also modeled the case were the power consumption and decentral power generation are considered as stochastic variables, which is inherently more complex.},
  AUTHOR      = {Marjan van den Akker and Gabriël Bloemhof and Joost Bosman and Daan Crommelin and Jason Frank and Ghuangyuan Yang},
  INSTITUTION = {KEMA},
  LOCALFILE   = {techreport/VandenAkker-etal-2010.pdf},
  MONTH       = {6},
  TITLE       = {Optimal distributed power generation under network load constraints},
  TYPE        = {techreport},
  YEAR        = {2010},
}

@ARTICLE{Alves-Costa-2009,
  ABSTRACT     = {In this paper we propose a new method to determine the exact nadir (minimum) criterion values over the efficient set in multiple objective linear programming (MOLP). The basic idea of the method is to determine, for each criterion, the region of the weight space associated with the efficient solutions that have a value in that criterion below the minimum already known (by default, the minimum in the payoff table). If this region is empty, the nadir value has been found. Otherwise, a new efficient solution is computed using a weight vector picked from the delimited region and a new iteration is performed. The method is able to find the nadir values in MOLP problems with any number of objective functions, although the computational effort increases significantly with the number of objectives. Computational experiments are described and discussed, comparing two slightly different versions of the method.},
  AUTHOR       = {Maria João Alves and João Paulo Costa},
  DOI          = {10.1016/j.ejor.2008.10.003},
  ISSN         = {0377-2217},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {Multiple criteria analysis; Multiple objective programming; Nadir point},
  LOCALFILE    = {article/Alves-Costa-2009.pdf},
  NUMBER       = {2},
  PAGES        = {637–646},
  TITLE        = {An exact method for computing the nadir values in multiple objective linear programming},
  VOLUME       = {198},
  YEAR         = {2009},
}

@INPROCEEDINGS{Antonucci-deRosa-Giusti-2011,
  ABSTRACT  = {Hidden Markov models (HMMs) are powerful tools to capture the dynamics of a human action by providing a sufficient level of abstraction to recognise what two video sequences, depicting the same kind of action, have in common. If the sequence is short and hence only few data are available, the EM algorithm, which is generally employed to learn HMMs, might return unreliable estimates. As a possible solution to this problem, a robust version of the EM algorithm, which provides an interval-valued quantification of the HMM probabilities is provided. This takes place in an imprecise-probabilistic framework, where action recognition can be based on the (bounds of the) likelihood assigned by an imprecise HMM to the considered video sequence. Experiments show that this approach is quite effective in discriminating the hard-to-recognise sequences from the easy ones. In practice, either the recognition algorithm returns a set of action labels, which typically includes the right one, either a single answer, which is very likely to be correct, is provided.},
  AUTHOR    = {Alessandro Antonucci and Rocco de Rosa and Alessandro Giusti},
  BOOKTITLE = {IPCV 2011: Proceedings of the 2011 International Conference on Image Processing, Computer Vision and Pattern Recognition},
  PAGES     = {474–478},
  PUBLISHER = {CSREA Press},
  TITLE     = {Action Recognition by Imprecise Hidden Markov Models},
  YEAR      = {2011},
}

@INCOLLECTION{Antonucci-Salvetti-Zaffalon-2007,
  AUTHOR    = {Alessandro Antonucci and Andrea Salvetti and Marco Zaffalon},
  BOOKTITLE = {Advanced Methods for Decision Making and Risk Management in Sustainability Science},
  CHAPTER   = {10},
  EDITOR    = {Jürgen Kropp and J. Scheffran},
  ISBN      = {1-60021-427-4},
  PAGES     = {237–256},
  PUBLISHER = {Nova Publishers},
  TITLE     = {Credal Networks for Hazard Assessment of Debris Flows},
  YEAR      = {2007},
}

@ARTICLE{Antonucci-etal-2010-GL2U,
  ABSTRACT     = {Credal networks generalize Bayesian networks by relaxing the requirement of precision of probabilities. Credal networks are considerably more expressive than Bayesian networks, but this makes belief updating NP-hard even on polytrees. We develop a new efficient algorithm for approximate belief updating in credal networks. The algorithm is based on an important representation result we prove for general credal networks: that any credal network can be equivalently reformulated as a credal network with binary variables; moreover, the transformation, which is considerably more complex than in the Bayesian case, can be implemented in polynomial time. The equivalent binary credal network is then updated by L2U, a loopy approximate algorithm for binary credal networks. Overall, we generalize L2U to non-binary credal networks, obtaining a scalable algorithm for the general case, which is approximate only because of its loopy nature. The accuracy of the inferences with respect to other state-of-the-art algorithms is evaluated by extensive numerical tests.},
  AUTHOR       = {Alessandro Antonucci and Yi Sun and Cassio Polpo de Campos and Marco Zaffalon},
  DOI          = {10.1016/j.ijar.2010.01.007},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Antonucci-etal-2010-GL2U.pdf},
  NUMBER       = {5},
  PAGES        = {474–484},
  TITLE        = {Generalized loopy 2U: a new algorithm for approximate inference in credal networks},
  VOLUME       = {51},
  YEAR         = {2010},
}

@ARTICLE{Antonucci-Zaffalon-2008,
  ABSTRACT     = {Credal networks are models that extend Bayesian nets to deal with imprecision in probability, and can actually be regarded as sets of Bayesian nets. Credal nets appear to be powerful means to represent and deal with many important and challenging problems in uncertain reasoning. We give examples to show that some of these problems can only be modeled by credal nets called non-separately specified. These, however, are still missing a graphical representation language and updating algorithms. The situation is quite the opposite with separately specified credal nets, which have been the subject of much study and algorithmic development. This paper gives two major contributions. First, it delivers a new graphical language to formulate any type of credal network, both separately and non-separately specified. Second, it shows that any non-separately specified net represented with the new language can be easily transformed into an equivalent separately specified net, defined over a larger domain. This result opens up a number of new outlooks and concrete outcomes: first of all, it immediately enables the existing algorithms for separately specified credal nets to be applied to non-separately specified ones. We explore this possibility for the 2U algorithm: an algorithm for exact updating of singly connected credal nets, which is extended by our results to a class of non-separately specified models. We also consider the problem of inference on Bayesian networks, when the reason that prevents some of the variables from being observed is unknown. The problem is first reformulated in the new graphical language, and then mapped into an equivalent problem on a separately specified net. This provides a first algorithmic approach to this kind of inference, which is also proved to be NP-hard by similar transformations based on our formalism.},
  AUTHOR       = {Alessandro Antonucci and Marco Zaffalon},
  DOI          = {10.1016/j.ijar.2008.02.005},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian networks; Conservative inference rule; Conservative updating; Credal networks; Credal sets; Imprecise probabilities; Probabilistic graphical models},
  LOCALFILE    = {article/Antonucci-Zaffalon-2008.pdf},
  NUMBER       = {2},
  PAGES        = {345–361},
  TITLE        = {Decision-theoretic specification of credal networks: A unified language for uncertain modeling with sets of Bayesian networks},
  VOLUME       = {49},
  YEAR         = {2008},
}

@ARTICLE{Applegate-et-al-2007,
  ABSTRACT     = {The use of floating-point calculations limits the accuracy of solutions obtained by standard LP software. We present a simplex-based algorithm that returns exact rational solutions, taking advantage of the speed of floating-point calculations and attempting to minimize the operations performed in rational arithmetic. Extensive computational results are presented.},
  AUTHOR       = {David L. Applegate and William Cook and Sanjeeb Dash and Daniel G. Espinoza},
  DOI          = {10.1016/j.orl.2006.12.010},
  ISSN         = {0167-6377},
  JOURNALTITLE = {Operations Research Letters},
  KEYWORDS     = {Linear programming; Simplex algorithm; Rational arithmetic},
  LOCALFILE    = {article/Applegate-et-al-2007.pdf},
  NUMBER       = {6},
  PAGES        = {693–699},
  TITLE        = {Exact solutions to linear programming problems},
  VOLUME       = {35},
  YEAR         = {2007},
}

@ARTICLE{Arnold-Castillo-Sarabia-1993,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Barry C. Arnold and Enrique Castillo and Jose María Sarabia},
  DOI          = {10.1080/02331889308802432},
  JOURNALTITLE = {Statistics},
  LOCALFILE    = {article/Arnold-Castillo-Sarabia-1993.pdf},
  PAGES        = {71–77},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Conjugate exponential family priors for exponential family likelihoods},
  VOLUME       = {25},
  YEAR         = {1993},
}

@ARTICLE{Aughenbaugh-Herrmann-2009,
  ABSTRACT     = {This paper considers the problem of choosing between an existing component whose reliability is well established and a new component that has an unknown reliability. In some scenarios, the designer may have some initial beliefs about the new component's reliability. The designer may also have the opportunity to obtain more information and to update these beliefs. Then, based on these updated beliefs, the designer must make a decision between the two components. This paper examines the statistical approaches for updating reliability assessments and the decision policy that the designer uses. We consider four statistical approaches for modeling the uncertainty about the new component and updating assessments of its reliability: A classical approach, a precise Bayesian approach, a robust Bayesian approach, and an imprecise probability approach. The paper investigates the impact of different approaches on the decision between the components and compares them. In particular, given that the test results are random, the paper considers the likelihood of making a correct decision with each statistical approach under different scenarios of available information and true reliability. In this way, the emphasis is on practical comparisons of the policies rather than on philosophical arguments.},
  AUTHOR       = {J. M. Aughenbaugh and J. W. Herrmann},
  DOI          = {10.1080/15598608.2009.10411926},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bayesian statistics; Imprecise probabilities; Reliability assessment},
  LOCALFILE    = {article/Aughenbaugh-Herrmann-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {289–303},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Reliability-Based Decision Making: A Comparison of Statistical Approaches},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Augustin-2005,
  ABSTRACT     = {Dempster-Shafer theory allows to construct belief functions from (precise) basic probability assignments. The present paper extends this idea substantially. By considering sets of basic probability assignments, an appealing constructive approach to general interval probability is achieved, which allows for a very flexible modelling of uncertain knowledge.},
  AUTHOR       = {Thomas Augustin},
  DOI          = {10.1080/03081070500190839},
  JOURNALTITLE = {International Journal of General Systems},
  KEYWORDS     = {Basic probability assignment; Belief function; Dempster–Shafer theory; Imprecise probabilities; Interval probability; Linear partial information},
  LOCALFILE    = {article/Augustin-2005.pdf},
  NUMBER       = {4},
  PAGES        = {451–463},
  TITLE        = {Generalized basic probability assignments},
  VOLUME       = {34},
  YEAR         = {2005},
}

@REPORT{Augustin-2003-note,
  AUTHOR      = {Thomas Augustin},
  INSTITUTION = {LMU München},
  TITLE       = {A Note on Lower Envelopes},
  TYPE        = {techreport},
  YEAR        = {2003},
}

@THESIS{Augustin-1998-phdthesis-parts,
  ANNOTATION  = {Extracts},
  AUTHOR      = {Thomas Augustin},
  INSTITUTION = {LMU München},
  ISBN        = {978-3-52511411-7},
  TITLE       = {Optimale Tests bei Intervallwahrscheinlichkeit},
  TYPE        = {phdthesis},
  YEAR        = {1998},
}

@ARTICLE{Augustin-Coolen-2004,
  ABSTRACT     = {The assumption A(n), proposed by Hill (J. Amer. Statist. Assoc. 63 (1968) 677), provides a natural basis for low structure non-parametric predictive inference, and has been justified in the Bayesian framework. This paper embeds A(n)-based inference into the theory of interval probability, by showing that the corresponding bounds are totally monotone F-probability and coherent. Similar attractive internal consistency results are proven to hold for conditioning and updating.},
  AUTHOR       = {Thomas Augustin and Frank P. A. Coolen},
  DOI          = {10.1016/j.jspi.2003.07.003},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {A(n); Capacities; Conditioning; Consistency; Imprecise probabilities; Interval probability; Low structure inference; Non- parametrics; Predictive inference; Updating},
  LOCALFILE    = {article/Augustin-Coolen-2004.pdf},
  NUMBER       = {2},
  PAGES        = {251–272},
  TITLE        = {Nonparametric predictive inference and interval probability},
  VOLUME       = {124},
  YEAR         = {2004},
}

@ARTICLE{Aumann-1964,
  ABSTRACT     = {Theorem B of [1] is false as it stands (Section 1). It is true if the preference order is defined by a finite number of linear functions (Section 2) or alternatively, if the archimidean assumption [1, (1.2)] is replaced by either of the stronger forms [1, (4.1)] or [1, (4.2)] (Section 3). A corresponding correction must be made for Theorem C of [1] (Section 4).},
  AUTHOR       = {Robert J. Aumann},
  ISSN         = {0012-9682},
  JOURNALTITLE = {Econometrica},
  LOCALFILE    = {article/Aumann-1964.pdf},
  NUMBER       = {1/2},
  PAGES        = {210–212},
  PUBLISHER    = {The Econometric Society},
  TITLE        = {Utility Theory without the Completeness Axiom: A Correction},
  URL          = {http://www.jstor.org/stable/1913746},
  VOLUME       = {32},
  YEAR         = {1964},
}

@ARTICLE{Aumann-1962,
  ABSTRACT     = {A utility theory is developed that parallels the von Neumann-Morgenstern utility theory, but makes no use of the assumption that preferences are complete (i.e., that any two alternatives are comparable).},
  AUTHOR       = {Robert J. Aumann},
  ISSN         = {0012-9682},
  JOURNALTITLE = {Econometrica},
  LOCALFILE    = {article/Aumann-1962.pdf},
  NUMBER       = {3},
  PAGES        = {445–462},
  PUBLISHER    = {The Econometric Society},
  TITLE        = {Utility Theory without the Completeness Axiom},
  URL          = {http://www.jstor.org/stable/1909888},
  VOLUME       = {30},
  YEAR         = {1962},
}

@INBOOK{Avis-2000-lrs,
  AUTHOR    = {David Avis},
  BOOKTITLE = {Polytopes - Combinatorics and Computation},
  EDITOR    = {Gil Kalai and Günther Ziegler},
  PAGES     = {177–198},
  PUBLISHER = {Birkhäuser},
  SERIES    = {DMV Seminar},
  TITLE     = {lrs: A Revised Implementation of the Reverse Search Vertex Enumeration Algorithm},
  URL       = {http://cgm.cs.mcgill.ca/~avis/C/lrs.html},
  VOLUME    = {29},
  YEAR      = {2000},
}

@ARTICLE{Avis-Bremner-Seidel-1997,
  ABSTRACT     = {A convex polytope P can be specified in two ways: as the convex hull of the vertex set V of P, or as the intersection of the set H of its facet-inducing halfspaces. The vertex enumeration problem is to compute V from H. The facet enumeration problem is to compute H from V. These two problems are essentially equivalent under point/hyperplane duality. They are among the central computational problems in the theory of polytopes. It is open whether they can be solved in time polynomial in |H| + |V| and the dimension. In this paper we consider the main known classes of algorithms for solving these problems. We argue that they all have at least one of two weaknesses: inability to deal well with "degeneracies", or, inability to control the sizes of intermediate results. We then introduce families of polytopes that exercise those weaknesses. Roughly speaking, fat-lattice or intricate polytopes cause algorithms with bad degeneracy handling to perform badly; dwarfed polytopes cause algorithms with bad intermediate size control to perform badly. We also present computational experience with trying to solve these problem on these hard polytopes, using various implementations of the main algorithms.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {David Avis and David Bremner and Raimund Seidel},
  DOI          = {10.1016/S0925-7721(96)00023-5},
  JOURNALTITLE = {Computational Geometry},
  KEYWORDS     = {Convex hulls; Convex polytopes; Lattice complexity; Triangulation complexity; Vertex enumeration},
  LOCALFILE    = {article/Avis-Bremner-Seidel-1997.pdf},
  PAGES        = {265–301},
  TITLE        = {How good are convex hull algorithms?},
  VOLUME       = {7},
  YEAR         = {1997},
}

@ARTICLE{Avis-Fukuda-1992,
  AUTHOR       = {David Avis and Komei Fukuda},
  DOI          = {10.1007/BF02293050},
  JOURNALTITLE = {Discrete \& Computational Geometry},
  LOCALFILE    = {article/Avis-Fukuda-1992.pdf},
  NUMBER       = {1},
  PAGES        = {295–313},
  PUBLISHER    = {Springer},
  TITLE        = {A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra},
  URL          = {http://www.digizeitschriften.de/dms/img/?PPN=GDZPPN000365548},
  VOLUME       = {8},
  YEAR         = {1992},
}

@INPROCEEDINGS{Bagnara-etal-2002,
  AUTHOR    = {Roberto Bagnara and Elisa Ricci and Enea Zaffanella and Patricia M. Hill},
  BOOKTITLE = {Static Analysis: Proceedings of the 9th International Symposium},
  DOI       = {10.1007/3-540-45789-5_17},
  EDITOR    = {Manuel V. Hermenegildo and Germán Puebla},
  ISBN      = {3-540-44235-9},
  LOCALFILE = {inproceedings/Bagnara-etal-2002.pdf},
  PAGES     = {213–229},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Possibly Not Closed Convex Polyhedra and the Parma Polyhedra Library.},
  URL       = {http://bugseng.com/products/ppl/documentation/BagnaraRZH02.pdf},
  VOLUME    = {2477},
  YEAR      = {2002},
}

@ARTICLE{Baioletti-Petturiti-2011,
  ABSTRACT     = {In this paper we study the computational aspects of coherence and extension of partial possibility assessments, both in an unconditional and a conditional setting, providing complexity results and algorithms for each problem. In particular, we propose an algorithm to check the coherence of a partial unconditional assessment which is based on propositional satisfiability. For the conditional case, we firstly prove a new characterization of coherent conditional assessments that allows us to define an algorithm again based on propositional satisfiability. The extension problem, in both settings, is solved by means of a search algorithm which relies on the corresponding coherence procedure.},
  AUTHOR       = {Marco Baioletti and Davide Petturiti},
  DOI          = {10.1016/j.fss.2011.01.001},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {Possibility theory; Conditioning; Coherence; Extension; Algorithms; Complexity},
  LOCALFILE    = {article/Baioletti-Petturiti-2011.pdf},
  NUMBER       = {1},
  PAGES        = {1–25},
  TITLE        = {Algorithms for possibility assessments: Coherence and extension},
  VOLUME       = {169},
  YEAR         = {2011},
}

@BOOK{BangJensen-Gutin-2008-digraphs,
  AUTHOR    = {Jørgen Bang-Jensen and Gregory Gutin},
  EDITION   = {2},
  PUBLISHER = {Springer},
  TITLE     = {Digraphs: Theory, Algorithms and Applications},
  URL       = {http://books.google.com/books?id=4UY-ucucWucC},
  YEAR      = {2008},
}

@ARTICLE{Barabasi-Oltvai-2004,
  ABSTRACT     = {A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Albert-László Barabási and Zoltán N. Oltvai},
  DOI          = {10.1038/nrg1272},
  JOURNALTITLE = {Nature Reviews Genetics},
  LOCALFILE    = {article/Barabasi-Oltvai-2004.pdf},
  NUMBER       = {2},
  PAGES        = {101–113},
  PUBLISHER    = {Nature Publishing Group},
  TITLE        = {Network biology: understanding the cell's functional organization},
  VOLUME       = {5},
  YEAR         = {2004},
}

@BOOK{BarndorffNielsen-1978,
  ANNOTATION = {Geselecteerde delen kopies},
  AUTHOR     = {Ole Barndorff-Nielsen},
  PUBLISHER  = {Wiley},
  TITLE      = {Information and Exponential Families In Statistical Theory},
  YEAR       = {1978},
}

@ARTICLE{Baroni-Vicig-2005-interchange,
  ABSTRACT     = {This paper addresses the problem of exchanging uncertainty assessments in multi-agent systems. Since it is assumed that each agent might completely ignore the internal representation of its partners, a common interchange format is needed. We analyze the case of an interchange format defined by means of imprecise probabilities, pointing out the reasons of this choice. A core problem with the interchange format concerns transformations from imprecise probabilities into other formalisms (in particular, precise probabilities, possibilities, belief functions). We discuss this so far little investigated question, analyzing how previous proposals, mostly regarding special instances of imprecise probabilities, would fit into this problem. We then propose some general transformation procedures, which take also account of the fact that information can be partial, i.e. may concern an arbitrary (finite) set of events.},
  AUTHOR       = {Pietro Baroni and Paolo Vicig},
  DOI          = {10.1016/j.ijar.2005.03.001},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Imprecise probability theory; Multi-agent systems; Partial possibilities; Pignistic probability; Uncertainty transformations},
  LOCALFILE    = {article/Baroni-Vicig-2005-interchange.pdf},
  PAGES        = {147–180},
  TITLE        = {An uncertainty interchange format with imprecise probabilities},
  VOLUME       = {40},
  YEAR         = {2005},
}

@INPROCEEDINGS{Baroni-Vicig-2000-interchange,
  AUTHOR    = {Pietro Baroni and Paolo Vicig},
  BOOKTITLE = {Proceedings of IPMU 2000},
  PAGES     = {1027–1034},
  TITLE     = {An uncertainty interchange format for multi-agent systems based on imprecise probabilities},
  YEAR      = {2000},
}

@ARTICLE{Basu-Pereira-1983a,
  ABSTRACT     = {A Skibinsky (1970) characterization of the family of hypergeometric distributions is re-examined from the point of view of sufficient experiments and a number of other distributions similarly characterized.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {D. Basu and Carlos A. B. Pereira},
  JOURNALTITLE = {Sankhya Series A},
  LOCALFILE    = {article/Basu-Pereira-1983a.pdf},
  NUMBER       = {1},
  PAGES        = {99–104},
  PUBLISHER    = {Springer},
  TITLE        = {A Note on Blackwell Sufficiency and a Skibinsky Characterization of Distributions},
  URL          = {http://www.jstor.org/stable/25050417},
  VOLUME       = {45},
  YEAR         = {1983},
}

@ARTICLE{Basu-Pereira-1983b,
  ABSTRACT     = {The theory of conditional independence is explained and the relations between ancillarity, sufficiency and statistical independence are discussed in depth. Some related concepts like specific sufficiency, bounded completeness, and splitting sets are also studied in some details by using the language of conditional independence.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {D. Basu and Carlos A. B. Pereira},
  JOURNALTITLE = {Sankhya Series A},
  KEYWORDS     = {(strong) identification; Conditional independence; Markov property; ancillarity; measuable separability; specific sufficiency; splitting sets; sufficiency; variation independence},
  LOCALFILE    = {article/Basu-Pereira-1983b.pdf},
  NUMBER       = {3},
  PAGES        = {324–337},
  TITLE        = {Conditional independence in statistics},
  URL          = {http://www.jstor.org/stable/25050444},
  VOLUME       = {45},
  YEAR         = {1983},
}

@ARTICLE{Bellman-Zadeh-1970,
  ABSTRACT     = {By decision-making in a fuzzy environment is meant a decision process in which the goals and/or the constraints, but not necessarily the system under control, are fuzzy in nature. This means that the goals and/or the constraints constitute classes of alternatives whose boundaries are not sharply defined. An example of a fuzzy constraint is: "The cost of A should not be substantially higher than $\alpha$," where $\alpha$ is a specified constant. Similarly, an example of a fuzzy goal is: "$\chi$ should be in the vicinity of $\chi$0," where $\chi$0 is a constant. The italicized words are the sources of fuzziness in these examples. Fuzzy goals and fuzzy constraints can be defined precisely as fuzzy sets in the space of alternatives. A fuzzy decision, then, may be viewed as an intersection of the given goals and constraints. A maximizing decision is defined as a point in the space of alternatives at which the membership function of a fuzzy decision attains its maximum value. The use of these conc},
  AUTHOR       = {R. E. Bellman and L. A. Zadeh},
  DOI          = {10.1287/mnsc.17.4.B141},
  ISSN         = {0025-1909},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Bellman-Zadeh-1970.pdf},
  NUMBER       = {4},
  PAGES        = {141–164},
  PUBLISHER    = {INFORMS},
  TITLE        = {Decision-making in a fuzzy environment},
  VOLUME       = {17},
  YEAR         = {1970},
}

@ARTICLE{Benabou-Tirole-2003,
  ABSTRACT     = {A central tenet of economics is that individuals respond to incentives. For psychologists and sociologists, in contrast, rewards and punishments are often counterproductive, because they undermine "intrinsic motivation". We reconcile these two views, showing how performance incentives offered by an informed principal (manager, teacher, parent) can adversely impact an agent's (worker, child) perception of the task, or of his own abilities. Incentives are then only weak reinforcers in the short run, and negative reinforcers in the long run. We also study the effects of empowerment, help and excuses on motivation, as well as situations of ego bashing reflecting a battle for dominance within a relationship.},
  AUTHOR       = {Roland Bénabou and Jean Tirole},
  ISSN         = {00346527},
  JOURNALTITLE = {The Review of Economic Studies},
  LOCALFILE    = {article/Benabou-Tirole-2003.pdf},
  NUMBER       = {3},
  PAGES        = {489–520},
  PUBLISHER    = {Oxford University Press},
  TITLE        = {Intrinsic and Extrinsic Motivation},
  URL          = {http://www.jstor.org/stable/3648598},
  VOLUME       = {70},
  YEAR         = {2003},
}

@ARTICLE{Benaim-Hirsch-1999,
  ABSTRACT     = {Fictitious play in infinitely repeated, randomly perturbed games is investigated. Dynamical systems theory is used to study the Markov process {x\_k}, whose state vector x\_k lists the empirical frequencies of player's actions in the first k games. For 2 × 2 games with countably many Nash distribution equilibria, we prove that sample paths converge almost surely. But for Jordan's 3 × 2 matching game, there are robust parameter values giving probability 0 of convergence. Applications are made to coordination and anticoordination games and to general theory. Proofs rely on results in stochastic approximation and dynamical systems.},
  ANNOTATION   = {op papier},
  AUTHOR       = {Michael Benaïm and Morris W. Hirsch},
  DOI          = {10.1006/game.1999.0717},
  ISSN         = {0899-8256},
  JOURNALTITLE = {Games and Economic Behavior},
  MONTH        = {10},
  NUMBER       = {1-2},
  PAGES        = {36–72},
  TITLE        = {Mixed Equilibria and Dynamical Systems Arising from Fictitious Play in Perturbed Games},
  VOLUME       = {29},
  YEAR         = {1999},
}

@INPROCEEDINGS{Alessio-Zaffalon-Miranda-2009-filtering,
  ABSTRACT     = {We extend hidden Markov models for continuous variables taking into account imprecision in our knowledge about the probabilistic relationships involved. To achieve that, we consider sets of probabilities, also called coherent lower previsions. In addition to the general formulation, we study in detail a particular case of interest: linear-vacuous mixtures. We also show, in a practical case, that our extension outperforms the Kalman filter when modelling errors are present in the system.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Alessio Benavoli and Marco Zaffalon and Enrique Miranda},
  BOOKTITLE    = {FUSION 2009: Proceedings of the 12th International Conference on Information Fusion.},
  ORGANIZATION = {IEEE},
  PAGES        = {1743–1750},
  TITLE        = {Reliable hidden Markov model filtering through coherent lower previsions},
  VENUE        = {Seattle, Washington},
  YEAR         = {2009},
}

@ARTICLE{Benavoli-et-al-2010,
  ABSTRACT     = {We extend hidden Markov models for continuous variables taking into account imprecision in our knowledge about the probabilistic relationships involved. To achieve that, we consider sets of probabilities, also called coherent lower previsions. In addition to the general formulation, we study in detail a particular case of interest: linear-vacuous mixtures. We also show, in a practical case, that our extension outperforms the Kalman filter when modelling errors are present in the system.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Alessio Benavoli and Marco Zaffalon and Enrique Miranda},
  JOURNALTITLE = {IEEE Transactions on Automatic Control},
  LOCALFILE    = {article/Benavoli-et-al-2010.pdf},
  ORGANIZATION = {IEEE},
  PAGES        = {1743–1750},
  TITLE        = {A new robust approach to filtering based on coherent lower previsions},
  VANUE        = {Seattle, Washington},
  YEAR         = {2009},
}

@BOOK{BenHaim-2006-info-gap,
  AUTHOR    = {Yakov Ben-Haim},
  EDITION   = {2},
  PUBLISHER = {Academic Press},
  TITLE     = {Info-Gap Decision Theory: Decisions Under Severe Uncertainty},
  YEAR      = {2006},
}

@ARTICLE{Benson-1998,
  AUTHOR       = {Harold P. Benson},
  DOI          = {10.1023/A:1008215702611},
  ISSN         = {0925-5001},
  JOURNALTITLE = {Journal of Global Optimization},
  KEYWORDS     = {Efficient set; Global optimization; Multiple objective linear programming; Outer approximation; Vector maximization},
  LOCALFILE    = {article/Benson-1998.pdf},
  NUMBER       = {1},
  PAGES        = {1–24},
  PUBLISHER    = {Kluwer Academic Publishers},
  TITLE        = {An outer approximation algorithm for generating all efficient extreme points in the outcome set of a multiple objective linear programming problem},
  VOLUME       = {13},
  YEAR         = {1998},
}

@ARTICLE{Benson-1981,
  ABSTRACT     = {This note describes a class of linear multiple objective programs for which Isermann's method for finding an initial efficient extreme point is valid. The note also proposes a new method for generating an initial efficient point which enjoys advantages of both the Isermann method and of the modified Ecker-Kouada method.},
  AUTHOR       = {Harold P. Benson},
  ISSN         = {01605682},
  JOURNALTITLE = {The Journal of the Operational Research Society},
  LOCALFILE    = {article/Benson-1981.pdf},
  NUMBER       = {6},
  PAGES        = {pp. 495–498},
  PUBLISHER    = {Palgrave Macmillan Journals on behalf of the Operational Research Society},
  TITLE        = {Finding an initial efficient extreme point for a linear multiple objective program},
  URL          = {http://www.jstor.org/stable/2581537},
  VOLUME       = {32},
  YEAR         = {1981},
}

@ARTICLE{BenTal-Nemirovski-2002,
  ABSTRACT     = {Robust Optimization (RO) is a modeling methodology, combined with computational tools, to process optimization problems in which the data are uncertain and is only known to belong to some uncertainty set. The paper surveys the main results of RO as applied to uncertain linear, conic quadratic and semidefinite programming. For these cases, computationally tractable robust counterparts of uncertain problems are explicitly obtained, or good approximations of these counterparts are proposed, making RO a useful tool for real-world applications. We discuss some of these applications, specifically: antenna design, truss topology design and stability analysis/synthesis in uncertain dynamic systems. We also describe a case study of 90 LPs from the NETLIB collection. The study reveals that the feasibility properties of the usual solutions of real world LPs can be severely affected by small perturbations of the data and that the RO methodology can be successfully used to overcome this phenomenon.},
  AUTHOR       = {Aharon Ben-Tal and Arkadi Nemirovski},
  DOI          = {10.1007/s101070100286},
  ISSN         = {0025-5610},
  JOURNALTITLE = {Mathematical Programming},
  LOCALFILE    = {article/BenTal-Nemirovski-2002.pdf},
  NUMBER       = {3},
  PAGES        = {453–480},
  PUBLISHER    = {Springer},
  TITLE        = {Robust optimization – methodology and applications},
  VOLUME       = {92},
  YEAR         = {2002},
}

@REPORT{Berger-1993,
  ANNOTATION  = {ook op papier},
  AUTHOR      = {James Berger},
  INSTITUTION = {Purdue University, Department of Statistics},
  NUMBER      = {93-53C},
  TITLE       = {An Overview of Robust Bayesian Analysis},
  TYPE        = {techreport},
  YEAR        = {1993},
}

@ARTICLE{Berger-1994-robust-overview,
  ABSTRACT     = {Robust Bayesian analysis is the study of the sensitivity of Bayesian answers to uncertain inputs. This paper seeks to provide an overview of the subject, one that is accessible to statisticians outside the field. Recent developments in the area are also reviewed, though with very uneven emphasis.},
  AUTHOR       = {James Berger and Elías Moreno and Luis Raúl Pericchi and M. Bayarri and José M. Bernardo and Juan Cano and Julián {De la Horra} and Jacinto Martín and David Ríos-Insúa and Bruno Betrò and A. Dasgupta and Paul Gustafson and Larry Wasserman and Joseph B. Kadane and Srinivasan Cid and Michael Lavine and Anthony O'Hagan and Wolfgang Polasek and Christian Robert and Constantinos Goutis and Fabrizio Ruggeri and Gabriella Salinetti and Siva Sivaganesan},
  DOI          = {10.1007/BF02562676},
  JOURNALTITLE = {Test},
  NUMBER       = {1},
  PAGES        = {5–124},
  TITLE        = {An overview of Robust Bayesian analysis},
  VOLUME       = {3},
  YEAR         = {1994},
}

@ARTICLE{Bernard-2005,
  ABSTRACT     = {The imprecise Dirichlet model (IDM) was recently proposed by Walley as a model for objective statistical inference from multinomial data with chances θ. In the IDM, prior or posterior uncertainty about θ is described by a set of Dirichlet distributions, and inferences about events are summarized by lower and upper probabilities. The IDM avoids shortcomings of alternative objective models, either frequentist or Bayesian. We review the properties of the model, for both parametric and predictive inferences, and some of its recent applications to various statistical problems.},
  AUTHOR       = {Jean-Marc Bernard},
  DOI          = {10.1016/j.ijar.2004.10.002},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian inference; Dirichlet distribution; Frequentist inference; IDM; Lower and upper probabilities; Predictive inference; Prior ignorance},
  LOCALFILE    = {article/Bernard-2005.pdf},
  PAGES        = {123–150},
  TITLE        = {An introduction to the imprecise Dirichlet model for multinomial data},
  VOLUME       = {39},
  YEAR         = {2005},
}

@MISC{Bernard-2003,
  ANNOTATION = {Tutorial for ISIPTA '03},
  AUTHOR     = {Jean-Marc Bernard},
  TITLE      = {An Introduction to the Imprecise Dirichlet Model for Multinomial Data},
  YEAR       = {2003},
}

@ARTICLE{Bernard-1997-specificity,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Jean-Marc Bernard},
  JOURNALTITLE = {Revue Internationale de Systémique},
  LOCALFILE    = {article/Bernard-1997-specificity.pdf},
  NUMBER       = {1},
  PAGES        = {11–29},
  TITLE        = {Bayesian analysis of tree-structured data},
  VOLUME       = {11},
  YEAR         = {1997},
}

@BOOK{Bernardo-Smith-1994,
  AUTHOR    = {José M. Bernardo and Adrian F. M. Smith},
  PUBLISHER = {Wiley},
  SERIES    = {Wiley Series in Probability and Mathematical Statistics},
  TITLE     = {Bayesian theory},
  YEAR      = {1994},
}

@INPROCEEDINGS{Bertens-VanderGaag-Renooij-2012,
  ABSTRACT  = {Naive Bayesian networks are often used for classification problems that involve variables of a continuous nature. Upon capturing such variables, their value ranges are modelled as finite sets of discrete values. While the output probabilities and conclusions established from a Bayesian network are dependent of the actual discretisations used for its variables, the effects of choosing alternative discretisations are largely unknown as yet. In this paper, we study the effects of changing discretisations on the probability distributions computed from a naive Bayesian network. We demonstrate how recent insights from the research area of sensitivity analysis can be exploited for this purpose.},
  AUTHOR    = {Roel Bertens and Linda C. van der Gaag and Silja Renooij},
  BOOKTITLE = {Advances in Computational Intelligence},
  DOI       = {10.1007/978-3-642-31718-7_17},
  EDITOR    = {Salvatore Greco and Bernadette Bouchon-Meunier and Giulianella Coletti and Mario Fedrizzi and Benedetto Matarazzo and Ronald R. Yager},
  ISBN      = {978-3-642-31718-7},
  PAGES     = {161–170},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Communications in Computer and Information Science},
  TITLE     = {Discretisation effects in naive Bayesian networks},
  VOLUME    = {299},
  YEAR      = {2012},
}

@BOOK{Bertsimas-Tsitsiklis-1997,
  AUTHOR    = {Dimitris Bertsimas and John N. Tsitsiklis},
  ISBN      = {1-886529-19-1},
  KEYWORDS  = {Linear programming; integer programming; mathematical optimization},
  PUBLISHER = {Athena Scientific},
  TITLE     = {Introduction to linear optimization},
  URL       = {http://athenasc.com/linoptbook.html},
  YEAR      = {1997},
}

@ARTICLE{2000-Biazzo+Gilio-IJAR,
  ABSTRACT     = {In this paper, we consider coherent imprecise probability assessments on finite families of conditional events and we study the problem of their extension. With this aim, we adopt a generalized definition of coherence, called g-coherence, which is based on a suitable generalization of the coherence principle of de Finetti. At first, we recall some theoretical results and an algorithm obtained in some previous papers where the case of precise conditional probability assessments has been studied. Then, we extend these results to the case of imprecise probabilistic assessments and we obtain a theorem which can be looked at as a generalization of the version of the fundamental theorem of de Finetti given by some authors for the case of conditional events. Our algorithm can also be exploited to produce lower and upper probabilities which are coherent in the sense of Walley and Williams. Moreover, we compare our approach to similar ones, like probability logic or probabilistic deduction. Finally, we apply our algorithm to some well-known inference rules assuming some logical relations among the given events.},
  AUTHOR       = {Veronica Biazzo and Angelo Gilio},
  DOI          = {10.1016/S0888-613X(00)00038-4},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Conditional events; Imprecise probabilities; Coherence; Generalized coherence; Natural extension; Extensions; Algorithms; Probability logic; Probabilistic deduction; Probabilistic satisfiability},
  LOCALFILE    = {article/2000-Biazzo+Gilio-IJAR.pdf},
  NUMBER       = {2–3},
  PAGES        = {251–272},
  TITLE        = {A generalization of the fundamental theorem of de Finetti for imprecise conditional probability assessments},
  VOLUME       = {24},
  YEAR         = {2000},
}

@INPROCEEDINGS{Biazzo-Gilio-Sanfilippo-2008,
  ABSTRACT  = {In this paper we consider imprecise conditional prevision assessments on random quantities with finite set of possible values. We use a notion of generalized coherence which is based on the coherence principle of de Finetti. We consider the checking of g-coherence, by extending some previous results obtained for imprecise conditional probability assessments. Then, we study a connection property of interval-valued gcoherent prevision assessments, by extending a result given in a previous paper for precise assessments.},
  AUTHOR    = {Veronica Biazzo and Angelo Gilio and Giuseppe Sanfilippo},
  BOOKTITLE = {Proceedings of the IPMU '08},
  PAGES     = {907–914},
  TITLE     = {Generalized coherence and connection property of imprecise conditional previsions},
  URL       = {http://www.gimac.uma.es/ipmu08/proceedings/html/120.html},
  VOLUME    = {8},
  YEAR      = {2008},
}

@ARTICLE{Bickis-2009,
  ABSTRACT     = {Given data on inter-arrival times, the imprecise Dirichlet model can be used to determine upper and lower values on the survival function. Similar bounds on the hazard function can be quite irregular without some structural assumptions. To address this problem, a family of prior distributions for a binomial success probability is contructed by assuming that the logit of the probability has a normal distribution. Posterior distributions so defined form a three-dimensional exponential family of which the beta family is a limiting case. This family is extended to the multivariate case, which provides for the inclusion of prior information about autocorrelation in the parameters. By restricting the hyperparameters to a suitably chosen subset, this model is proposed as an alternative to the usual imprecise Dirichlet model of Walley, having the advantage of providing smoother estimates of the hazard function. The methods are applied to data on inter-occurrence times of pandemic influenza.},
  ANNOTATION   = {doi: 10.1080/15598608.2009.10411919},
  AUTHOR       = {Miķelis Bickis},
  DOI          = {10.1080/15598608.2009.10411919},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Autocorrelation; Hazard function; Imprecise inference},
  LOCALFILE    = {article/Bickis-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {183–195},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {The Imprecise Logit-Normal Model and its Application to Estimating Hazard Functions},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Bildikar-Patil-1968,
  ABSTRACT     = {Let \mathbf{x} and \mathbf{θ} denote s-dimensional column vectors. The components x\_1, x\_2,⋯ x\_s of \mathbf{x} are random variables jointly following an s-variate distribution and components θ\_1, θ\_2,⋯, θ\_s of \mathbf{θ} are real numbers. The random vector \mathbf{x} is said to follow an s-variate Exponential-type distribution with the parameter vector (pv) \mathbf{θ}, if its probability function (pf) is given by \begin{equation*}\tag{1.1} f(\mathbf{x}, \mathbf{θ}) = h(\mathbf{x}) \exp {\mathbf{x'θ} - q(\mathbf{θ})},\end{equation*} \mathbf{x} \varepsilon R\_s and \mathbf{θ} \varepsilon (\mathbf{a}, \mathbf{b}) \subset R\_s. R\_s denotes the s-dimensional Euclidean space. The s-dimensional open interval (\mathbf{a}, \mathbf{b}) may or may not be finite. h(\mathbf{x}) is a function of \mathbf{x}, independent of \mathbf{θ}, and q(\mathbf{θ}) is a bounded analytic function of θ\_1, θ\_2,⋯ θ\_s, independent of \mathbf{x}. We note that f(\mathbf{x}, \mathbf{θ}), given by (1.1), defines the class of multivariate exponential-type distributions which includes distributions like multivariate normal, multinomial, multivariate negative binomial, multivariate logarithmic series, etc. This paper presents a theoretical study of the structural properties of the class of multivariate exponential-type distributions. For example, different distributions connected with a multivariate exponential-type distribution are derived. Statistical independence of the components x\_1, x\_2,⋯, x\_s is discussed. The problem of characterization of different distributions in the class is studied under suitable restrictions on the cumulants. A canonical representation of the characteristic function of an infinitely divisible (id), purely discrete random vector, whose moments of second order are all finite, is also obtained. $\phi$(\mathbf{t}), m(\mathbf{t}), k(\mathbf{t}) denote, throughout this paper, the characteristic function (ch. f.), the moment generating function (mgf), and the cumulant generating function (cgf), respectively, of a random vector \mathbf{x}. The components t\_i of the s-dimensional column vector \mathbf{t} are all real.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Sheela Bildikar and G. P. Patil},
  DOI          = {10.1214/aoms/1177698257},
  JOURNALTITLE = {The Annals of Mathematical Statistics},
  LOCALFILE    = {article/Bildikar-Patil-1968.pdf},
  NUMBER       = {4},
  PAGES        = {1316–1326},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Multivariate exponential-type distributions},
  VOLUME       = {39},
  YEAR         = {1968},
}

@ARTICLE{Billingsley-1961,
  ABSTRACT     = {This paper is an expository survey of the mathematical aspects of statistical inference as it applies to finite Markov chains, the problem being to draw inferences about the transition probabilities from one long, unbroken observation {x\_1, x\_2, ⋯, x\_n} on the chain. The topics covered include Whittle's formula, chi-square and maximum-likelihood methods, estimation of parameters, and multiple Markov chains. At the end of the paper it is briefly indicated how these methods can be applied to a process with an arbitrary state space or a continuous time parameter. Section 2 contains a simple proof of Whittle's formula; Section 3 provides an elementary and self-contained development of the limit theory required for the application of chi-square methods to finite chains. In the remainder of the paper, the results are accompanied by references to the literature, rather than by complete proofs. As is usual in a review paper, the emphasis reflects the author's interests. Other general accounts of statistical inference on Markov processes will be found in Grenander [53], Bartlett [9] and [10], Fortet [35], and in my monograph [18]. I would like to thank Paul Meier for a number of very helpful discussions on the topics treated in this paper, particularly those of Section 3.},
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Patrick Billingsley},
  JOURNALTITLE = {The Annals of Mathematical Statistics},
  LOCALFILE    = {article/Billingsley-1961.pdf},
  NUMBER       = {1},
  PAGES        = {12–40},
  TITLE        = {Statistical methods in Markov chains},
  URL          = {http://www.jstor.org/stable/2237603},
  VOLUME       = {32},
  YEAR         = {1961},
}

@INPROCEEDINGS{Blanco-etal-2004,
  AUTHOR    = {Rosa Blanco and Linda C. van der Gaag and Iñaki Inza and Pedro Larrañaga},
  BOOKTITLE = {ISBMDA 2004: Proceedings of the 5th International Symposium on Biological and Medical Data Analysis},
  EDITOR    = {José María Barreiro and Fernando Martín-Sánchez and Victor Maojo and Ferran Sanz},
  ISBN      = {3-540-23964-2},
  PAGES     = {212–223},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Selective classifiers can be too restrictive: a case-study in oesophageal cancer},
  VOLUME    = {3337},
  YEAR      = {2004},
}

@ARTICLE{Bloch-Watson-1967,
  ABSTRACT     = {Lindley [6] studies the topic in our title. By using Fisher's conditional-Poisson approach to the multinomial and the logarithmic transformation of gamma variables to normality, he showed that linear contrasts in the logarithms of the cell probabilities θ\_i are asymptotically jointly normal and suggested that the approximation can be improved by applying a "correction" to the sample. By studying the asymptotic series for the joint distribution in Section 2 an improved correction procedure is found below. A more detailed expansion is given in Section 3 for the distribution of a single contrast in the \log θ\_i. In many problems a linear function of the θ\_i is of interest. The exact distribution is obtained and is of a form familiar in the theory of serial correlation coefficients. A beta approximation is given. For three cells, a numerical example is given to show the merit of this approximation. A genetic linkage example is considered which requires the joint distribution of two linear functions of the θ\_i. The exact joint distribution is found but is too involved for practical use. A normal approximation leads to Lindley's results [7].},
  AUTHOR       = {Daniel A. Bloch and Geoffrey S. Watson},
  JOURNALTITLE = {The Annals of Mathematical Statistics},
  LOCALFILE    = {article/Bloch-Watson-1967.pdf},
  NUMBER       = {5},
  PAGES        = {1423–1435},
  TITLE        = {A Bayesian study of the multinomial distribution},
  URL          = {http://www.jstor.org/stable/2238958},
  VOLUME       = {38},
  YEAR         = {1967},
}

@INCOLLECTION{Bolt+Van_der_Gaag-2010,
  AUTHOR    = {Janneke H. Bolt and Linda C. van der Gaag},
  BOOKTITLE = {Information Processing and Management of Uncertainty in Knowledge-Based Systems. Theory and Methods},
  DOI       = {10.1007/978-3-642-14055-6_2},
  EDITOR    = {Eyke Hüllermeier and Rudolf Kruse and Frank Hoffmann},
  ISBN      = {978-3-642-14054-9},
  LOCALFILE = {inproceedings/Bolt+Van_der_Gaag-2010.pdf},
  PAGES     = {11–20},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Communications in Computer and Information Science},
  TITLE     = {An empirical study of the use of the Noisy-OR model in a real-life Bayesian network},
  VOLUME    = {80},
  YEAR      = {2010},
}

@INCOLLECTION{Bolt+Van_der_Gaag-2007,
  AUTHOR    = {Janneke H. Bolt and Linda C. van der Gaag},
  BOOKTITLE = {Advances in Probabilistic Graphical Models},
  DOI       = {10.1007/978-3-540-68996-6_7},
  EDITOR    = {Peter Lucas and José A. Gámez and Antonio Salmerón},
  ISBN      = {978-3-540-68994-2},
  LOCALFILE = {incollection/Bolt+Van_der_Gaag-2007.pdf},
  PAGES     = {153–173},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Studies in Fuzziness and Soft Computing},
  TITLE     = {Decisiveness in loopy propagation},
  VOLUME    = {214},
  YEAR      = {2007},
}

@ARTICLE{Bolt-VanderGaag-Renooij-2005,
  ABSTRACT     = {A qualitative probabilistic network is a graphical model of the probabilistic influences among a set of statistical variables, in which each influence is associated with a qualitative sign. A non-monotonic influence between two variables is associated with the ambiguous sign ‘?’, which indicates that the actual sign of the influence depends on the state of the network. The presence of such ambiguous signs is undesirable as it tends to lead to uninformative results upon inference. In this paper, we argue that, although a non-monotonic influence may have varying effects, in each specific state of the network, its effect is unambiguous. To capture the current effect of the influence, we introduce the concept of situational sign. We show how situational signs can be used upon inference and how they are updated as the state of the network changes. By means of a real-life qualitative network in oncology, we show that the use of situational signs can effectively forestall uninformative results upon inference.},
  AUTHOR       = {Janneke H. Bolt and Linda C. van der Gaag and Silja Renooij},
  DOI          = {10.1016/j.ijar.2004.05.009},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  NUMBER       = {3},
  PAGES        = {333–354},
  TITLE        = {Introducing situational signs in qualitative probabilistic networks},
  VOLUME       = {38},
  YEAR         = {2005},
}

@BOOK{Boole-1854,
  AUTHOR    = {George Boole},
  PUBLISHER = {Macmillan},
  TITLE     = {The laws of thought},
  URL       = {http://www.gutenberg.org/ebooks/15114},
  YEAR      = {1854},
}

@UNPUBLISHED{DeBoor-1995-BBForm,
  AUTHOR   = {C. de Boor},
  KEYWORDS = {Bernstein polynomials},
  TITLE    = {B-form basics},
  YEAR     = {1995},
}

@ARTICLE{Boratynska-1997,
  ABSTRACT     = {The problem of estimating the unknown parameter of a one-parameter exponential family with the conjugate prior is considered. Some uncertainty about the prior is assumed by introducing a class of priors Gamma. The most robust and conditional Gamma-minimax estimators are constructed. The situations when those estimators coincide are presented. The paper is a generalization of the result for the Poisson distribution obtained in Mezarski and Zielinski (1991).},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Agata Boratyńska},
  DOI          = {10.1016/S0167-7152(97)00060-6},
  JOURNALTITLE = {Statistics \& Probability Letters},
  KEYWORDS     = {Bayes estimators; classes of priors; one-parameter exponential family; robust Bayesian estimation},
  LOCALFILE    = {article/Boratynska-1997.pdf},
  NUMBER       = {2},
  PAGES        = {173–178},
  PUBLISHER    = {Elsevier},
  TITLE        = {Stability of Bayesian inference in exponential families},
  VOLUME       = {36},
  YEAR         = {1997},
}

@ARTICLE{Bose-2009-imposition,
  ABSTRACT     = {We consider the problem of imposing shape constraints on a neighborhood class – the density ratio class (DeRobertis and Hartigan, 1981). Bose (1994) used mixture distributions to impose shape and smoothness constraints simultaneously. We discuss how one may impose either or both unimodality and symmetry without requiring simultaneous imposition of a smoothness constraint.},
  AUTHOR       = {Sudip Bose},
  DOI          = {10.1080/15598608.2009.10411910},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bayesian robustness; Convexity; Density bounded; Density ratio; Likelihood; Minimax; Neighborhood class; Posterior regret; Smoothness; Symmetry; Unimodality; $\Gamma$-minimax},
  LOCALFILE    = {article/Bose-2009-imposition.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {39–55},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {On the imposition of shape constraints in a robust Bayesian analysis},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Bose-2009-smoothness,
  ABSTRACT     = {We examine the role of the likelihood in Bayesian robustness with the density ratio class (DeRobertis and Hartigan, 1981. Ann. Stat.) and show how to impose smoothness on the density ratio class after imposing shape constraints. We discuss how to impose shape constraints on the density bounded class (Lavine, 1991. JASA)},
  ANNOTATION   = {doi: 10.1080/15598608.2009.10411911},
  AUTHOR       = {Sudip Bose},
  DOI          = {10.1080/15598608.2009.10411911},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bayesian robustness; Density bounded; Density ratio; Likelihood; Neighborhood class; Smoothness; Symmetry; Unimodality; Convexity; Minimax; Posterior regret; $\Gamma$-minimax},
  LOCALFILE    = {article/Bose-2009-smoothness.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {57–67},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {On smoothness constraints with shape constraints in a robust Bayesian analysis},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Bot-Lorenz-Wanka-2010,
  AUTHOR       = {Radu Ioan Bot and Nicole Lorenz and Gert Wanka},
  DOI          = {10.4134/JKMS.2010.47.1.017},
  JOURNALTITLE = {Journal of The Korean Mathematical Society},
  LOCALFILE    = {article/Bot-Lorenz-Wanka-2010.pdf},
  PAGES        = {17–28},
  TITLE        = {Duality for linear chance-constrained optimization problems},
  VOLUME       = {47},
  YEAR         = {2010},
}

@INPROCEEDINGS{Bouckaert-2004,
  AUTHOR    = {Remco R. Bouckaert},
  BOOKTITLE = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
  EDITOR    = {Geoffrey I. Webb and Xinghuo Yu},
  PAGES     = {1089–1094},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in AI},
  TITLE     = {Naive Bayes Classifiers that Perform Well with Continuous Variables},
  YEAR      = {2004},
}

@ARTICLE{Boute-2005,
  AUTHOR       = {Raymond T. Boute},
  DOI          = {10.1145/1086642.1086647},
  ISSN         = {0164-0925},
  JOURNALTITLE = {ACM Transactions on Programming Languages and Systems},
  LOCALFILE    = {article/Boute-2005.pdf},
  MONTH        = {09},
  NUMBER       = {5},
  PAGES        = {988–1047},
  TITLE        = {Functional declarative language design and predicate calculus},
  URL          = {http://portal.acm.org/citation.cfm?doid=1086642.1086647},
  VOLUME       = {27},
  YEAR         = {2005},
}

@INBOOK{Boute-2004,
  ANNOTATION = {Op papier},
  AUTHOR     = {Raymond T. Boute},
  BOOKTITLE  = {Information Technology},
  PAGES      = {85–114},
  PUBLISHER  = {Kluwer Academic Publishers},
  TITLE      = {Formal Reasoning about Systems, Software and Hardware using Functionals, Predicates and Relations},
  YEAR       = {2004},
}

@BOOK{Boyd-Vandenberghe-2004,
  AUTHOR    = {Stephen Boyd and Lieven Vandenberghe},
  LOCALFILE = {book/Boyd-Vandenberghe-2004.pdf},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Convex Optimization},
  URL       = {http://www.stanford.edu/~boyd/cvxbook},
  YEAR      = {2004},
}

@INCOLLECTION{Brams-Fishburn-1991-altvote,
  AUTHOR    = {Steven J. Brams and Peter C. Fishburn},
  BOOKTITLE = {Political Pareties and Elections in the United States: An Encyclopedia},
  EDITOR    = {Sandy L Maisel},
  PAGES     = {23–31},
  PUBLISHER = {garland},
  TITLE     = {Alternative voting systems},
  URL       = {http://bcn.boulder.co.us/government/approvalvote/altvote.html},
  VOLUME    = {1},
  YEAR      = {1991},
}

@ARTICLE{1998-Bremner++-pd,
  AUTHOR       = {David Bremner and Komei Fukuda and Ambros Marzetta},
  DOI          = {10.1007/PL00009389},
  JOURNALTITLE = {Discrete \& Computational Geometry},
  LOCALFILE    = {article/1998-Bremner++-pd.pdf},
  PAGES        = {333–357},
  TITLE        = {Primal-Dual Methods for Vertex and Facet Enumeration},
  URL          = {http://www.cs.unb.ca/profs/bremner/pd},
  VOLUME       = {20},
  YEAR         = {1998},
}

@BOOK{Brokken-2006,
  AUTHOR    = {Frank B. Brokken},
  MONTH     = {09},
  PUBLISHER = {University of Groningen},
  TITLE     = {C++ Annotations},
  YEAR      = {2006},
}

@INPROCEEDINGS{Brown-1951,
  AUTHOR       = {George W. Brown},
  BOOKTITLE    = {Activity analysis of production and allocation},
  EDITOR       = {Tjalling C. Koopmans},
  NUMBER       = {13},
  ORGANIZATION = {Cowles Commission for Research in Economics},
  PAGES        = {374–376},
  SERIES       = {Cowles Commission Monographs},
  TITLE        = {Iterative solution of games by fictitious play},
  YEAR         = {1951},
}

@BOOK{Brown-1986,
  ANNOTATION = {Geselecteerde delen kopies},
  AUTHOR     = {Lawrence D. Brown},
  EDITOR     = {Shanti S. Gupta},
  LOCATION   = {Hayward, California},
  PUBLISHER  = {Institute of Mathematical Statistics},
  SERIES     = {Institute of Mathematical Statistics: Lecture Notes—Monograph Series},
  TITLE      = {Fundamentals of Statistical Exponential Families (with Applications in Statistical Decision Theory)},
  VOLUME     = {9},
  YEAR       = {1986},
}

@ARTICLE{Bruening-Dennenberg-2008-belELP,
  ABSTRACT     = {It is known that the $\sigma$-additive Möbius transform of a belief function (we prefer to call it belief measure) can be derived from Choquet's Theorem. One has to show that the extreme points of the compact convex set of belief measures are the {0,1}-valued belief measures, which are called filter games as well. A proof is implicit in the famous 1953/54 paper of Choquet but it is hard to read it. We present a direct proof and – for the sake of completeness – derive the Möbius transform.},
  ANNOTATION   = {explicitation of an implicit result of Choquet-1954},
  AUTHOR       = {Martin Brüning and Dieter Denneberg},
  DOI          = {10.1016/j.ijar.2006.11.003},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Bruening-Dennenberg-2007-belELP.pdf},
  NUMBER       = {3},
  PAGES        = {670–675},
  PUBLISHER    = {Elsevier},
  TITLE        = {The extreme points of the set of belief measures},
  VOLUME       = {48},
  YEAR         = {2008},
}

@REPORT{Bruening-Dennenberg-2003-belELP,
  ANNOTATION  = {Direct proof (of Choquet's implicit proof) that {0,1}-valued belief measures are the extreme points of the set of belief measures.},
  AUTHOR      = {Martin Brüning and Dieter Denneberg},
  INSTITUTION = {Universität Bremen},
  TITLE       = {The $\sigma$-additive Möbius Transform of Belief Measures via Choquet's Theorem},
  TYPE        = {techreport},
  YEAR        = {2003},
}

@ARTICLE{Bryant-Webster-1977-cvxIII,
  AUTHOR       = {V. W. Bryant and R. J. Webster},
  DOI          = {10.1016/0022-247X(77)90267-0},
  ISSN         = {0022-247X},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  LOCALFILE    = {article/Bryant-Webster-1972-cvxIII.pdf},
  NUMBER       = {2},
  PAGES        = {382–392},
  TITLE        = {Convexity spaces. III. Dimension},
  VOLUME       = {57},
  YEAR         = {1977},
}

@ARTICLE{Bryant-Webster-1972-cvxII,
  ABSTRACT     = {This is the second of a series of three papers dealing with convexity spaces. In the first paper [1] we defined a convexity space and investigated some of its basic properties. Here we consider the separation and support of convex sets. Throughout the paper we will be dealing with a convexity space (X, ·) and the terminology and notation used will be those of [1]. In particular Ac denotes the complement of the set A in X and \ is used to denote set-theoretic difference.},
  AUTHOR       = {V. W. Bryant and R. J. Webster},
  DOI          = {10.1016/0022-247X(73)90076-0},
  ISSN         = {0022-247X},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  LOCALFILE    = {article/Bryant-Webster-1972-cvxII.pdf},
  NUMBER       = {2},
  PAGES        = {321–327},
  TITLE        = {Convexity spaces. II. Separation},
  VOLUME       = {43},
  YEAR         = {1973},
}

@ARTICLE{Bryant-Webster-1972-cvxI,
  AUTHOR       = {V. W. Bryant and R. J. Webster},
  DOI          = {10.1016/0022-247X(72)90268-5},
  ISSN         = {0022-247X},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  LOCALFILE    = {article/Bryant-Webster-1972-cvxI.pdf},
  NUMBER       = {1},
  PAGES        = {206–213},
  TITLE        = {Convexity spaces. I. The basic properties},
  VOLUME       = {37},
  YEAR         = {1972},
}

@ARTICLE{Buckley-1995,
  ABSTRACT     = {We propose a new solution concept for fuzzy programming problems. It is based on our new method of solving fuzzy equations [10]. For simplicity we discuss in detail only fuzzy linear programming in this paper. We define, and obtain the basic properties of the joint solution (a fuzzy vector in R^n) and the optimal value of the objective function (a fuzzy number). Three examples are presented illustrating these concepts.},
  AUTHOR       = {J. J. Buckley},
  DOI          = {10.1016/0165-0114(94)00353-9},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {mathematical programming},
  LOCALFILE    = {article/Buckley-1995.pdf},
  NUMBER       = {2},
  PAGES        = {215–220},
  TITLE        = {Joint solution to fuzzy programming problems},
  VOLUME       = {72},
  YEAR         = {1995},
}

@ARTICLE{Buckley-Qu-1991,
  AUTHOR       = {J. J. Buckley and Y. Qu},
  DOI          = {10.1016/0165-0114(91)90019-M},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {algebra; fuzzy number},
  LOCALFILE    = {article/Buckley-Qu-1991.pdf},
  NUMBER       = {1},
  PAGES        = {33–43},
  PUBLISHER    = {Elsevier},
  TITLE        = {Solving systems of linear fuzzy equations},
  VOLUME       = {43},
  YEAR         = {1991},
}

@ARTICLE{Buehler-1976,
  ABSTRACT     = {De Finetti has defined coherent previsions and coherent probabilities, and others have described concepts of coherent actions or coherent decisions. Here we consider a related concept of coherent preferences. Willingness to accept one side of a bet is an example of a preference. A set of preferences is called incoherent if reversal of some subset yields a uniform increase in utility, as with a sure win for a collection of bets. In both probability and statistical models (where preferences are conditional on data) separating hyperplane theorems show that coherence implies existence of a probability measure from which the preferences could have been inferred. Relationships to confidence intervals and to decision theory are indicated. No single definition of coherence is given which covers all cases of interest. The various cases distinguish between probability and statistical models and between finite and infinite spaces. No satisfactory theory is given for continuous statistical models.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Robert J. Buehler},
  DOI          = {10.1214/aos/1176343641},
  ISSN         = {0090-5364},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Buehler-1976.pdf},
  NUMBER       = {6},
  PAGES        = {1051–1064},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Coherent preferences},
  URL          = {http://www.jstor.org/stable/2958578},
  VOLUME       = {4},
  YEAR         = {1976},
}

@ARTICLE{Burge-karlin-1997,
  AUTHOR       = {Chris Burge and Samuel Karlin},
  JOURNALTITLE = {Journal of Molecular Biology},
  PAGES        = {78–94},
  TITLE        = {Prediction of Complete gene Structures in Human genomic DNA},
  VOLUME       = {268},
  YEAR         = {1997},
}

@BOOK{Burrill-1972-measure,
  AUTHOR    = {Claude W. Burrill},
  PUBLISHER = {McGraw-Hill},
  TITLE     = {Measure, Integration, and Probability},
  YEAR      = {1972},
}

@ARTICLE{Bushell-1986-Hilbert-metric,
  ABSTRACT     = {The Cayley-Hilbert metric is defined for a real Banach space containing a closed cone. By restricting the domain of a particular type of positive nonlinear operator, the Banach contraction-mapping theorem is used to prove the existence of a unique fixed point of the operator with explicit upper and lower bounds. Applications to quasilinear elliptic partial differential equations and to matrix theory are considered.},
  AUTHOR       = {P. J. Bushell},
  DOI          = {10.1016/0024-3795(86)90319-8},
  JOURNALTITLE = {Linear Algebra and its Applications},
  LOCALFILE    = {article/Bushell-1986-Hilbert-metric.pdf},
  PAGES        = {271–280},
  PUBLISHER    = {Elsevier},
  TITLE        = {The Cayley-Hilbert metric and positive operators},
  VOLUME       = {84},
  YEAR         = {1986},
}


@ARTICLE{Campos-Cozman-2007-epistemic,
  ABSTRACT     = {This paper investigates the computation of lower/upper expectations that must cohere with a collection of probabilistic assessments and a collection of judgements of epistemic independence. New algorithms, based on multilinear programming, are presented, both for independence among events and among random variables. Separation properties of graphical models are also investigated.},
  AUTHOR       = {Cassio Polpo de Campos and Fabio Gagliardi Cozman},
  DOI          = {10.1016/j.ijar.2006.07.013},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {concepts of independence; epistemic independence; imprecise probabilities; multilinear programming; sets of probability measures},
  LOCALFILE    = {article/Campos-Cozman-2007-epistemic.pdf},
  NUMBER       = {3},
  PAGES        = {244–260},
  PUBLISHER    = {Elsevier},
  TITLE        = {Computing lower and upper expectations under epistemic independence},
  VOLUME       = {44},
  YEAR         = {2007},
}

@INPROCEEDINGS{deCampos-Cozman-2005-UAI,
  AUTHOR    = {Cassio Polpo de Campos and Fabio Gagliardi Cozman},
  BOOKTITLE = {UAI-05: Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence},
  ISBN      = {0-9749039-1-4},
  LOCALFILE = {inproceedings/deCampos-Cozman-2005-UAI.pdf},
  PAGES     = {153–160},
  PUBLISHER = {AUAI Press},
  TITLE     = {Belief Updating and Learning in Semi-Qualitative Probabilistic Networks.},
  URL       = {http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1169&proceeding_id=21; http://arxiv.org/abs/1207.1367},
  YEAR      = {2005},
}

@ARTICLE{DeCampos-etal-2009,
  ABSTRACT     = {This paper explores the application of semi-qualitative probabilistic networks (SQPNs) that combine numeric and qualitative information to computer vision problems. Our version of SQPN allows qualitative influences and imprecise probability measures using intervals. We describe an Imprecise Dirichlet model for parameter learning and an iterative algorithm for evaluating posterior probabilities, maximum a posteriori and most probable explanations. Experiments on facial expression recognition and image segmentation problems are performed using real data.},
  ANNOTATION   = {doi: 10.1080/15598608.2009.10411920},
  AUTHOR       = {Cassio Polpo de Campos and Lei Zhang and Yan Tong and Qiang Ji},
  DOI          = {10.1080/15598608.2009.10411920},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Computer vision applications; Imprecise probabilities; Probabilistic networks; Qualitative relations},
  LOCALFILE    = {article/DeCampos-etal-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {197–210},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Semi-Qualitative Probabilistic Networks in Computer Vision Problems},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{DeCampos-Huete-Moral-1994-intervals,
  ABSTRACT     = {We study probability intervals as an interesting tool to represent uncertain information. A number of basic operations necessary to develop a calculus with probability intervals, such as combination, marginalization, conditioning and integration are studied in detail. Moreover, probability intervals are compared with other uncertainty theories, such as lower and upper probabilities, Choquet capacities of order two and belief and plausibility functions. The advantages of probability intervals with respect to these formalisms in computational efficiency are also highlighted.},
  ANNOTATION   = {op papier},
  AUTHOR       = {Luis M. de Campos and Juan F. Huete and Serafín Moral},
  DOI          = {10.1142/S0218488594000146},
  JOURNALTITLE = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  KEYWORDS     = {Combination; Conditioning; Lower and Upper Probability; Marginalization; Probability Intervals; Uncertainty Management},
  LOCALFILE    = {article/DeCampos-Huete-Moral-1994-intervals.pdf},
  NUMBER       = {2},
  PAGES        = {167–196},
  TITLE        = {Probability intervals: a tool for uncertain reasoning},
  URL          = {http://decsai.ugr.es/~lci/journal-papers-pdf/ijuf94.pdf},
  VOLUME       = {2},
  YEAR         = {1994},
}

@ARTICLE{Cano-etal-2007-cn,
  ABSTRACT     = {This paper proposes two new algorithms for inference in credal networks. These algorithms enable probability intervals to be obtained for the states of a given query variable. The first algorithm is approximate and uses the hill-climbing technique in the Shenoy-Shafer architecture to propagate in join trees ; the second is exact and is a modification of Rocha and Cozman's branch-and-bound algorithm, but applied to general directed acyclic graphs.},
  AUTHOR       = {Andrés Cano and Manuel Gómez and Serafín Moral and Joaquín Abellán},
  DOI          = {10.1016/j.ijar.2006.07.020},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian networks; branch-and-bound algorithms; credal network; hill-climbing; probability intervals; strong independence},
  LOCALFILE    = {article/Cano-etal-2007-cn.pdf},
  NUMBER       = {3},
  PAGES        = {261–280},
  TITLE        = {Hill-climbing and branch-and-bound algorithms for exact and approximate inference in credal networks},
  VOLUME       = {44},
  YEAR         = {2007},
}

@ARTICLE{Capotorti-Galli-Vantaggi-2003,
  ABSTRACT     = {We introduce an operational way to reduce the spatial complexity in inference processes based on conditional lower–upper probabilities assessments. To reach such goal we must suitably exploit zero probabilities taking account of logical conditions characterizing locally strong coherence. We actually re-formulate for conditional lower–upper probabilities the notion of locally strong coherence already introduced for conditional precise probabilities. Thanks to the characterization, we avoid to build all atoms, so that several real problems become feasible. In fact, the real complexity problem is connected to the number of atoms. Since for an inferential process with lower–upper probabilities several sequences of constraints must be fulfilled, our simplification can have either a “global” or a “partial” effect, being applicable to all or just to some sequences. The whole procedure has been implemented by XLisp-Stat language. A comparison with other approaches will be done by an example.},
  AUTHOR       = {Andrea Capotorti and L. Galli and Barbara Vantaggi},
  DOI          = {10.1007/s00500-002-0214-6},
  ISSN         = {1432-7643},
  JOURNALTITLE = {Soft Computing},
  LOCALFILE    = {article/Capotorti-Galli-Vantaggi-2003.pdf},
  NUMBER       = {5},
  PAGES        = {280–287},
  PUBLISHER    = {Springer},
  TITLE        = {Locally strong coherence and inference with lower-upper probabilities},
  VOLUME       = {7},
  YEAR         = {2003},
}

@INPROCEEDINGS{Capotorti-Zagoraiou-2006,
  AUTHOR    = {Andrea Capotorti and Maroussa Zagoraiou},
  BOOKTITLE = {Proceedings of the Eleventh International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
  LOCATION  = {Paris},
  TITLE     = {Implicit Degree of Support for Finite Lower-Upper Conditional Probabilities Extensions},
  YEAR      = {2006},
}

@BOOK{Carnap-1952,
  AUTHOR    = {Rudoplh Carnap},
  LOCATION  = {Chicago},
  PUBLISHER = {The University of Chicago Press},
  TITLE     = {The Continuum of Inductive Methods},
  YEAR      = {1952},
}

@ARTICLE{Casalis-1996,
  ABSTRACT     = {The present paper describes all the natural exponential families on \mathbb{R}^d whose variance function is of the form V(m) = am øtimes m + B(m) + C, with m øtimes m(θ) = ⟨θ, m ⟩m and B linear in m. There are 2d + 4 types of such families, which are built from particular mixtures of families of Normal, Poisson, gamma, hyperbolic on \mathbb{R}^d and negative-multinomial distributions. The proof of this result relies mainly on techniques used in the elementary theory of Lie algebras.},
  AUTHOR       = {M. Casalis},
  DOI          = {10.1214/aos/1032298298},
  ISSN         = {0090-5364},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {Morris class; Variance functions},
  LOCALFILE    = {article/Casalis-1996.pdf},
  NUMBER       = {4},
  PAGES        = {1828–1854},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {The 2d+4 simple quadratic natural exponential families on R^d},
  URL          = {http://www.jstor.org/stable/2242752},
  VOLUME       = {24},
  YEAR         = {1996},
}

@ARTICLE{Castillo-Gutierrez-Hadi-1997,
  ABSTRACT     = {This paper presents an efficient computational method for performing sensitivity analysis in discrete Bayesian networks. The method exploits the structure of conditional probabilities of a target node given the evidence. First, the set of parameters which is relevant to the calculation of the conditional probabilities of the target node is identified. Next, this set is reduced by removing those combinations of the parameters which either contradict the available evidence or are incompatible. Finally, using the canonical components associated with the resulting subset of parameters, the desired conditional probabilities are obtained. In this way, an important saving in the calculations is achieved. The proposed method can also be used to compute exact upper and lower bounds for the conditional probabilities, hence a sensitivity analysis can be easily performed. Examples are used to illustrate the proposed methodology},
  AUTHOR       = {Enrique Castillo and J. M. Gutierrez and A. S. Hadi},
  DOI          = {10.1109/3468.594909},
  ISSN         = {1083-4427},
  JOURNALTITLE = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on},
  KEYWORDS     = {conditional probabilities; discrete Bayesian networks; efficient computational method; lower bounds; sensitivity analysis; upper bounds; Bayes methods; case-based reasoning; directed graphs; probability; sensitivity analysis},
  MONTH        = {7},
  NUMBER       = {4},
  PAGES        = {412–423},
  TITLE        = {Sensitivity analysis in discrete Bayesian networks},
  VOLUME       = {27},
  YEAR         = {1997},
}

@INPROCEEDINGS{Chan-Darwiche-2004,
  ABSTRACT  = {Previous work on sensitivity analysis in Bayesian networks has focused on single parameters, where the goal is to understand the sensitivity of queries to single parameter changes, and to identify single parameter changes that would enforce a certain query constraint. In this paper, we expand the work to multiple parameters which may be in the CPT of a single variable, or the CPTs of multiple variables. Not only do we identify the solution space of multiple parameter changes that would be needed to enforce a query constraint, but we also show how to find the optimal solution, that is, the one which disturbs the current probability distribution the least (with respect to a specific measure of disturbance). We characterize the computational complexity of our new techniques and discuss their applications to developing and debugging Bayesian networks, and to the problem of reasoning about the value (reliability) of new information.},
  AUTHOR    = {Hei Chan and Adnan Darwiche},
  BOOKTITLE = {UAI-04: Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence},
  LOCATION  = {Arlington, Virginia},
  PAGES     = {67–75},
  PUBLISHER = {AUAI Press},
  TITLE     = {Sensitivity analysis in Bayesian networks: from single to multiple parameters},
  YEAR      = {2004},
}

@INPROCEEDINGS{2006-Charitos+Gaag,
  AUTHOR    = {Theodore Charitos and Linda C. van der Gaag},
  BOOKTITLE = {FLAIRS Conference},
  EDITOR    = {Geoff Sutcliffe and Randy Goebel},
  LOCALFILE = {inproceedings/2006-Charitos+Gaag.pdf},
  PAGES     = {806–811},
  PUBLISHER = {AAAI Press},
  TITLE     = {Sensitivity analysis of Markovian models},
  YEAR      = {2006},
}

@ARTICLE{Charitos+al-2009,
  ABSTRACT     = {Diagnosing ventilator-associated pneumonia in mechanically ventilated patients in intensive care units is seen as a clinical challenge. The difficulty in diagnosing ventilator-associated pneumonia stems from the lack of a simple yet accurate diagnostic test. To assist clinicians in diagnosing and treating patients with pneumonia, a decision-theoretic network had been designed with the help of domain experts. A major limitation of this network is that it does not represent pneumonia as a dynamic process that evolves over time. In this paper, we construct a dynamic Bayesian network that explicitly captures the development of the disease over time. We discuss how probability elicitation from domain experts served to quantify the dynamics involved and how the nature of the patient data helps reduce the computational burden of inference. We evaluate the diagnostic performance of our dynamic model for a number of real patients and report promising results.},
  AUTHOR       = {Theodore Charitos and Linda C. van der Gaag and Stefan Visscher and Karin A. M. Schurink and Peter J. F. Lucas},
  DOI          = {10.1016/j.eswa.2007.11.065},
  ISSN         = {0957-4174},
  JOURNALTITLE = {Expert Systems with Applications},
  KEYWORDS     = {Ventilator-associated pneumonia; Diagnosis; Dynamic Bayesian networks; Stochastic processes; Inference},
  LOCALFILE    = {article/Charitos+al-2009.pdf},
  NUMBER       = {2, Part 1},
  PAGES        = {1249–1258},
  TITLE        = {A dynamic Bayesian network for diagnosing ventilator-associated pneumonia in ICU patients},
  VOLUME       = {36},
  YEAR         = {2009},
}

@ARTICLE{Charitos+De_Waal+Van_der_Gaag-2008,
  ABSTRACT     = {Markov chains constitute a common way of modelling the progression of a chronic disease through various severity states. For these models, a transition matrix with the probabilities of moving from one state to another for a specific time interval is usually estimated from cohort data. Quite often, however, the cohort is observed at specific times with intervals that may be greater than the interval of interest. The transition matrix computed then needs to be decomposed in order to estimate the desired interval transition matrix suited to the model. Although simple to implement, this method of matrix decomposition can yet result in an invalid short-interval transition matrix with negative or complex entries. In this paper, we present a method for computing short-interval transition matrices that is based on regularization techniques. Our method operates separately on each row of the invalid short-interval transition matrix aiming to minimize an appropriate distance measure. We test our method on various matrix structures and sizes, and evaluate its performance on a real-life transition model for HIV-infected individuals.},
  AUTHOR       = {Theodore Charitos and Peter R. de Waal and Linda C. van der Gaag},
  DOI          = {10.1002/sim.2970},
  ISSN         = {1097-0258},
  JOURNALTITLE = {Statistics in Medicine},
  KEYWORDS     = {Markov chain; transition matrix; regularization techniques},
  LOCALFILE    = {article/Charitos+De_Waal+Van_der_Gaag-2008.pdf},
  NUMBER       = {6},
  PAGES        = {905–921},
  PUBLISHER    = {John Wiley \& Sons, Ltd.},
  TITLE        = {Computing short-interval transition matrices of a discrete-time Markov chain from partially observed data},
  VOLUME       = {27},
  YEAR         = {2008},
}

@ARTICLE{Charitos+De_Waal+Van_der_Gaag-2007,
  ABSTRACT     = {Sequential statistical models such as dynamic Bayesian networks and hidden Markov models more specifically, model stochastic processes over time. In this paper, we study for these models the effect of consecutive similar observations on the posterior probability distribution of the represented process. We show that, given such observations, the posterior distribution converges to a limit distribution. Building upon the rate of the convergence, we further show that, given some wished-for level of accuracy, part of the inference can be forestalled. To evaluate our theoretical results, we study their implications for a real-life model from the medical domain and for a benchmark model for agricultural purposes. Our results indicate that whenever consecutive similar observations arise, the computational requirements of inference in Markovian models can be drastically reduced.},
  AUTHOR       = {Theodore Charitos and Peter R. de Waal and Linda C. van der Gaag},
  DOI          = {10.1016/j.ijar.2006.09.011},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Markovian models; Consecutive similar observations; Convergence; Inference; Efficiency},
  LOCALFILE    = {article/Charitos+De_Waal+Van_der_Gaag-2007.pdf},
  NUMBER       = {2},
  PAGES        = {300–319},
  TITLE        = {Convergence in Markovian models with implications for efficiency of inference},
  VOLUME       = {46},
  YEAR         = {2007},
}

@ARTICLE{Charnes-Cooper-1959,
  ABSTRACT     = {A new conceptual and analytical vehicle for problems of temporal planning under uncertainty, involving determination of optimal (sequential) stochastic decision rules is defined and illustrated by means of a typical industrial example. The paper presents a method of attack which splits the problem into two non-linear (or linear) programming parts, (i) determining optimal probability distributions, (ii) approximating the optimal distributions as closely as possible by decision rules of prescribed form.},
  AUTHOR       = {A. Charnes and W. W. Cooper},
  DOI          = {10.1287/mnsc.6.1.73},
  ISSN         = {0025-1909},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Charnes-Cooper-1959.pdf},
  MONTH        = {oct},
  NUMBER       = {1},
  PAGES        = {73–79},
  PUBLISHER    = {INFORMS},
  TITLE        = {Chance-constrained programming},
  URL          = {http://www.jstor.org/stable/2627476},
  VOLUME       = {6},
  YEAR         = {1959},
}

@INCOLLECTION{Chateauneuf-Jaffray-1995,
  ABSTRACT  = {The concept of local Möbius transform of a capacity is introduced and shown to provide a handier characterization of K-monotonicity than the standard Möbius transformation. It is moreover used to give a new proof of the preservation of K monotonicity by conditional lower probabilities.},
  AUTHOR    = {Alain Chateauneuf and Jean-Yves Jaffray},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning and Uncertainty},
  DOI       = {10.1007/3-540-60112-0_14},
  EDITOR    = {Christine Froidevaux and Jürg Kohlas},
  ISBN      = {978-3-540-60112-8},
  LOCALFILE = {inbook/Chateauneuf-Jaffray-1995.pdf},
  NUMBER    = {1},
  PAGES     = {115–124},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Local Möbius transforms of monotone capacities},
  VOLUME    = {946},
  YEAR      = {1995},
}

@ARTICLE{Chateauneuf-Jaffray-1989,
  ABSTRACT     = {Monotone capacities (on finite sets) of finite or infinite order (lower probabilities) are characterized by properties of their Möbius inverses. A necessary property of probabilities dominating a given capacity is demonstrated through the use of Gale's theorem for the transshipment problem. This property is shown to be also sufficient if and only if the capacity is monotone of infinite order. A characterization of dominating probabilities specific to capacities of order 2 is also proved.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Alain Chateauneuf and Jean-Yves Jaffray},
  DOI          = {10.1016/0165-4896(89)90056-5},
  JOURNALTITLE = {Mathematical Social Sciences},
  KEYWORDS     = {Decision theory; belief funct; lower probability},
  LOCALFILE    = {article/Chateauneuf-Jaffray-1989.pdf},
  NUMBER       = {3},
  PAGES        = {263–283},
  PUBLISHER    = {Elsevier},
  TITLE        = {Some characterizations of lower probabilities and other monotone capacities through the use of Möbius inversion},
  VOLUME       = {17},
  YEAR         = {1989},
}

@ARTICLE{Chepoi-1994,
  ABSTRACT     = {A convexity structure satisfies the separation property S 4 if any two disjoint convex sets extend to complementary half-spaces. This property is investigated for alignment spaces, n -ary convexities, and graphs. In particular, it is proven that a) an n -ary convexity is S 4 iff every pair of disjoint polytopes with at most n vertices can be separated by complementary half spaces, and b) an interval convexity is S 4 iff it satisfies the analogue of the Pasch axiom of plane geometry. A characterization of bipartite and weakly modular spaces with S 4 convexity is given in terms of forbidden subgraphs.},
  AUTHOR       = {Victor Chepoi},
  DOI          = {10.1007/BF01222661},
  ISSN         = {0047-2468},
  JOURNALTITLE = {Journal of Geometry},
  LOCALFILE    = {article/Chepoi-1994.pdf},
  NUMBER       = {1},
  PAGES        = {30–51},
  PUBLISHER    = {Birkhäuser Basel},
  TITLE        = {Separation of two convex sets in convexity structures},
  VOLUME       = {50},
  YEAR         = {1994},
}

@ARTICLE{Choquet-1954,
  ABSTRACT     = {C'est un essai de théorie générale des fonctions croissantes d'ensemble. On est amené à mettre en évidence diverses classes importantes de telles fonctions, en particulier la classe des fonctions fortement sous-additives, pour lesquelles existe une théorie analogue à celle de la mesure, et une sous-classe de celle-ci, à savoir la classe des fonctions alternées d'ordre infini, analogues aux fonctions numériques complètement monotones. La capacité classique est une telle fonction d'ensemble ; il en résulte l'identité des capacités intérieure et extérieure de tout ensemble borélien ou analytique. Un outil de recherche puissant est la représentation intégrale des fonctions d'une classe additive et convexe au moyen des éléments extrémaux d'une telle classe. Cette représentation permet d'identifier les fonctions alternées d'ordre infini avec certaines probabilités associées à l'ensemble variable.},
  AUTHOR       = {Gustave Choquet},
  DOI          = {10.5802/aif.53},
  JOURNALTITLE = {Annales de l'Institut Fourier},
  LOCALFILE    = {article/Choquet-1954.pdf},
  PAGES        = {131–295},
  TITLE        = {Theory of capacities},
  URL          = {http://www.numdam.org/item?id=AIF_1954__5__131_0},
  VOLUME       = {5},
  YEAR         = {1954},
}

@INPROCEEDINGS{Chrisman-1996,
  AUTHOR    = {Lonnie Chrisman},
  BOOKTITLE = {Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {Propagation of 2-Monotone Lower Probabilities on an Undirected Graph},
  YEAR      = {1996},
}

@ARTICLE{Cifarelli-Regazzini-1996,
  ABSTRACT     = {This paper summarizes the scientific activity of de Finetti in probability and statistics. It falls into three sections: Section 1 includes an essential biography of de Finetti and a survey of the basic features of the scientific milieu in which he took the first steps of his scientific career; Section 2 concerns de Finetti's work in probability: (a) foundations, (b) processes with independent increments, (c) sequences of exchangeable random variables, and (d) contributions which fall within other fields; Section 3 deals with de Finetti's contributions to statistics: (a) description of frequency distributions, (b) induction and statistics, (c) probability and induction, and (d) objectivistic schools and theory of decision. Many recent developments of de Finetti's work are mentioned here and briefly described.},
  AUTHOR       = {Donato Michele Cifarelli and Eugenio Regazzini},
  DOI          = {10.1214/ss/1032280303},
  JOURNALTITLE = {Statistical Science},
  KEYWORDS     = {Associative mean; Bayes-Laplace paradigm; Bayesian nonparametric statistics; Glivenko-Cantelli theorem; completely additive probabilities; correlation and monotone dependence; exchangeable and partially exchangeable random var; finitely additive probabilities; gambler's ruin; infinitely decomposable laws; predictive inference; prevision; principle of coherence processes with independent; reasoning by induction; statistical decision; subjective probability; utility function},
  LOCALFILE    = {article/Cifarelli-Regazzini-1996.pdf},
  NUMBER       = {4},
  PAGES        = {253–282},
  TITLE        = {De Finetti's Contribution to probability and Statistics},
  VOLUME       = {11},
  YEAR         = {1996},
}

@INPROCEEDINGS{Clarkson-1994,
  ABSTRACT  = {A simple idea for speeding up the computation of extrema of a partially ordered set turns out to have a number of interesting applications in geometric algorithms; the resulting algorithms generally replace an appearance of the input size n in the running time by an output size A les;n. In particular, the A coordinate-wise minima of a set of n points in Rd can be found by an algorithm needing O(nA) time. Given n points uniformly distributed in the unit square, the algorithm needs n+O(n5/8) point comparisons on average. Given a set of n points in Rd, another algorithm can find its A extreme points in O(nA) time. Thinning for nearest-neighbor classification can be done in time O(n log n) Sigma;i Ai ni, finding the Ai irredundant points among ni points for each class i, where n= Sigma;i ni is the total number of input points. This sharpens a more obvious O(n3) algorithm, which is also given here. Another algorithm is given that needs O(n) space to compute the convex hull of n points in O(nA) time. Finally, a new randomized algorithm finds the convex hull of n points in O(n log A) expected time, under the condition that a random subset of the points of size r has expected hull complexity O(r). All but the last of these algorithms has polynomial dependence on the dimension d, except possibly for linear programming},
  AUTHOR    = {Kenneth L. Clarkson},
  BOOKTITLE = {Proceedings of the 35th Annual Symposium on Foundations of Computer Science},
  DOI       = {10.1109/SFCS.1994.365723},
  KEYWORDS  = {Linear programming; Polynomials; Sorting; computational complexity; computational geometry; set theory; coordinate-wise minima; extrema; geometric algorithms; hull complexity; irredundant points; output-sensitive; partially ordered set},
  MONTH     = {11},
  PAGES     = {695–702},
  TITLE     = {More output-sensitive geometric algorithms},
  YEAR      = {1994},
}

@ARTICLE{Clarkson-Shor-1989,
  AUTHOR       = {Kenneth L. Clarkson and Peter W. Shor},
  DOI          = {10.1007/BF02187740},
  ISSN         = {0179-5376},
  JOURNALTITLE = {Discrete \& Computational Geometry},
  NUMBER       = {1},
  PAGES        = {387–421},
  PUBLISHER    = {Springer-Verlag},
  TITLE        = {Applications of random sampling in computational geometry, II},
  VOLUME       = {4},
  YEAR         = {1989},
}

@BOOK{Coletti-Scozzafava-2002,
  AUTHOR    = {Gulianella Coletti and R. Scozzafava},
  DOI       = {10.1007/978-94-010-0474-9},
  PUBLISHER = {Kluwer Academic Publishers},
  SERIES    = {Trends in Logic},
  TITLE     = {Probabilistic logic in a coherent setting},
  VOLUME    = {15},
  YEAR      = {2002},
}

@ARTICLE{Combarro-Miranda-2008-polytope,
  ABSTRACT     = {In this paper we deal with the problem of studying the structure of the polytope of non-additive measures for finite referential sets. We give a necessary and sufficient condition for two extreme points of this polytope to be adjacent. We also show that it is possible to find out in polynomial time whether two vertices are adjacent. These results can be extended to the polytope given by the convex hull of monotone Boolean functions. We also give some results about the facets and edges of the polytope of non-additive measures; we prove that the diameter of the polytope is 3 for referentials of three elements or more. Finally, we show that the polytope is combinatorial and study the corresponding properties; more concretely, we show that the graph of non-additive measures is Hamilton connected if the cardinality of the referential set is not 2.},
  AUTHOR       = {Elías F. Combarro and Pedro Miranda},
  DOI          = {10.1016/j.fss.2007.12.021},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {Adjacency; Combinatorial polytopes; Complexity; Diameter; Monotone Boolean functions; Non-additive measures; Stack filters},
  LOCALFILE    = {article/Combarro-Miranda-2008-polytope.pdf},
  NUMBER       = {16},
  PAGES        = {2145–2162},
  TITLE        = {On the polytope of non-additive measures},
  VOLUME       = {159},
  YEAR         = {2008},
}

@ARTICLE{Connor-Mosimann-1969,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Robert J. Connor and James E. Mosimann},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Conner-Mosimann-1969.pdf},
  NUMBER       = {325},
  PAGES        = {194–206},
  TITLE        = {Concepts of independence for proportions with a generalization of the Dirichlet distribution},
  URL          = {http://www.jstor.org/stable/2283728},
  VOLUME       = {64},
  YEAR         = {1969},
}

@ARTICLE{Consonni-Veronese-2001,
  ABSTRACT     = {Consider a standard conjugate family of prior distributions for a vector-parameter indexing an exponential family. Two distinct model parameterizations may well lead to standard conjugate families which are not consistent, i.e. one family cannot be derived from the other by the usual change-of-variable technique. This raises the problem of finding suitable parameterizations that may lead to enriched conjugate families which are more flexible than the traditional ones. The previous remark motivates the definition of a new property for an exponential family, named conditional reducibility. Features of conditionally-reducible natural exponential families are investigated thoroughly. In particular, we relate this new property to the notion of cut, and show that conditionally-reducible families admit a reparameterization in terms of a vector having likelihood-independent components. A general methodology to obtain enriched conjugate distributions for conditionally-reducible families is described in detail, generalizing previous works and more recent contributions in the area. The theory is illustrated with reference to natural exponential families having simple quadratic variance function.},
  AUTHOR       = {Guido Consonni and Piero Veronese},
  DOI          = {10.1111/1467-9469.00243},
  JOURNALTITLE = {Scandinavian Journal of Statistics},
  KEYWORDS     = {Bayesian inference; conditional reducibility; cut; enriched prior; exponential family; simple quadratic variance function},
  LOCALFILE    = {article/Consonni-Veronese-2001.pdf},
  NUMBER       = {2},
  PAGES        = {377–406},
  TITLE        = {Conditionally Reducible Natural Exponential Families and Enriched Conjugate Priors},
  VOLUME       = {28},
  YEAR         = {2001},
}

@ARTICLE{Consonni-Veronese-1992,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Guido Consonni and Piero Veronese},
  JOURNALTITLE = {Journal of the American Statistical Association},
  KEYWORDS     = {Bayesian statistics; least favorable prior; partial prior information},
  LOCALFILE    = {article/Consonni-Veronese-1992.pdf},
  NUMBER       = {420},
  PAGES        = {1123–1127},
  TITLE        = {Conjugate priors for exponential families having quadratic variance functions},
  URL          = {http://www.jstor.org/stable/2290650},
  VOLUME       = {87},
  YEAR         = {1992},
}

@ARTICLE{Consonni-Veronese-GutierrezPena-2004,
  ABSTRACT     = {Reference analysis is one of the most successful general methods to derive noninformative prior distributions. In practice, however, reference priors are often difficult to obtain. Recently developed theory for conditionally reducible natural exponential families identifies an attractive reparameterization which allows one, among other things, to construct an enriched conjugate prior. In this paper, under the assumption that the variance function is simple quadratic, the order-invariant group reference prior for the above parameter is found. Furthermore, group reference priors for the mean- and natural parameter of the families are obtained. A brief discussion of the frequentist coverage properties is also presented. The theory is illustrated for the multinomial and negative-multinomial family. Posterior computations are especially straightforward due to the fact that the resulting reference distributions belong to the corresponding enriched conjugate family. A substantive application of the theory relates to the construction of reference priors for the Bayesian analysis of two-way contingency tables with respect to two alternative parameterizations.},
  AUTHOR       = {Guido Consonni and Piero Veronese and Eduardo Gutiérrez-Peña},
  DOI          = {10.1016/S0047-259X(03)00095-2},
  JOURNALTITLE = {Journal of Multivariate Analysis},
  KEYWORDS     = {Bayesian inference; Contingency table; Enriched conjugate prior; Multinomial family; Negative-multinomial family; Noninformative prior; conditional reducibility},
  LOCALFILE    = {article/Consonni-Veronese-GutierrezPena-2004.pdf},
  NUMBER       = {2},
  PAGES        = {335–364},
  PUBLISHER    = {Elsevier},
  TITLE        = {Reference priors for exponential families with simple quadratic variance function},
  VOLUME       = {88},
  YEAR         = {2004},
}

@THESIS{Coolen-1994,
  AUTHOR      = {Frank P. A. Coolen},
  INSTITUTION = {Technische Universiteit Eindhoven},
  TITLE       = {Statistical Modelling of Expert Opinions Using Imprecise Probabilities},
  TYPE        = {phdthesis},
  YEAR        = {1994},
}

@ARTICLE{Coolen-1993,
  ABSTRACT     = {Reconsidering generalizations of the original Bayesian framework that have been suggested during the last three decades, imprecise conjugate prior densities are proposed for members of the one-parameter exponential family of distributions.},
  ANNOTATION   = {reprint},
  AUTHOR       = {Frank P. A. Coolen},
  DOI          = {10.1016/0167-7152(93)90066-R},
  JOURNALTITLE = {Statistics \& Probability Letters},
  KEYWORDS     = {Bayesian theory; conjugate priors; imprecise pro},
  LOCALFILE    = {article/Coolen-1993.pdf},
  NUMBER       = {5},
  PAGES        = {337–342},
  PUBLISHER    = {Elsevier},
  TITLE        = {Imprecise conjugate prior densities for the one-parameter exponential family of distributions},
  VOLUME       = {16},
  YEAR         = {1993},
}

@ARTICLE{Coolen-Augustin-2009-IDM-alternative,
  ABSTRACT     = {Nonparametric predictive inference (NPI) is a general methodology to learn from data in the absence of prior knowledge and without adding unjustified assumptions. This paper develops NPI for multinomial data when the total number of possible categories for the data is known. We present the upper and lower probabilities for events involving the next observation and several of their properties. We also comment on differences between this NPI approach and corresponding inferences based on Walley's Imprecise Dirichlet Model.},
  ANNOTATION   = {Special Section on The Imprecise Dirichlet Model and Special Section on Bayesian Robustness (Issues in Imprecise Probability)},
  AUTHOR       = {Frank P. A. Coolen and Thomas Augustin},
  DOI          = {10.1016/j.ijar.2008.03.011},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Circular A(n); Imprecise Dirichlet Model; Imprecise probabilities; Interval probability; Lower and upper probabilities; Multinomial data; Nonparametric predictive inference; Rule of succession},
  LOCALFILE    = {article/Coolen-Augustin-2008-IDM-alternative.pdf},
  NUMBER       = {2},
  PAGES        = {217–230},
  PUBLISHER    = {Elsevier},
  TITLE        = {A nonparametric predictive alternative to the Imprecise Dirichlet Model: the case of a known number of categories},
  VOLUME       = {50},
  YEAR         = {2009},
}

@REPORT{Coolen-Augustin-2006-cNPI1,
  AUTHOR = {Frank P. A. Coolen and Thomas Augustin},
  TITLE  = {Nonparametric predictive inference for multinomial data - Notes 1 (m-functions and interval probabilities for events)},
  TYPE   = {techreport},
  YEAR   = {2006},
}

@REPORT{Coolen-Augustin-2006-cNPI4,
  AUTHOR = {Frank P. A. Coolen and Thomas Augustin},
  TITLE  = {Nonparametric predictive inference for multinomial data - Notes 4 (known number of categories, interval previsions for gambles, classification, and further comments)},
  TYPE   = {techreport},
  YEAR   = {2006},
}

@ARTICLE{CoolenSchrijner-etal-2009,
  ABSTRACT     = {More and more often the traditional (classical) concept of probability, and the statistical methods based on it, have been criticized for being unable to cope with the multidimensional nature of uncertainty. Careful handling of imprecision is essential to draw reliable conclusions from complex data. This paper presents a short introductory discussion on the general area of imprecision in statistical theory and practice, and briefly introduces the further papers in this collection, demonstrating the importance of the adequate modelling of imprecision in different areas of application.},
  AUTHOR       = {Pauline Coolen-Schrijner and Frank P. A. Coolen and Matthias C. M. Troffaes and Thomas Augustin},
  DOI          = {10.1080/15598608.2009.10411907},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Decision making; Elicition; Inference; Lower and upper probabilities; Robustness; Theory},
  LOCALFILE    = {article/CoolenSchrijner-etal-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {1–9},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Imprecision in Statistical Theory and Practice},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{CoolenSchrijner-Maturi-Coolen-2009.pdf,
  ABSTRACT     = {This paper presents a statistical method for comparison of two groups based on nonparametric predictive inference (NPI). NPI is a statistical approach based on few modelling assumptions, with inferences strongly based on data and uncertainty quantified via lower and upper probabilities. Life- times of units from groups X and Y are compared, based on observed lifetimes from an experiment that may have ended before all units failed. We present upper and lower probabilities for the event that the lifetime of a future unit from X is less than the lifetime of a future unit from Y, and we compare this approach with traditional precedence testing.},
  AUTHOR       = {Pauline Coolen-Schrijner and Tahani A. Maturi and Frank P. A. Coolen},
  DOI          = {10.1080/15598608.2009.10411925},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Lower and upper probabilities; Nonparametric predictive inference; Pairwise comparison; Precedence tests},
  LOCALFILE    = {article/CoolenSchrijner-Maturi-Coolen-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {273–287},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Nonparametric predictive precedence testing for two groups},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Corani-Zaffalon-2008-NCC2,
  ABSTRACT     = {In this paper, the naive credal classifier, which is a set-valued counterpart of naive Bayes, is extended to a general and flexible treatment of incomplete data, yielding a new classifier called naive credal classifier 2 (NCC2). The new classifier delivers classifications that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classifications, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classifier, such as naive Bayes. This phenomenon adds even more value to the robust approach to classification implemented by NCC2.},
  AUTHOR       = {Giorgio Corani and Marco Zaffalon},
  JOURNALTITLE = {Journal of Machine Learning Research},
  PAGES        = {581–621},
  TITLE        = {Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2},
  VOLUME       = {9},
  YEAR         = {2008},
}

@ARTICLE{Coupe-VanderGaag-2002,
  ABSTRACT     = {The assessments for the various conditional probabilities of a Bayesian belief network inevitably are inaccurate, influencing the reliability of its output. By subjecting the network to a sensitivity analysis with respect to its conditional probabilities, the reliability of its output can be investigated. Unfortunately, straightforward sensitivity analysis of a belief network is highly time-consuming. In this paper, we show that by qualitative considerations several analyses can be identified as being uninformative as the conditional probabilities under study cannot affect the output. In addition, we show that the analyses that are informative comply with simple mathematical functions. More specifically, we show that a belief network's output can be expressed as a quotient of two functions that are linear in a conditional probability under study. These properties allow for considerably reducing the computational burden of sensitivity analysis of Bayesian belief networks.},
  AUTHOR       = {Veerle M. H. Coupé and Linda C. van der Gaag},
  DOI          = {10.1023/A:1016398407857},
  ISSN         = {1012-2443},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  NUMBER       = {4},
  PAGES        = {323–356},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Properties of sensitivity analysis of Bayesian belief networks},
  VOLUME       = {36},
  YEAR         = {2002},
}

@INPROCEEDINGS{Couso-Moral-2009-ISIPTA,
  AUTHOR       = {Inés Couso and Serafín Moral},
  BOOKTITLE    = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
  EDITOR       = {Thomas Augustin and Frank P. A. Coolen and Serafín Moral and Matthias C. M. Troffaes},
  ORGANIZATION = {SIPTA},
  PAGES        = {99–108},
  TITLE        = {Sets of desirable gambles and credal sets},
  URL          = {http://www.sipta.org/isipta09/proceedings/063.html},
  VANUE        = {Durham, United Kingdom},
  YEAR         = {2009},
}

@ARTICLE{2011-Couso+Moral-desir,
  ABSTRACT     = {The theory of sets of desirable gambles is a very general model which covers most of the existing theories for imprecise probability as special cases; it has a clear and simple axiomatic justification; and mathematical definitions are natural and intuitive. However, much work remains to be done until the theory of desirable gambles can be considered as generally applicable to reasoning tasks as other approaches to imprecise probability are. This paper gives an overview of some of the fundamental concepts for reasoning with uncertainty expressed in terms of desirable gambles in the finite case, provides a characterization of regular extension, and studies the nature of maximally coherent sets of desirable gambles, which correspond to finite sequences of probability distributions, each one of them defined on the set where the previous one assigns probability zero.},
  AUTHOR       = {Inés Couso and Serafín Moral},
  DOI          = {10.1016/j.ijar.2011.04.004},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/2011-Couso+Moral-desir.pdf},
  NUMBER       = {7},
  PAGES        = {1034–1055},
  TITLE        = {Sets of desirable gambles: conditioning, representation, and precise probabilities},
  VOLUME       = {52},
  YEAR         = {2011},
}

@ARTICLE{Couso-Moral-Walley-2000-independence,
  ABSTRACT     = {Our aim in this paper is to clarify the notion of independence for imprecise probabilities. Suppose that two marginal experiments are each described by an imprecise probability model, i.e., by a convex set of probability distributions or an equivalent model such as upper and lower probabilities or previsions. Then there are several ways to define independence of the two experiments and to construct an imprecise probability model for the joint experiment. We survey and compare six definitions of independence. To clarify the meaning of the definitions and the relationships between them, we give simple examples which involve drawing balls from urns. For each concept of independence, we give a mathematical definition, an intuitive or behavioural interpretation, assumptions under which the definition is justified, and an example of an urn model to which the definition is applicable. Each of the independence concepts we study appears to be useful in some kinds of application. The concepts of strong independence and epistemic independence appear to be the most frequently applicable.},
  AUTHOR       = {Inés Couso and Serafín Moral and Peter Walley},
  DOI          = {10.1017/S1357530900000156},
  JOURNALTITLE = {Risk, Decision and Policy},
  LOCALFILE    = {article/Couso-Moral-Walley-2000-independence.pdf},
  NUMBER       = {2},
  PAGES        = {165–181},
  TITLE        = {A survey of concepts of independence for imprecise probabilities},
  VOLUME       = {5},
  YEAR         = {2000},
}

@BOOK{Cowell-etal-2003-expert,
  AUTHOR    = {R. G. Cowell and A. Philip Dawid and Steffen L. Lauritzen and D. J. Spiegelhalter},
  ISBN      = {978-0-387-98767-5},
  PUBLISHER = {Springer},
  SERIES    = {Information Science and Statistics},
  TITLE     = {Probabilistic Networks and Expert Systems},
  YEAR      = {2003},
}

@MISC{Cozman-website,
  AUTHOR = {Fabio Gagliardi Cozman},
  TITLE  = {Website of Fabio Cozman},
  URL    = {http://sites.poli.usp.br/p/fabio.cozman},
}

@ARTICLE{Cozman-2005-graphical,
  ABSTRACT     = {This paper presents an overview of graphical models that can handle imprecision in probability values. The paper first reviews basic concepts and presents a brief historical account of the field. The main characteristics of the credal network model are then discussed, as this model has received considerable attention in the literature.},
  AUTHOR       = {Fabio Gagliardi Cozman},
  DOI          = {10.1016/j.ijar.2004.10.003},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Credal network; Graphical models; Sets of probability distributions; imprecise probability},
  LOCALFILE    = {article/Cozman-2005-graphical.pdf},
  NUMBER       = {2-3},
  PAGES        = {167–184},
  TITLE        = {Graphical models for imprecise probabilities},
  VOLUME       = {39},
  YEAR         = {2005},
}

@MISC{Cozman-2003,
  AUTHOR = {Fabio Gagliardi Cozman},
  NOTE   = {slide printout},
  TITLE  = {Graph-Theoretical Models for Multivariate Modeling with Imprecise Probabilities},
  YEAR   = {2003},
}

@ARTICLE{Cozman-2000-cn,
  ABSTRACT     = {This paper presents a complete theory of credal networks, structures that associate convex sets of probability measures with directed acyclic graphs. Credal networks are graphical models for precise/imprecise beliefs. The main contribution of this work is a theory of credal networks that displays as much flexibility and representational power as the theory of standard Bayesian networks. Results in this paper show how to express judgements of irrelevance and independence, and how to compute inferences in credal networks. A credal network admits several extensions–several sets of probability measures comply with the constraints represented by a network. Two types of extensions are investigated. The properties of strong extensions are clarified through a new generalization of d-separation, and exact and approximate inference methods are described for strong extensions. Novel results are presented for natural extensions, and linear fractional programming methods are described for natural extensions. The paper also investigates credal networks that are defined globally through perturbations of a single network.},
  AUTHOR       = {Fabio Gagliardi Cozman},
  DOI          = {10.1016/S0004-3702(00)00029-1},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Bayesian networks; Convex sets of probability measures; Graphical d-separation relations; Graphical models of inference; Independence relations; Lower and upper expectations; Robust Bayesian analysis},
  LOCALFILE    = {article/Cozman-2000-cn.pdf},
  NUMBER       = {2},
  PAGES        = {199–233},
  TITLE        = {Credal networks},
  VOLUME       = {120},
  YEAR         = {2000},
}

@INPROCEEDINGS{Cozman-Seidenfeld-2009-graphoid,
  ABSTRACT   = {This paper examines definitions of independence for events and variables in the context of full conditional measures; that is, when conditional probability is a primitive notion and conditioning is allowed on null events. Several independence concepts are evaluated with respect to graphoid properties; we show that properties of weak union, contraction and intersection may fail when null events are present. We propose a concept of “full” independence, characterize the form of a full conditional measure under full independence, and suggest how to build a theory of Bayesian networks that accommodates null events.},
  ANNOTATION = {Conference held in Amsterdam, May 2-5, 2007},
  AUTHOR     = {Fabio Gagliardi Cozman and Teddy Seidenfeld},
  BOOKTITLE  = {Foundations of the Formal Sciences VI: Reasoning about Probabilities and Probabilistic Reasoning},
  EDITOR     = {Benedikt Löwe and Eric Pacuit and Jan-Willem Romeijn},
  PUBLISHER  = {College Publications},
  SERIES     = {Studies in Logic},
  TITLE      = {Independence for Full Conditional Measures and their Graphoid Problems},
  YEAR       = {2010},
}

@ARTICLE{Cozman-Walley-2005-graphoid,
  ABSTRACT     = {This paper investigates Walley's concepts of epistemic irrelevance and epistemic independence for imprecise probability models. We study the mathematical properties of irrelevance and independence, and their relation to the graphoid axioms. Examples are given to show that epistemic irrelevance can violate the symmetry, contraction and intersection axioms, that epistemic independence can violate contraction and intersection, and that this accords with informal notions of irrelevance and independence.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Fabio Gagliardi Cozman and Peter Walley},
  DOI          = {10.1007/s10472-005-9004-z},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  LOCALFILE    = {article/Cozman-Walley-2005-graphoid.pdf},
  NUMBER       = {1},
  PAGES        = {173–195},
  TITLE        = {Graphoid properties of epistemic irrelevance and independence},
  VOLUME       = {45},
  YEAR         = {2005},
}

@BOOK{Cramer-1946,
  AUTHOR    = {Harald Cramér},
  PUBLISHER = {Princeton University Press},
  TITLE     = {Mathematical methods of statistics},
  YEAR      = {1946},
}

@ARTICLE{Crossman-CoolenSchrijner-Coolen-2009,
  ABSTRACT     = {This paper concerns discrete-time time-homogeneous birth-death processes on a finite state space, containing a single absorbing state, with interval-valued transition probabilities. As absorption is certain, the quasi-stationary behaviour of the process is studied with the distribution of the process conditional on non-absorption. It is shown that the set of all possible limiting conditional distributions is the set of all possible quasi-stationary distributions. An approximation of the possibly infinite set of conditional distributions at time n is presented, together with an example.},
  AUTHOR       = {Richard J. Crossman and Pauline Coolen-Schrijner and Frank P. A. Coolen},
  DOI          = {10.1080/15598608.2009.10411914},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Absorbing state; Birth-death process; Interval probability; Limiting conditional distribu- tion; Time-homogeneity; Quasi-stationary distribution},
  LOCALFILE    = {article/Crossman-CoolenSchrijner-Coolen-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {103–118},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Time-Homogeneous Birth-Death Processes with Probability Intervals and Absorbing State},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Daboni-1975,
  AUTHOR       = {Luciano Daboni},
  JOURNALTITLE = {Rendiconti di matematica},
  LOCALFILE    = {article/Daboni-1975.pdf},
  PAGES        = {399–412},
  TITLE        = {Caratterizzatione delle successioni (funzioni) completamente monotone in termini di rappresentabilià delle funzioni di sopravvivenza di particolari intervalli scambiabli tra successi (arrivi) contigui},
  VOLUME       = {8},
  YEAR         = {1975},
}

@ARTICLE{Daboni-1953,
  AUTHOR       = {Luciano Daboni},
  JOURNALTITLE = {Giornale dell'Istituto italiano degli attuari},
  LOCALFILE    = {article/Daboni-1953.pdf},
  PAGES        = {58–65},
  TITLE        = {Considerazioni geometriche sulla condizione di equivalenza per una classa di eventi},
  VOLUME       = {16},
  YEAR         = {1953},
}

@ARTICLE{Daniell-1918,
  AUTHOR       = {P. J. Daniell},
  JOURNALTITLE = {The Annals of Mathematics},
  LOCALFILE    = {article/Daniell-1918.pdf},
  NUMBER       = {4},
  PAGES        = {279–294},
  TITLE        = {A general form of integral},
  URL          = {http://www.jstor.org/stable/1967495},
  VOLUME       = {19},
  YEAR         = {1918},
}

@ARTICLE{Danielson-Ekenberg-Riabacke-2009,
  ABSTRACT     = {Most current decision analytical tools and elicitation methods are built on the assumption that decision-makers are able to make their probability and utility assessments in a proper manner. This is, however, often not the case. The specification and execution of elicitation processes are in the majority of cases left to the discretion of the users, not least in user-driven cases such as public information and e-democracy projects. A number of studies have shown, among other things, that people's natural choice behaviour deviates from normative assumptions, and that the results display an inertia gap due to differently framed prospects. One reason for the occurrence of the inertia gap is people's inability to express their preferences as single numbers. Instead of considering this as being a human error, this paper uses the gap in order to develop a class of methods more aligned to the observed behaviour. The core idea of the class is to acknowledge the existence of the gap and, as a consequence, not elicit single point numbers.},
  AUTHOR       = {Mats Danielson and Love Ekenberg and Ari Riabacke},
  DOI          = {10.1080/15598608.2009.10411917},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Decision analysis; Elicitation method; Imprecise information; Interval assessments},
  LOCALFILE    = {article/Danielson-Ekenberg-Riabacke-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {157–168},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {A Prescriptive Approach to Elicitation of Decision Data},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Dantzig-1955,
  AUTHOR       = {George B. Dantzig},
  DOI          = {10.1287/mnsc.1.3-4.197},
  ISSN         = {0025-1909},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Dantzig-1955.pdf},
  NUMBER       = {3/4},
  PAGES        = {197–206},
  PUBLISHER    = {INFORMS},
  TITLE        = {Linear Programming under Uncertainty},
  URL          = {http://www.jstor.org/stable/2627159},
  VOLUME       = {1},
  YEAR         = {1955},
}

@BOOK{Dantzig-Thapa-2003,
  AUTHOR    = {George B. Dantzig and Mukund N. Thapa},
  BOOKTITLE = {Linear Programming:2: Theory and Extensions},
  DOI       = {10.1007/b97283},
  LOCALFILE = {book/Dantzig-Thapa-2003.pdf},
  PUBLISHER = {Springer},
  SERIES    = {Springer Series in Operations Research},
  YEAR      = {2003},
}

@INPROCEEDINGS{Dash-Cooper-2002,
  AUTHOR    = {Denver Dash and Gregory F. Cooper},
  BOOKTITLE = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
  PAGES     = {91–98},
  TITLE     = {Exact model averaging with naive Bayesian classifiers},
  YEAR      = {2002},
}

@BOOK{Davey-Priestley-1990,
  AUTHOR    = {B. A. Davey and H. A. Priestley},
  PUBLISHER = {Cambridge University Press},
  SERIES    = {Cambridge Mathematical Textbooks},
  TITLE     = {Introduction to Lattices and Order},
  YEAR      = {1990},
}

@ARTICLE{Davis-1954,
  AUTHOR       = {Chandler Davis},
  ISSN         = {0002-9327},
  JOURNALTITLE = {American Journal of Mathematics},
  LOCALFILE    = {article/Davis-1954.pdf},
  NUMBER       = {4},
  PAGES        = {733–746},
  PUBLISHER    = {The Johns Hopkins University Press},
  TITLE        = {Theory of positive linear dependence},
  URL          = {http://www.jstor.org/stable/2372648},
  VOLUME       = {76},
  YEAR         = {1954},
}

@THESIS{Davison-2002,
  ABSTRACT    = {User-perceived retrieval latencies in the World Wide Web can be improved by preloading a local cache with resources likely to be accessed. A user requesting content that can be served by the cache is able to avoid the delays inherent in the Web, such as congested networks and slow servers. The difficulty, then, is to determine what content to prefetch into the cache. This work explores machine learning algorithms for user sequence prediction, both in general and speciffically for sequences of Web requests. We also consider information retrieval techniques to allow the use of the content of Web pages to help predict future requests. Although history-based mechanisms can provide strong performance in predicting future requests, performance can be improved by including predictions from additional sources. While past researchers have used a variety of techniques for evaluating caching algorithms and systems, most of those methods were not applicable to the evaluation of prefetching algorithms or systems. Therefore, two new mechanisms for evaluation are introduced. The first is a detailed trace-based simulator, built from scratch, that estimates client-side response times in a simulated network of clients, caches, and Web servers with various connectivity. This simulator is then used to evaluate various prefetching approaches. The second evaluation method presented is a novel architecture to simultaneously evaluate multiple proxy caches in a live network, which we introduce, implement, and demonstrate through experiments. The simulator is appropriate for evaluation of algorithms and research ideas, while simultaneous proxy evaluation is ideally suited to implemented systems. We also consider the present and the future ofWeb prefetching, nding that changes to the HTTP standard will be required in order for Web prefetching to become commonplace.},
  AUTHOR      = {Brian Douglas Davison},
  INSTITUTION = {Rutgers, The State University of New Jersey},
  TITLE       = {The design and evaluation of web prefetching and caching techniques},
  TYPE        = {phdthesis},
  YEAR        = {2002},
}

@ARTICLE{Dawid-1985-symmetry,
  AUTHOR       = {A. Philip Dawid},
  DOI          = {10.1093/bjps},
  JOURNALTITLE = {The British Journal for the Philosophy of Science},
  LOCALFILE    = {article/Dawid-1985-symmetry.pdf},
  NUMBER       = {2},
  PAGES        = {107–128},
  PUBLISHER    = {British Society for the Philosophy of Science},
  TITLE        = {Probability, symmetry and frequency},
  VOLUME       = {36},
  YEAR         = {1985},
}

@INCOLLECTION{Dayhoff-Schwartz-Orcutt-1978,
  AUTHOR    = {M. O. Dayhoff and R. M. Schwartz and B. C. Orcutt},
  BOOKTITLE = {Atlas of Protein Sequence and Structure},
  CHAPTER   = {22},
  EDITOR    = {M. O. Dayhoff},
  LOCALFILE = {inbook/Dayhoff-Schwartz-Orcutt-1978.pdf},
  PAGES     = {345–352},
  PUBLISHER = {National Biomedical Research Foundation},
  TITLE     = {A Model of Evolutionary Change in Proteins},
  YEAR      = {1978},
}

@INPROCEEDINGS{DeBock-DeCooman-2011,
  ABSTRACT  = {We present an efficient exact algorithm for estimating state sequences from outputs (or observations) in imprecise hidden Markov models (iHMM), where both the uncertainty linking one state to the next, and that linking a state to its output, are represented using coherent lower previsions. The notion of independence we associate with the credal network representing the iHMM is that of epistemic irrelevance. We consider as best estimates for state sequences the (Walley–Sen) maximal sequences for the posterior joint state model (conditioned on the observed output sequence), associated with a gain function that is the indicator of the state sequence. This corresponds to (and generalises) finding the state sequence with the highest posterior probability in HMMs with precise transition and output probabilities (pHMMs). We argue that the computational complexity is at worst quadratic in the length of the Markov chain, cubic in the number of states, and essentially linear in the number of maximal state sequences. For binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters.},
  AUTHOR    = {Jasper {De Bock} and Gert de Cooman},
  BOOKTITLE = {ISIPTA'11: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications},
  EDITOR    = {Frank P. A. Coolen and Gert de Cooman and Thomas Fetz and Michael Oberguggenberger},
  PAGES     = {159–168},
  PUBLISHER = {SIPTA},
  TITLE     = {State sequence prediction in imprecise hidden Markov models},
  VENUE     = {Innsbruck, Austria},
  YEAR      = {2011},
}

@MISC{DeCooman-website,
  AUTHOR = {Gert de Cooman},
  TITLE  = {Website of Gert de Cooman},
  URL    = {http://users.ugent.be/~gdcooma},
}

@REPORT{DeCooman-1991-margextconv,
  AUTHOR = {Gert de Cooman},
  TITLE  = {Marginal extension and convexity},
  TYPE   = {techreport},
  YEAR   = {2008},
}

@ARTICLE{DeCooman-2005-order,
  ANNOTATION   = {reprint},
  AUTHOR       = {Gert de Cooman},
  DOI          = {10.1007/s10472-005-9006-x},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  KEYWORDS     = {belief model; belief revision; classical propositional logic; imprecise probability; order theory; possibility measure; system of spheres},
  LOCALFILE    = {article/DeCooman-2005-order.pdf},
  NUMBER       = {1},
  PAGES        = {5–34},
  PUBLISHER    = {Springer},
  TITLE        = {Belief models: An order-theoretic investigation},
  VOLUME       = {45},
  YEAR         = {2005},
}

@MISC{DeCooman-2004-summer,
  ANNOTATION = {Slides voor inleidende presentatie SIPTA summer school},
  AUTHOR     = {Gert de Cooman},
  TITLE      = {Coherent lower and upper previsions (and their behavioural interpretation)},
  YEAR       = {2004},
}

@MISC{DeCooman-2003,
  ANNOTATION = {Transparanten voor ISIPTA '03},
  AUTHOR     = {Gert de Cooman},
  TITLE      = {Theory of Imprecise Probabilities (Basic ideas)},
  YEAR       = {2003},
}

@ARTICLE{DeCooman-2002,
  ABSTRACT     = {Hierarchical models are rather common in uncertainty theory. They arise when there is a ‘correct’ or ‘ideal’ (the so-called first-order) uncertainty model about a phenomenon of interest, but the modeler is uncertain about what it is. The modeler's uncertainty is then called second-order uncertainty. For most of the hierarchical models in the literature, both the first- and the second-order models are precise, i.e., they are based on classical probabilities. In the present paper, I propose a specific hierarchical model that is imprecise at the second level, which means that at this level, lower probabilities are used. No restrictions are imposed on the underlying first-order model: that is allowed to be either precise or imprecise. I argue that this type of hierarchical model generalizes and includes a number of existing uncertainty models, such as imprecise probabilities, Bayesian models, and fuzzy probabilities. The main result of the paper is what I call precision–imprecision equivalence: the implications of the model for decision making and statistical reasoning are the same, whether the underlying first-order model is assumed to be precise or imprecise.},
  ANNOTATION   = {reprint},
  AUTHOR       = {Gert de Cooman},
  DOI          = {10.1016/S0378-3758(01)00209-9},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {coherence; hierarchical uncertainty model; imprecision; natural extension},
  LOCALFILE    = {article/DeCooman-2002.pdf},
  NUMBER       = {1},
  PAGES        = {175–198},
  PUBLISHER    = {Elsevier},
  TITLE        = {Precision-imprecision equivalence in a broad class of imprecise hierarchical uncertainty models},
  VOLUME       = {105},
  YEAR         = {2002},
}

@ARTICLE{DeCooman-2001,
  ABSTRACT     = {The paper discusses integration and some aspects of conditioning in numerical possibility theory, where possibility measures have the behavioural interpretation of upper probabilities, that is, systems of upper betting rates. In such a context, integration can be used to extend upper probabilities to upper previsions. It is argued that the role of the fuzzy integral in this context is limited, as it can only be used to define a coherent upper prevision if the associated upper probability is 0–1-valued, in which case it moreover coincides with the Choquet integral. These results are valid for arbitrary coherent upper probabilities, and therefore also relevant for possibility theory. It follows from the discussion that in a numerical context, the Choquet integral is better suited than the fuzzy integral for producing coherent upper previsions starting from possibility measures. At the same time, alternative expressions for the Choquet integral associated with a possibility measure are derived. Finally, it is shown that a possibility measure is fully conglomerable and satisfies Walley's regularity axiom for conditioning, ensuring that it can be coherently extended to a conditional possibility measure using both the methods of natural and regular extension.},
  AUTHOR       = {Gert de Cooman},
  DOI          = {10.1023/A:1016705331195},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  LOCALFILE    = {article/DeCooman-2001.pdf},
  MONTH        = {08},
  NUMBER       = {1},
  PAGES        = {87–123},
  PUBLISHER    = {Springer},
  TITLE        = {Integration and conditioning in numerical possibility theory},
  VOLUME       = {32},
  YEAR         = {2001},
}

@ARTICLE{DeCooman-1997-postheo1,
  ABSTRACT     = {In this paper, I provide the basis for a measure- and integral-theoretic formulation of possibility theory. It is shown thai, using a general definition of possibility measures, and a generalization of Sugeno's fuzzy integral-the semi-normed fuzzy integral, or possibility integral-. a unified and consistent account can be given of many of the possibilistic results extant in the literature. The striking formal analogy between this treatment of possibility theory, using possibility integrals, and Kolmogorov's measure-theoretic formulation of probability theory, using Lebesgue integrals, is explored and exploited. I introduce and study possibilistic and fuzzy variables as possibilistic counterparts of stochastic and real stochastic variables respeclively, and develop the notion of a possibility distribution for these variables. The almost everywhere equality and dominance of fuzzy variables is defined and studied. The proof is given for a Radon-Nikodym-like theorem in possibility theory. Following the example set by the classical theory of integration, product possibility measures and multiple possibility integrals are introduced, and a Fubini-like theorem is proven. In this way, the groundwork is laid for a unifying measure- and integral-theoretic treatment of conditional possibility and possibilistic independence, discussed in more detail in Parts II and III of this series of three papers.},
  AUTHOR       = {Gert de Cooman},
  DOI          = {10.1080/03081079708945160},
  EPRINT       = {1854/LU-182365},
  EPRINTTYPE   = {hdl},
  JOURNALTITLE = {International Journal of General Systems},
  KEYWORDS     = {Fubini-like theorem; Possibility measure; Radon-Nikodym-like theorem; fuzzy variable; possibilistic variable; possibility distribution; seminormed fuzzy integral},
  LOCALFILE    = {article/DeCooman-1997-postheo1.pdf},
  NUMBER       = {4},
  PAGES        = {291–323},
  TITLE        = {Possibility theory I: the measure- and integral-theoretic groundwork},
  URL          = {http://hdl.handle.net/1854/LU-182365},
  VOLUME       = {25},
  YEAR         = {1997},
}

@ARTICLE{DeCooman-Aeyels-2000,
  ABSTRACT     = {The relationship is studied between possibility and necessity measures defined on arbitrary spaces, the theory of imprecise probabilities, and elementary random set theory. It is shown how special random sets can be used to generate normal possibility and necessity measures, as well as their natural extensions. This leads to interesting alternative formulas for the calculation of these natural extensions},
  AUTHOR       = {Gert de Cooman and Dirk Aeyels},
  DOI          = {10.1109/3468.833093},
  JOURNALTITLE = {IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans},
  LOCALFILE    = {article/DeCooman-Aeyels-2000.pdf},
  NUMBER       = {2},
  PAGES        = {124–130},
  PUBLISHER    = {IEEE},
  TITLE        = {A random set description of a possibility measure and its natural extension},
  VOLUME       = {30},
  YEAR         = {2000},
}

@ARTICLE{DeCooman-Aeyels-1999,
  ABSTRACT     = {We study the relation between possibility measures and the theory of imprecise probabilities, and argue that possibility measures have an important part in this theory. It is shown that a possibility measure is a coherent upper probability if and only if it is normal. A detailed comparison is given between the possibilistic and natural extension of an upper probability, both in the general case and for upper probabilities defined on a class of nested sets. We prove in particular that a possibility measure is the restriction to events of the natural extension of a special kind of upper probability, defined on a class of nested sets. We show that possibilistic extension can be interpreted in terms of natural extension. We also prove that when either the upper or the lower cumulative distribution function of a random quantity is specified, possibility measures very naturally emerge as the corresponding natural extensions. Next, we go from upper probabilities to upper previsions. We show that if a coherent upper prevision defined on the convex cone of all non-negative gambles is supremum preserving, then it must take the form of a Shilkret integral associated with a possibility measure. But at the same time, we show that such a supremum preserving upper prevision is never coherent unless it is the vacuous upper prevision with respect to a non-empty subset of the universe of discourse.},
  AUTHOR       = {Gert de Cooman and Dirk Aeyels},
  DOI          = {10.1016/S0020-0255(99)00007-9},
  ISSN         = {0020-0255},
  JOURNALTITLE = {Information Sciences},
  KEYWORDS     = {choquet integral; coherence; lower cumulative distribution function; natural extension; possibilistic extension; possibility measure; upper cumulative distribution function; upper probability},
  LOCALFILE    = {article/DeCooman-Aeyels-1999.pdf},
  NUMBER       = {1-4},
  PAGES        = {173–212},
  PUBLISHER    = {Elsevier},
  TITLE        = {Supremum preserving upper probabilities},
  VOLUME       = {118},
  YEAR         = {1999},
}

@ARTICLE{DeCooman-Hermans-2008-bridging,
  ABSTRACT     = {We give an overview of two approaches to probability theory where lower and upper probabilities, rather than probabilities, are used: Walley's behavioural theory of imprecise probabilities, and Shafer and Vovk's game-theoretic account of probability. We show that the two theories are more closely related than would be suspected at first sight, and we establish a correspondence between them that (i) has an interesting interpretation, and (ii) allows us to freely import results from one theory into the other. Our approach leads to an account of probability trees and random processes in the framework of Walley's theory. We indicate how our results can be used to reduce the computational complexity of dealing with imprecision in probability trees, and we prove an interesting and quite general version of the weak law of large numbers.},
  AUTHOR       = {Gert de Cooman and Filip Hermans},
  DOI          = {10.1016/j.artint.2008.03.001},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Coherence; Conglomerability; Event tree; Game-theoretic probability; Hoeffding’s inequality; Immediate prediction; Imprecise probabilities; Imprecise probability tree; Law of large numbers; Lower prevision; Markov chain; Prequential Principle; Probability tree; Random process},
  LOCALFILE    = {article/DeCooman-Hermans-2008-bridging.pdf},
  NUMBER       = {11},
  PAGES        = {1400–1427},
  PUBLISHER    = {Elsevier},
  TITLE        = {Imprecise probability trees: Bridging two theories of imprecise probability},
  VOLUME       = {172},
  YEAR         = {2008},
}

@INPROCEEDINGS{DeCooman-etal-ISIPTA09-Markovtrees,
  ABSTRACT     = {We replace strong independence in credal networks with the weaker notion of epistemic irrelevance. Focusing on directed trees, we show how to combine local credal sets into a global model, and we use this to construct and justify an exact message-passing algorithm that computes updated beliefs for a variable in the tree. The algorithm, which is essentially linear in the number of nodes, is formulated entirely in terms of coherent lower previsions. We supply examples of the algorithm's operation, and report an application to on-line character recognition that illustrates the advantages of our model for prediction.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Gert de Cooman and Filip Hermans and Alessandro Antonucci and Marco Zaffalon},
  BOOKTITLE    = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
  EDITOR       = {Thomas Augustin and Frank P. A. Coolen and Serafín Moral and Matthias C. M. Troffaes},
  ORGANIZATION = {SIPTA},
  PAGES        = {149–158},
  TITLE        = {Epistemic irrelevance in credal networks: the case of imprecise Markov trees},
  VENUE        = {Durham, United Kingdom},
  YEAR         = {2009},
}

@INPROCEEDINGS{DeCooman-Miranda-2011,
  AUTHOR    = {Gert de Cooman and Enrique Miranda},
  BOOKTITLE = {ISIPTA '11: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications},
  EDITOR    = {Frank P. A. Coolen and Gert de Cooman and Thomas Fetz and Michael Oberguggenberger},
  LOCALFILE = {inproceedings/DeCooman-Miranda-2011.pdf},
  PAGES     = {169–178},
  PUBLISHER = {SIPTA},
  TITLE     = {Independent natural extension for sets of desirable gambles},
  VENUE     = {Innsbruck, Austria},
  YEAR      = {2011},
}

@INCOLLECTION{DeCooman-Miranda-2007-symmetry,
  ANNOTATION = {in svn-repo},
  AUTHOR     = {Gert de Cooman and Enrique Miranda},
  BOOKTITLE  = {Probability and Inference: Essays in Honor of Henry E. Kyburg, Jr.},
  EDITOR     = {William Harper and Gregory Wheeler},
  LOCATION   = {London},
  PAGES      = {67–149},
  PUBLISHER  = {King's College Publications},
  TITLE      = {Symmetry of models versus models of symmetry},
  YEAR       = {2007},
}

@ARTICLE{DeCooman-Miranda-2012,
  ABSTRACT     = {The results in this paper add useful tools to the theory of sets of desirable gambles, a growing toolbox for reasoning with partial probability assessments. We investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence. We provide formulas for the smallest such joint, called their independent natural extension, and study its main properties. The independent natural extension of maximal coherent sets of desirable gambles allows us to define the strong product of sets of desirable gambles. Finally, we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence. Having such a set of tools that are easily implemented in computer programs is clearly beneficial to fields, like AI, with a clear interest in coherent reasoning under uncertainty using general and robust uncertainty models that require no full specification.},
  AUTHOR       = {Gert de Cooman and Enrique Miranda},
  DOI          = {10.1613/jair.3770},
  JOURNALTITLE = {Journal of Artificial Intelligence Research},
  LOCALFILE    = {article/DeCooman-Miranda-2012.pdf},
  PAGES        = {601–640},
  TITLE        = {Irrelevant and independent natural extension for sets of desirable gambles},
  VOLUME       = {45},
  YEAR         = {2012},
}

@ARTICLE{DeCooman-Miranda-2009,
  ABSTRACT     = {We investigate how to combine marginal assessments about the values that random variables assume separately into a model for the values that they assume jointly, when (i) these marginal assessments are modelled by means of coherent lower previsions and (ii) we have the additional assumption that the random variables are forward epistemically irrelevant to each other. We consider and provide arguments for two possible combinations, namely the forward irrelevant natural extension and the forward irrelevant product, and we study the relationships between them. Our treatment also uncovers an interesting connection between the behavioural theory of coherent lower previsions, and Shafer and Vovk's game-theoretic approach to probability theory.},
  AUTHOR       = {Gert de Cooman and Enrique Miranda},
  DOI          = {10.1016/j.jspi.2008.01.012},
  ISSN         = {0378-3758},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Imprecise probabilities; Coherent lower previsions; Forward irrelevance; Game-theoretic probability; Epistemic irrelevance; Epistemic independence; Event trees},
  LOCALFILE    = {article/DeCooman-Miranda-2009.pdf},
  NUMBER       = {2},
  PAGES        = {256–276},
  TITLE        = {Forward irrelevance},
  VOLUME       = {139},
  YEAR         = {2009},
}

@INPROCEEDINGS{DeCooman-Miranda-2004,
  AUTHOR    = {Gert de Cooman and Enrique Miranda},
  BOOKTITLE = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
  PAGES     = {451–458},
  TITLE     = {A weak law of large numbers for coherent lower previsions},
  YEAR      = {2004},
}

@ARTICLE{De-Cooman-etal-2011,
  ABSTRACT     = {There is no unique extension of the standard notion of probabilistic independence to the case where probabilities are indeterminate or imprecisely specified. Epistemic independence is an extension that formalises the intuitive idea of mutual irrelevance between different sources of information. This gives epistemic independence very wide scope as well as appeal: this interpretation of independence is often taken as natural also in precise-probabilistic contexts. Nevertheless, epistemic independence has received little attention so far. This paper develops the foundations of this notion for variables assuming values in finite spaces. We define (epistemically) independent products of marginals (or possibly conditionals) and show that there always is a unique least-committal such independent product, which we call the independent natural extension. We supply an explicit formula for it, and study some of its properties, such as associativity, marginalisation and external additivity, which are basic tools to work with the independent natural extension. Additionally, we consider a number of ways in which the standard factorisation formula for independence can be generalised to an imprecise-probabilistic context. We show, under some mild conditions, that when the focus is on least-committal models, using the independent natural extension is equivalent to imposing a so-called strong factorisation property. This is an important outcome for applications as it gives a simple tool to make sure that inferences are consistent with epistemic independence judgements. We discuss the potential of our results for applications in Artificial Intelligence by recalling recent work by some of us, where the independent natural extension was applied to graphical models. It has allowed, for the first time, the development of an exact linear-time algorithm for the imprecise probability updating of credal trees.},
  AUTHOR       = {Gert de Cooman and Enrique Miranda and Marco Zaffalon},
  DOI          = {10.1016/j.artint.2011.06.001},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Epistemic irrelevance; Epistemic independence; Independent natural extension; Strong product; Factorisation; Coherent lower previsions},
  LOCALFILE    = {article/De-Cooman-etal-2011.pdf},
  NUMBER       = {12–13},
  PAGES        = {1911–1950},
  TITLE        = {Independent natural extension},
  VOLUME       = {175},
  YEAR         = {2011},
}

@ARTICLE{DeCooman-Troffaes-2004,
  ABSTRACT     = {We discuss why coherent lower previsions provide a good uncertainty model for solving generic uncertainty problems involving possibly conflicting expert information. We study various ways of combining expert assessments on different domains, such as natural extension, independent natural extension and the type-I product, as well as on common domains, such as conjunction and disjunction. We provide each of these with a clear interpretation, and we study how they are related. Observing that in combining expert assessments no information is available about the order in which they should be combined, we suggest that the final result should be independent of the order of combination. The rules of combination we study here satisfy this requirement.},
  ANNOTATION   = {reprint},
  AUTHOR       = {Gert de Cooman and Matthias C. M. Troffaes},
  DOI          = {10.1016/j.ress.2004.03.007},
  JOURNALTITLE = {Reliability Engineering \& System Safety},
  KEYWORDS     = {coherent lower previsions; conjunction; disjunction; expert information; independence; marginal extension; natural extension; type-1 product},
  LOCALFILE    = {article/DeCooman-Troffaes-2004.pdf},
  NUMBER       = {1},
  PAGES        = {113–134},
  PUBLISHER    = {Elsevier},
  TITLE        = {Coherent lower previsions in systems modelling: products and aggregation rules},
  VOLUME       = {85},
  YEAR         = {2004},
}

@ARTICLE{DeCooman-Troffaes-Miranda-2008-exact,
  ABSTRACT     = {We study n-monotone functionals, which constitute a generalisation of n-monotone set functions. We investigate their relation to the concepts of exactness and natural extension, which generalise coherence and natural extension in the behavioural theory of imprecise probabilities. We improve upon a number of results in the literature, and prove among other things a representation result for exact n-monotone functionals in terms of Choquet integrals.},
  AUTHOR       = {Gert de Cooman and Matthias C. M. Troffaes and Enrique Miranda},
  DOI          = {10.1016/j.jmaa.2008.05.071},
  ISSN         = {0022-247X},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  KEYWORDS     = {Choquet integral; coherence; comonotone additivi},
  LOCALFILE    = {article/DeCooman-Troffaes-Miranda-2008-exact.pdf},
  NUMBER       = {1},
  PAGES        = {143–156},
  PUBLISHER    = {Elsevier},
  TITLE        = {n-Monotone exact functionals},
  VOLUME       = {347},
  YEAR         = {2008},
}

@ARTICLE{DeCooman2005a,
  ABSTRACT     = {We study n-monotone lower previsions, which constitute a generalisation of n-monotone lower probabilities. We investigate their relation with the concepts of coherence and natural extension in the behavioural theory of imprecise probabilities, and improve along the way upon a number of results from the literature.},
  AUTHOR       = {Gert de Cooman and Matthias C. M. Troffaes and Enrique Miranda},
  JOURNALTITLE = {Journal of Intelligent and Fuzzy Systems},
  KEYWORDS     = {Choquet integral; coherence; comonotone additivity; n-monotonicity; natural extension},
  LOCALFILE    = {article/DeCooman-Troffaes-Miranda-2005-Kerre.pdf},
  NUMBER       = {4},
  PAGES        = {253–263},
  PUBLISHER    = {IOS Press},
  TITLE        = {n-Monotone lower previsions and lower integrals},
  URL          = {http://iospress.metapress.com/content/22bh7djyjk86a55h},
  VOLUME       = {16},
  YEAR         = {2005},
}

@ARTICLE{DeCooman-Zaffalon-2004-incomplete,
  ABSTRACT     = {Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete (or set-valued). This is a fundamental problem in general, and of particular interest for Bayesian networks. Recently, Grünwald and Halpern have shown that commonly used updating strategies fail in this case, except under very special assumptions. In this paper we propose a new method for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no assumptions about the so-called incompleteness mechanism that associates complete with incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we use only coherence arguments to turn prior into posterior (updated) probabilities. In general, this new approach to updating produces lower and upper posterior probabilities and previsions (expectations), as well as partially determinate decisions. This is a logical consequence of the existing ignorance about the incompleteness mechanism. As an example, we use the new updating method to properly address the apparent paradox in the [`]Monty Hall' puzzle. More importantly, we apply it to the problem of classification of new evidence in probabilistic expert systems, where it leads to a new, so-called conservative updating rule. In the special case of Bayesian networks constructed using expert knowledge, we provide an exact algorithm to compare classes based on our updating rule, which has linear-time complexity for a class of networks wider than polytrees. This result is then extended to the more general framework of credal networks, where computations are often much harder than with Bayesian nets. Using an example, we show that our rule appears to provide a solid basis for reliable updating with incomplete observations, when no strong assumptions about the incompleteness mechanism are justified.},
  AUTHOR       = {Gert de Cooman and Marco Zaffalon},
  DOI          = {10.1016/j.artint.2004.05.006},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Incomplete observations; Updating probabilities},
  LOCALFILE    = {article/DeCooman-Zaffalon-2004-incomplete.pdf},
  NUMBER       = {1-2},
  PAGES        = {75–125},
  PUBLISHER    = {Elsevier},
  TITLE        = {Updating beliefs with incomplete observations},
  VOLUME       = {159},
  YEAR         = {2004},
}

@THESIS{DeMunck-2009-PhD,
  AUTHOR      = {Maarten {De Munck}},
  INSTITUTION = {Katholieke Universiteit Leuven},
  MONTH       = {04},
  TITLE       = {Efficient optimization approaches for interval and fuzzy finite element analysis},
  TYPE        = {phdthesis},
  YEAR        = {2009},
}

@INCOLLECTION{Deb-2005,
  AUTHOR    = {Kalyanmoy Deb},
  BOOKTITLE = {Search Methodologies},
  CHAPTER   = {10},
  DOI       = {10.1007/0-387-28356-0_10},
  EDITOR    = {Edmund K. Burke and Graham Kendall},
  ISBN      = {978-0-387-23460-1},
  PAGES     = {273–316},
  PUBLISHER = {Springer US},
  TITLE     = {Multi-Objective Optimization},
  YEAR      = {2005},
}

@THESIS{Degrauwe-2007-PhD,
  AUTHOR      = {Daan Degrauwe},
  INSTITUTION = {Katholieke Universiteit Leuven},
  TITLE       = {Uncertainty propagation in structural analysis by fuzzy numbers},
  TYPE        = {phdthesis},
  YEAR        = {2007},
}

@BOOK{DeGroot-2004,
  ANNOTATION = {Wiley Classics Library Edition},
  AUTHOR     = {Morris H. DeGroot},
  PUBLISHER  = {Wiley},
  TITLE      = {Optimal Statistical Decisions},
  YEAR       = {2004},
}

@ARTICLE{Delbaen-1974,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Freddy Delbaen},
  DOI          = {10.1016/0022-247X(74)90133-4},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  LOCALFILE    = {article/Delbaen-1974.pdf},
  NUMBER       = {1},
  PAGES        = {210–233},
  PUBLISHER    = {Elsevier},
  TITLE        = {Convex games and extreme points},
  VOLUME       = {45},
  YEAR         = {1974},
}

@INPROCEEDINGS{Delort-BouchonMeunier-2000,
  AUTHOR    = {Jean-Yves Delort and Bernadette Bouchon-Meunier},
  BOOKTITLE = {Proceedings of the Eleventh International World Wide Web Conference},
  TITLE     = {Facing Uncertainty in Link Recommender Systems},
  YEAR      = {2002},
}

@ARTICLE{Dempster-1968,
  ABSTRACT     = {Procedures of statistical inference are described which generalize Bayesian inference in specific ways. Probability is used in such a way that in general only bounds may be placed on the probabilities of given events, and probability systems of this kind are suggested both for sample information and for prior information. These systems are then combined using a specified rule. Illustrations are given for inferences about trinomial probabilities, and for inferences about a monotone sequence of binomial p\_i. Finally, some comments are made on the general class of models which produce upper and lower probabilities, and on the specific models which underlie the suggested inference procedures.},
  ANNOTATION   = {with discussion},
  AUTHOR       = {Arthur P. Dempster},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Dempster-1968.pdf},
  NUMBER       = {2},
  PAGES        = {205–247},
  TITLE        = {A generalization of Bayesian inference},
  URL          = {http://www.jstor.org/stable/2984504},
  VOLUME       = {30},
  YEAR         = {1968},
}

@ARTICLE{Dempster-1967,
  ABSTRACT     = {A multivalued mapping from a space X to a space S carries a probability measure defined over subsets of X into a system of upper and lower probabilities over subsets of S. Some basic properties of such systems are explored in Sections 1 and 2. Other approaches to upper and lower probabilities are possible and some of these are related to the present approach in Section 3. A distinctive feature of the present approach is a rule for conditioning, or more generally, a rule for combining sources of information, as discussed in Sections 4 and 5. Finally, the context in statistical inference from which the present theory arose is sketched briefly in Section 6.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Arthur P. Dempster},
  JOURNALTITLE = {The Annals of Mathematical Statistics},
  LOCALFILE    = {article/Dempster-1967.pdf},
  NUMBER       = {2},
  PAGES        = {325–339},
  PUBLISHER    = {Springer},
  TITLE        = {Upper and lower probabilities induced by a multivalued mapping},
  URL          = {http://www.jstor.org/stable/2239146},
  VOLUME       = {38},
  YEAR         = {1967},
}

@ARTICLE{Derriennic-1985,
  AUTHOR       = {Marie-Madeleine Derriennic},
  JOURNALTITLE = {Journal of Approximation Theory},
  PAGES        = {155–166},
  TITLE        = {On Multivariate Approximation by Bernstein-Type Polynomials},
  VOLUME       = {45},
  YEAR         = {1985},
}

@REPORT{Deshpande-Karypis-2000,
  AUTHOR      = {Mukund Deshpande and George Karypis},
  INSTITUTION = {University of Minnesota, Department of Computer Science},
  NUMBER      = {00-056},
  TITLE       = {Selective Markov Models for Predicting Web-Page Accesses},
  TYPE        = {techreport},
  YEAR        = {2000},
}

@ARTICLE{Destercke-etal-2008-unifying1,
  AUTHOR       = {Sébastien Destercke and Didier Dubois and E. Chojnacki},
  DOI          = {10.1016/j.ijar.2008.07.003},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Destercke-etal-2008-unifying1.pdf},
  PAGES        = {649–663},
  TITLE        = {Unifying practical uncertainty representations: I. Generalized p-boxes},
  VOLUME       = {49},
  YEAR         = {2008},
}

@ARTICLE{Destercke-etal-2008-unifying2,
  ABSTRACT     = {There exist many simple tools for jointly capturing variability and incomplete information by means of uncertainty representations. Among them are random sets, possibility distributions, probability intervals, and the more recent Ferson’s p-boxes and Neumaier’s clouds, both defined by pairs of possibility distributions. In the companion paper, we have extensively studied a generalized form of p-box and situated it with respect to other models. This paper focuses on the links between clouds and other representations. Generalized p-boxes are shown to be clouds with comonotonic distributions. In general, clouds cannot always be represented by random sets, in fact not even by two-monotone (convex) capacities.},
  AUTHOR       = {Sébastien Destercke and Didier Dubois and E. Chojnacki},
  DOI          = {10.1016/j.ijar.2008.07.004},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Imprecise probability representations; p-Boxes; Possibility theory; Random sets; Clouds; Probability intervals},
  LOCALFILE    = {article/Destercke-etal-2008-unifying2.pdf},
  NUMBER       = {3},
  PAGES        = {664–677},
  TITLE        = {Unifying practical uncertainty representations. II: Clouds},
  VOLUME       = {49},
  YEAR         = {2008},
}

@THESIS{Dhaenens-2007,
  AUTHOR      = {Stefaan Dhaenens},
  EPRINT      = {1854/LU-470245},
  EPRINTTYPE  = {hdl},
  INSTITUTION = {Universiteit Gent},
  LOCALFILE   = {mastersthesis/Dhaenens-2007.pdf},
  TITLE       = {Onderzoek van Imprecieze Markov-modellen},
  TYPE        = {mathesis},
  URL         = {http://hdl.handle.net/1854/LU-470245},
  YEAR        = {2007},
}

@REPORT{Dhillon-Sra-2003-directional,
  ABSTRACT    = {Traditionally multi-variate normal distributions have been the staple of data modeling in most domains. For some domains, the model they provide is either inadequate or incorrect because of the disregard for the directional components of the data. We present a generative model for data that is suitable for modeling directional data (as can arise in text and gene expression clustering). We use mixtures of von Mises-Fisher distributions to model our data since the von Mises-Fisher distribution is the natural distribution for directional data. We derive an Expectation Maximization (EM) algorithm to find the maximum likelihood estimates for the parameters of our mixture model, and provide various experimental results to evaluate the “correctness” of our formulation. In this paper we also provide some of the mathematical background necessary to carry out all the derivations and to gain insight for an implementation.},
  AUTHOR      = {Inderjit S. Dhillon and Sra Suvrit},
  INSTITUTION = {The University of Texas at Austin},
  LOCATION    = {Austin, Texas},
  NUMBER      = {TR-03-06},
  TITLE       = {Modeling Data using Directional Distributions},
  TYPE        = {techreport},
  URL         = {http://www.cs.utexas.edu/~suvrit/work/research.html},
  YEAR        = {2003},
}

@ARTICLE{Diaconis-Freedman-1980-partial-xch,
  AUTHOR       = {Persi Diaconis and D. Freedman},
  DOI          = {10.1214/aop/1176994828},
  JOURNALTITLE = {The Annals of Probability},
  LOCALFILE    = {article/Diaconis-Freedman-1980-partial-xch.pdf},
  NUMBER       = {1},
  PAGES        = {115–130},
  TITLE        = {De Finetti's theorem for Markov chains},
  URL          = {http://www.jstor.org/stable/2243063},
  VOLUME       = {8},
  YEAR         = {1980},
}

@ARTICLE{Diaconis-Freedman-1982-exchangeability,
  ABSTRACT     = {Let X\_1, X\_2,⋯, X\_k, X\_{k+1},⋯, X\_n be exchangeable random variables taking values in the set S. The variation distance between the distribution of X\_1, X\_2,⋯, X\_k and the closest mixture of independent, identically distributed random variables is shown to be at most 2 ck/n, where c is the cardinality of S. If c is infinite, the bound k(k - 1)/n is obtained. These results imply the most general known forms of de Finetti's theorem. Examples are given to show that the rates k/n and k(k - 1)/n cannot be improved. The main tool is a bound on the variation distance between sampling with and without replacement. For instance, suppose an urn contains n balls, each marked with some element of the set S, whose cardinality c is finite. Now k draws are made at random from this urn, either with or without replacement. This generates two probability distributions on the set of k-tuples, and the variation distance between them is at most 2 ck/n.},
  AUTHOR       = {Persi Diaconis and D. Freedman},
  JOURNALTITLE = {The Annals of Probability},
  LOCALFILE    = {article/Diaconis-Freedman-1982-exchangeability.pdf},
  NUMBER       = {4},
  PAGES        = {745–764},
  TITLE        = {Finite exchangeable sequences},
  URL          = {http://www.jstor.org/stable/2242823},
  VOLUME       = {8},
  YEAR         = {1980},
}

@ARTICLE{Diaconis-Ylvisaker-1979,
  ABSTRACT     = {Let X be a random vector distributed according to an exponential family with natural parameter θ∈Θ. We characterize conjugate prior measures on Θ through the property of linear posterior expectation of the mean parameter of X: E{E(X|θ)|X = x} = ax + b. We also delineate which hyperparameters permit such conjugate priors to be proper.},
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Persi Diaconis and Donald Ylvisaker},
  DOI          = {10.1214/aos/1176344611},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {Bayes; conjugate priors; linearity of regression},
  LOCALFILE    = {article/Diaconis-Ylvisaker-1979.pdf},
  NUMBER       = {2},
  PAGES        = {269–281},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Conjugate priors for exponential families},
  VOLUME       = {7},
  YEAR         = {1979},
}

@ARTICLE{Diaconis-Zabell-1982,
  AUTHOR       = {Persi Diaconis and Sandy L. Zabell},
  ISSN         = {0162-1459},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Diaconis-Zabell-1982.pdf},
  NUMBER       = {380},
  PAGES        = {822–830},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {Updating subjective probability},
  URL          = {http://www.jstor.org/stable/2287313},
  VOLUME       = {77},
  YEAR         = {1982},
}

@ARTICLE{Dietzenbacher-1994-Perron-Frobenius,
  ABSTRACT     = {The dominant eigenvalue and the corresponding eigenvector (or Perron vector) of a non-linear eigensystem are considered. We discuss the effects upon these, of perturbations and of aggregation of the underlying mapping. The results are applied to study the sensitivity of the outputs in a non-linear input-output model. For that purpose, it is shown that the input-output model can be rewritten as a non-linear eigensystem. It turns out that the Perron vector of this eigensystem includes the solution vector of the input-output model.},
  AUTHOR       = {Erik Dietzenbacher},
  DOI          = {10.1016/0304-4068(94)90033-7},
  JOURNALTITLE = {Journal of Mathematical Economics},
  LOCALFILE    = {article/Dietzenbacher-1994-Perron-Frobenius.pdf},
  NUMBER       = {1},
  PAGES        = {21–31},
  PUBLISHER    = {Elsevier},
  TITLE        = {The non-linear Perron-Frobenius theorem: Perturbations and aggregation},
  VOLUME       = {23},
  YEAR         = {1994},
}

@ARTICLE{Domingos-Pazzani-1997,
  ABSTRACT     = {The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.},
  AUTHOR       = {Pedro Domingos and Michael Pazzani},
  DOI          = {10.1023/A:1007413511361},
  JOURNALTITLE = {Machine Learning},
  KEYWORDS     = {induction; naive bayesian classifier; optimal classification; simple bayesian classifier; with attribute dependences; zero-one loss},
  LOCALFILE    = {article/Domingos-Pazzani-1997.pdf},
  NUMBER       = {2},
  PAGES        = {103–130},
  TITLE        = {On the optimality of the simple Bayesian classifier under zero-one loss},
  VOLUME       = {29},
  YEAR         = {1997},
}

@ARTICLE{Dose-2007-gravity,
  AUTHOR       = {Volker Dose},
  DOI          = {10.1088/0957-0233},
  JOURNALTITLE = {Measurement Science and Technology},
  KEYWORDS     = {gravitational constant; robust estimation},
  LOCALFILE    = {article/Dose-2007-gravity.pdf},
  PAGES        = {176–182},
  TITLE        = {Bayesian estimate of the Newtonian constant of gravitation},
  VOLUME       = {18},
  YEAR         = {2007},
}

@ARTICLE{Boumont-2002,
  AUTHOR       = {Jean-luc Doumont},
  DOI          = {10.1109/TPC.2002.805164},
  JOURNALTITLE = {IEEE Transactions on Professional Communication},
  LOCALFILE    = {article/Doumont-2002.pdf},
  MONTH        = {12},
  NUMBER       = {4},
  PAGES        = {291–296},
  TITLE        = {The three laws of professional communication},
  VOLUME       = {45},
  YEAR         = {2002},
}

@MISC{Doumont-2001-feedback,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Giving Feedback},
  YEAR   = {2001},
}

@MISC{Doumont-2001-fundamentals,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Fundamentals},
  YEAR   = {2001},
}

@MISC{Doumont-2001-graphing,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Graphing data},
  YEAR   = {2001},
}

@MISC{Doumont-2001-internet,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {How Internet works},
  YEAR   = {2001},
}

@MISC{Doumont-2001-persuading,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Persuading others},
  YEAR   = {2001},
}

@MISC{Doumont-2001-speaking,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Speaking in public},
  YEAR   = {2001},
}

@MISC{Doumont-2001-training,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Training others},
  YEAR   = {2001},
}

@MISC{Doumont-2001-writing,
  AUTHOR = {Jean-luc Doumont},
  TITLE  = {Writing documents},
  YEAR   = {2001},
}

@INPROCEEDINGS{Druzdzel-VanderGaag-1995,
  AUTHOR    = {Marek J. Druzdzel and Linda C. van der Gaag},
  BOOKTITLE = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
  EDITOR    = {Philippe Besnard and Steve Hanks},
  PAGES     = {141–148},
  PUBLISHER = {Morgan Kaufmann Publishers, San Francisco},
  TITLE     = {Elicitation of probabilities for belief networks: Combining qualitative and quantitative information},
  YEAR      = {1995},
}

@INPROCEEDINGS{Druzdzel-Henrion-1993,
  AUTHOR    = {Marek J. Druzdzel and M. Henrion},
  BOOKTITLE = {Proceedings of the Eleventh National Conference on Artificial Intelligence},
  PAGES     = {548 – 553},
  PUBLISHER = {AAAI Press, Menlo Park, California},
  TITLE     = {Efficient Reasoning in Qualitative Probabilistic Networks},
  YEAR      = {1993},
}

@ARTICLE{Dubois-2006,
  ABSTRACT     = {Numerical possibility distributions can encode special convex families of probability measures. The connection between possibility theory and probability theory is potentially fruitful in the scope of statistical reasoning when uncertainty due to variability of observations should be distinguished from uncertainty due to incomplete information. This paper proposes an overview of numerical possibility theory. Its aim is to show that some notions in statistics are naturally interpreted in the language of this theory. First, probabilistic inequalites (like Chebychev's) offer a natural setting for devising possibility distributions from poor probabilistic information. Moreover, likelihood functions obey the laws of possibility theory when no prior probability is available. Possibility distributions also generalize the notion of confidence or prediction intervals, shedding some light on the role of the mode of asymmetric probability densities in the derivation of maximally informative interval substitutes of probabilistic information. Finally, the simulation of fuzzy sets comes down to selecting a probabilistic representation of a possibility distribution, which coincides with the Shapley value of the corresponding consonant capacity. This selection process is in agreement with Laplace indifference principle and is closely connected with the mean interval of a fuzzy interval. It sheds light on the “defuzzification” process in fuzzy set theory and provides a natural definition of a subjective possibility distribution that sticks to the Bayesian framework of exchangeable bets. Potential applications to risk assessment are pointed out.},
  AUTHOR       = {Didier Dubois},
  DOI          = {10.1016/j.csda.2006.04.015},
  ISSN         = {0167-9473},
  JOURNALTITLE = {Computational Statistics \& Data Analysis},
  KEYWORDS     = {Possibility theory; Imprecise probability; Confidence intervals; Uncertainty propagation},
  LOCALFILE    = {article/Dubois-2006.pdf},
  NUMBER       = {1},
  PAGES        = {47–69},
  TITLE        = {Possibility theory and statistical reasoning},
  VOLUME       = {51},
  YEAR         = {2006},
}

@ARTICLE{Dubois-Guyonnet-2011,
  AUTHOR       = {Didier Dubois and Dominique Guyonnet},
  DOI          = {10.1080/03081079.2010.506179},
  JOURNALTITLE = {International Journal of General Systems},
  LOCALFILE    = {article/Dubois-Guyonnet-2011.pdf},
  NUMBER       = {2},
  PAGES        = {145–167},
  TITLE        = {Risk-informed decision-making in the presence of epistemic uncertainty},
  VOLUME       = {40},
  YEAR         = {2011},
}

@BOOK{Dubois-Prade-1988,
  AUTHOR    = {Didier Dubois and Henri Prade},
  EDITION   = {2},
  LOCATION  = {Paris},
  PUBLISHER = {Masson},
  TITLE     = {Théorie des possibilités: applications à la représentation des connaissances en informatique},
  YEAR      = {1988},
}

@BOOK{Dubois-Prade-1988-en,
  AUTHOR    = {Didier Dubois and Henri Prade},
  LOCATION  = {New York},
  PUBLISHER = {Plenum Press},
  TITLE     = {Possibility Theory: An Approach to Computerized Processing of Uncertainty},
  YEAR      = {1988},
}

@ARTICLE{Dubois-Prade-Sabbadin-2001,
  AUTHOR       = {Didier Dubois and Henri Prade and Régis Sabbadin},
  DOI          = {10.1016/S0377-2217(99)00473-7},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {decision theory; possibility theory; uncertainty},
  LOCALFILE    = {article/Dubois-Prade-Sabbadin-2001.pdf},
  PAGES        = {459–478},
  TITLE        = {Decision-theoretic foundations of qualitative possibility theory},
  VOLUME       = {128},
  YEAR         = {2001},
}

@BOOK{Dunford-Schwartz-1958,
  AUTHOR    = {Nelson Dunford and Jacob T. Schwartz},
  LOCATION  = {New York},
  PUBLISHER = {Interscience Publishers},
  SERIES    = {Pure and Applied Mathematics},
  TITLE     = {Linear Operators Part I},
  VOLUME    = {VII},
  YEAR      = {1958},
}

@BOOK{Durbin-etal-1998-seqalign,
  AUTHOR    = {R. Durbin and S. R. Eddy and Anders Krogh and G. Mitchison},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Biological sequence analysis: probabilistic models of proteins and nucleic acids},
  YEAR      = {1998},
}

@ARTICLE{Dyer-1983,
  ABSTRACT     = {The computational complexity of problems relating to the enumeration of all the vertices of a convex polyhedron defined by linear inequalities is examined. Several published approaches are evaluated in this light. A new algorithm is described, and shown to have a better complexity estimate than existing methods. Empirical evidence supporting the theoretical superiority is presented. Finally vertex enumeration is discussed when the space containing the polyhedra is of fixed dimension and only the size of the inequality system is permitted to vary.},
  AUTHOR       = {M. E. Dyer},
  ISSN         = {0364-765X},
  JOURNALTITLE = {Mathematics of Operations Research},
  LOCALFILE    = {article/Dyer-1983.pdf},
  NUMBER       = {3},
  PAGES        = {381–402},
  PUBLISHER    = {INFORMS},
  TITLE        = {The complexity of vertex enumeration methods},
  URL          = {http://www.jstor.org/stable/3689308},
  VOLUME       = {8},
  YEAR         = {1983},
}

@BOOK{Eckel-2000-C++V1,
  AUTHOR    = {Bruce Eckel},
  EDITION   = {2},
  PUBLISHER = {Prentice Hall},
  TITLE     = {Thinking in C++},
  VOLUME    = {1},
  YEAR      = {2000},
}

@BOOK{Eckel-2000-C++V2,
  AUTHOR    = {Bruce Eckel},
  EDITION   = {2},
  PUBLISHER = {Prentice Hall},
  TITLE     = {Thinking in C++},
  VOLUME    = {2},
  YEAR      = {2000},
}

@ARTICLE{Edwards-1983-Pascal,
  AUTHOR       = {A. W. F. Edwards},
  JOURNALTITLE = {International Statistical Review},
  LOCALFILE    = {article/Edwards-1983-Pascal.pdf},
  PAGES        = {73–79},
  TITLE        = {Pascal's Problem: The ‘Gambler's Ruin’},
  URL          = {http://www.jstor.org/stable/1402732},
  VOLUME       = {51},
  YEAR         = {1983},
}

@ARTICLE{Efron-1978-expfam,
  ABSTRACT     = {There are two important spaces connected with every multivariate exponential family, the natural parameter space and the expectation parameter space. We describe some geometric results relating the two. (In the simplest case, that of a normal translation family, the two spaces coincide and the geometry is the familiar Euclidean one.) Maximum likelihood estimation, within one-parameter curved subfamilies of the multivariate family, has two simple and useful geometric interpretations. The geometry also relates to the Fisherian question: to what extent can the Fisher information be replaced by -\partial^2/\partialθ^2\lbrack\log f\_θ(x)\rbrack\mid\_{θ=\hat{θ}} in the variance bound for \hat{θ}, the maximum likelihood estimator?},
  AUTHOR       = {Bradley Efron},
  DOI          = {10.1214/aos/1176344130},
  ISSN         = {0090-5364},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {Curvature; Kullback-Leibler distance; duality; maximum likelihood estimation},
  LOCALFILE    = {article/Efron-1978-expfam.pdf},
  MONTH        = {03},
  NUMBER       = {2},
  PAGES        = {362–376},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {The geometry of exponential families},
  VOLUME       = {6},
  YEAR         = {1978},
}

@BOOK{Ehrgott-2005,
  AUTHOR    = {Matthias Ehrgott},
  EDITION   = {2},
  LOCALFILE = {book/Ehrgott-2005.pdf},
  PUBLISHER = {Springer},
  TITLE     = {Multicriteria optimization},
  YEAR      = {2005},
}

@ARTICLE{Ehrgott-Lohne-Shao-2012,
  AUTHOR       = {Matthias Ehrgott and Andreas Löhne and Lizhen Shao},
  DOI          = {10.1007/s10898-011-9709-y},
  ISSN         = {0925-5001},
  JOURNALTITLE = {Journal of Global Optimization},
  KEYWORDS     = {Multiobjective optimization; Vector optimization; Linear programming; Duality; Objective space; Outer approximation},
  LOCALFILE    = {article/Ehrgott-Lohne-Shao-2012.pdf},
  NUMBER       = {4},
  PAGES        = {757–778},
  PUBLISHER    = {Springer US},
  TITLE        = {A dual variant of Benson’s “outer approximation algorithm” for multiple objective linear programming},
  VOLUME       = {52},
  YEAR         = {2012},
}

@ARTICLE{Ehrgott-Puerto-RodriguezChia-2007,
  AUTHOR       = {Matthias Ehrgott and J. Puerto and A.M. Rodríguez-Chía},
  DOI          = {10.1007/s10957-007-9232-y},
  ISSN         = {0022-3239},
  JOURNALTITLE = {Journal of Optimization Theory and Applications},
  KEYWORDS     = {Multiobjective linear programming; Primal-dual simplex algorithm},
  LOCALFILE    = {article/Ehrgott-Puerto-RodriguezChia-2007.pdf},
  NUMBER       = {3},
  PAGES        = {483–497},
  PUBLISHER    = {Springer US},
  TITLE        = {Primal-Dual Simplex Method for Multiobjective Linear Programming},
  VOLUME       = {134},
  YEAR         = {2007},
}

@ARTICLE{Ehrgott-TenfeldePodehl-2003,
  ABSTRACT     = {In this paper we investigate the problem of finding the Nadir point for multicriteria optimization problems (MOP). The Nadir point is characterized by the componentwise maximal values of efficient points for MOP. It can be easily computed in the bicriteria case. However, in general this problem is very difficult. We review some existing methods and heuristics and also discuss some new ones. We propose a general method to compute Nadir values based on theoretical results on Pareto optimal solutions of subproblems with fewer criteria. The general scheme is valid for any number of criteria and practical for the case of three objectives. We also investigate the use of the Nadir point for compromise programming, when the goal is to be as far away as possible from the worst outcomes. We prove some results about (weak) Pareto optimality of the resulting solutions and present examples, which show the limitations of this idea, and therefore limitations of multicriteria methods using this type of problems.},
  AUTHOR       = {Matthias Ehrgott and Dagmar Tenfelde-Podehl},
  DOI          = {10.1016/S0377-2217(02)00595-7},
  ISSN         = {0377-2217},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {Multicriteria optimization; Nadir points; Compromise programming},
  LOCALFILE    = {article/Ehrgott-TenfeldePodehl-2003.pdf},
  NUMBER       = {1},
  PAGES        = {119–139},
  TITLE        = {Computation of ideal and nadir values and implications for their use in MCDM methods},
  VOLUME       = {151},
  YEAR         = {2003},
}

@ARTICLE{Ellis-1952,
  AUTHOR       = {J. W. Ellis},
  DOI          = {10.1215/S0012-7094-52-01941-8},
  JOURNALTITLE = {Duke Mathematical Journal},
  LOCALFILE    = {article/Ellis-1952.pdf},
  NUMBER       = {3},
  PAGES        = {417–421},
  TITLE        = {A general set-separation theorem},
  VOLUME       = {19},
  YEAR         = {1952},
}

@ARTICLE{Epstein-Seo-2010,
  ABSTRACT     = {The de Finetti Theorem is a cornerstone of the Bayesian approach. Bernardo (1996, p. 5) writes that its “message is very clear: if a sequence of observations is judged to be exchangeable, then any subset of them must be regarded as a random sample from some model, and there exists a prior distribution on the parameter of such model, hence requiring a Bayesian approach.” We argue that although exchangeability, interpreted as symmetry of evidence, is a weak assumption, when combined with subjective expected utility theory, it also implies complete confidence that experiments are identical. When evidence is sparse and there is little evidence of symmetry, this implication of de Finetti's hypotheses is not intuitive. This motivates our adoption of multiple-priors utility as the benchmark model of preference. We provide two alternative generalizations of the de Finetti Theorem for this framework. A model of updating is also provided.},
  AUTHOR       = {Larry G. Epstein and Kyoungwon Seo},
  DOI          = {10.3982/TE596},
  ISSN         = {1555-7561},
  JOURNALTITLE = {Theoretical Economics},
  KEYWORDS     = {ambiguity; exchangeability; learning; multiple priors; symmetry; updating},
  LOCALFILE    = {article/Epstein-Seo-2010.pdf},
  NUMBER       = {3},
  PAGES        = {313–368},
  PUBLISHER    = {Blackwell Publishing Ltd},
  TITLE        = {Symmetry of evidence without evidence of symmetry},
  VOLUME       = {5},
  YEAR         = {2010},
}

@ARTICLE{Ericson-1969,
  ABSTRACT     = {Scheffe (1958) introduced the simplex-lattice design for experiments with mixtures of q components. The purpose of this design is the empirical prediction of the response to any mixture of the components when the response depends only on the proportions of the components but not on the total amount of the mixture. In this paper an alternative to the simplex-lattice design is developed in which all the features of the design are maintained except that the pure mixtures are replaced by the (q - 1)-nary mixtures.},
  ANNOTATION   = {with discussion},
  AUTHOR       = {W. A. Ericson},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  NUMBER       = {2},
  PAGES        = {195–233},
  TITLE        = {Subjective Bayesian Models in Sampling Finite Populations},
  URL          = {http://links.jstor.org/sici?sici=0035-9246(1969)31:2%3C195:SBMISF%3E2.0.CO},
  VOLUME       = {31},
  YEAR         = {1969},
}

@ARTICLE{Evans-Steuer-1973,
  ABSTRACT     = {For linear multiple-objective problems, a necessary and sufficient condition for a point to be efficient is employed in the development of a revised simplex algorithm for the enumeration of the set of efficient extreme points. Five options within this algorithm were tested on a variety of problems. Results of these tests provide indications for effective use of the algorithm.},
  AUTHOR       = {J. P. Evans and Ralph E. Steuer},
  DOI          = {10.1007/BF01580111},
  ISSN         = {0025-5610},
  JOURNALTITLE = {Mathematical Programming},
  LOCALFILE    = {article/Evans-Steuer-1973.pdf},
  NUMBER       = {1},
  PAGES        = {54–72},
  PUBLISHER    = {Springer},
  TITLE        = {A revised simplex method for linear multiple objective programs},
  VOLUME       = {5},
  YEAR         = {1973},
}

@ARTICLE{Fagiuoli-Zaffalon-1998-2U,
  ABSTRACT     = {This paper addresses the problem of computing posterior probabilities in a discrete Bayesian network where the conditional distributions of the model belong to convex sets. The computation on a general Bayesian network with convex sets of conditional distributions is formalized as a global optimization problem. It is shown that such a problem can be reduced to a combinatorial problem, suitable to exact algorithmic solutions. An exact propagation algorithm for the updating of a polytree with binary variables is derived. The overall complexity is linear to the size of the network, when the maximum number of parents is fixed.},
  AUTHOR       = {Enrico Fagiuoli and Marco Zaffalon},
  DOI          = {10.1016/S0004-3702(98)00089-7},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Bayesian networks; convex sets; credal sets; intervals; uncertain reasoning},
  LOCALFILE    = {article/Fagiuoli-Zaffalon-1998-2U.pdf},
  MONTH        = {11},
  NUMBER       = {1},
  PAGES        = {77–107},
  PUBLISHER    = {Elsevier},
  TITLE        = {2U: an exact interval propagation algorithm for polytrees with binary variables},
  VOLUME       = {106},
  YEAR         = {1998},
}

@ARTICLE{Farrow-Goldstein-2009,
  ABSTRACT     = {We develop methods for analysing decision problems based on multi-attribute utility hierarchies, structured by mutual utility independence, which are not precisely specified due to unwillingness or inability of an individual or group to agree on precise values for the trade-offs between the various attributes. Instead, our analysis is based on whatever limited collection of preferences we may assert between attribute collections. These preferences identify a class of Pareto optimal decisions. We show how to reduce the class further by combining rules which are almost equivalent and introduce general principles appropriate to selecting decisions in an imprecise hierarchy. The approach is illustrated by the design of a university course module.},
  AUTHOR       = {M. Farrow and Michael Goldstein},
  DOI          = {10.1080/15598608.2009.10411916},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Imprecise utilities; Mutual utility independence; Pareto optimality; Robust decisions; Utility hierarchies},
  LOCALFILE    = {article/Farrow-Goldstein-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {137–155},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Almost-Pareto Decision Sets in Imprecise Utility Hierarchies},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Feelders-VanderGaag-2006,
  ABSTRACT     = {We consider the problem of learning the parameters of a Bayesian network from data, while taking into account prior knowledge about the signs of influences between variables. Such prior knowledge can be readily obtained from domain experts. We show that this problem of parameter learning is a special case of isotonic regression and provide a simple algorithm for computing isotonic estimates. Our experimental results for a small Bayesian network in the medical domain show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution, especially when only a small sample of data is available. More importantly, however, the isotonic estimator provides parameter estimates that are consistent with the specified prior knowledge, thereby resulting in a network that is more likely to be accepted by experts in its domain of application.},
  AUTHOR       = {Ad Feelders and Linda C. van der Gaag},
  DOI          = {10.1016/j.ijar.2005.10.003},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian networks; Parameter learning; Order-constrained estimation},
  NUMBER       = {1-2},
  PAGES        = {37–53},
  TITLE        = {Learning Bayesian network parameters under order constraints},
  VOLUME       = {42},
  YEAR         = {2006},
}

@INPROCEEDINGS{Feron-1981,
  ANNOTATION = {Proceedings of the International Congress on Applied Systems Research and Cybernetics, Acapulco, Mexico, December 1980 enkel op papier},
  AUTHOR     = {R. Feron},
  BOOKTITLE  = {Applied Systems and Cybernetics},
  EDITOR     = {G. E. Lasker},
  LOCATION   = {New York},
  PAGES      = {2831–2836},
  PUBLISHER  = {Pergamom Press},
  SERIES     = {Fuzzy Sets and Systems, Possibility Theory and Special Topics in Systems Research},
  TITLE      = {Probabilistic and statistical study of random fuzzy sets whose referential is R^N},
  VOLUME     = {VI},
  YEAR       = {1981},
}

@ARTICLE{Ferreira-Cozman-2005-AR+,
  ABSTRACT     = {A credal network is a graphical representation for a set of joint probability distributions. In this paper we discuss algorithms for exact and approximate inferences in credal networks. We propose a branch-and-bound framework for inference, and focus on inferences for polytree-shaped networks. We also propose a new algorithm, A/R+, for outer approximations in polytree-shaped credal networks.},
  AUTHOR       = {José Carlos {Ferreira da Rocha} and Fabio Gagliardi Cozman},
  DOI          = {10.1016/j.ijar.2004.10.009},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Ferreira-Cozman-2005-AR+.pdf},
  NUMBER       = {2-3},
  PAGES        = {279–296},
  TITLE        = {Inference in credal networks: branch-and-bound methods and the A/R+ algorithm},
  VOLUME       = {39},
  YEAR         = {2005},
}

@REPORT{Ferson-et-al-2002,
  ANNOTATION  = {unabridged version, with corrections, printed in 2003},
  AUTHOR      = {Scott Ferson and Vladik Kreinovich and Lev Ginzburg and Davis S. Myers and Kari Sentz},
  INSTITUTION = {Sandia National Laboratories},
  NUMBER      = {SAND2002-4015},
  TITLE       = {Constructing probability boxes and Dempster-Shafer structures},
  TYPE        = {techreport},
  YEAR        = {2002},
}

@BOOK{Fiedler-etal-2006-inexactLP,
  AUTHOR    = {M. Fiedler and Jiri Nedoma and Jaroslav Ramík and Jiri Rohn and Karel Zimmermann},
  LOCALFILE = {book/Fiedler-etal-2006-inexactLP.pdf},
  PUBLISHER = {Springer},
  TITLE     = {Linear Optimization Problems with Inexact Data},
  URL       = {http://www.nsc.ru/interval/Library/InteBooks/InexactLP.pdf},
  YEAR      = {2006},
}

@BOOK{Fine-1973,
  AUTHOR    = {Terrence L. Fine},
  LOCATION  = {New York and London},
  PUBLISHER = {Academic Press},
  TITLE     = {Theories of Probability (An Examination of Foundations)},
  YEAR      = {1973},
}

@INCOLLECTION{deFinetti-1937-foresight,
  AUTHOR    = {Bruno de Finetti},
  BOOKTITLE = {Studies in Subjective Probability},
  EDITOR    = {Kyburg, Jr., Henry E. and Smokler},
  LOCALFILE = {inbook/deFinetti-1937-foresight.pdf},
  NOTE      = {Translation of \cite{DeFinetti-1937}},
  PAGES     = {93–158},
  PUBLISHER = {Wiley},
  TITLE     = {Foresight: Its Logical Laws, Its Subjective Sources},
  YEAR      = {1964},
}

@BOOK{deFinetti-1992,
  AUTHOR    = {Bruno de Finetti},
  EDITOR    = {Paola Monari and Daniela Cocchi},
  PUBLISHER = {CLUEB, Bologna},
  TITLE     = {Probabilità e Induzione – Induction and Probability},
  URL       = {http://diglib.cib.unibo.it/diglib.php?inv=35&term_ptnum=1&format=jpg},
  VOLUME    = {52},
  YEAR      = {1992},
}

@BOOK{Mura-2008,
  AUTHOR    = {Bruno de Finetti},
  DOI       = {10.1007/978-1-4020-8202-3},
  EDITOR    = {Alberto Mura},
  PUBLISHER = {Springer},
  SERIES    = {Synthese Library},
  TITLE     = {Philosophical Lectures on Probability},
  VOLUME    = {340},
  YEAR      = {2008},
}

@INCOLLECTION{Mura-2008-backmatter,
  AUTHOR    = {Bruno de Finetti},
  DOI       = {10.1007/978-1-4020-8202-3},
  EDITOR    = {Alberto Mura},
  PUBLISHER = {Springer},
  SERIES    = {Synthese Library},
  TITLE     = {Back Matter},
  VOLUME    = {340},
  YEAR      = {2008},
}

@INCOLLECTION{Mura-2008-ch12,
  AUTHOR    = {Bruno de Finetti},
  DOI       = {10.1007/978-1-4020-8202-3},
  EDITOR    = {Alberto Mura},
  PUBLISHER = {Springer},
  SERIES    = {Synthese Library},
  TITLE     = {Complete additivity and zero probabilities},
  VOLUME    = {340},
  YEAR      = {2008},
}

@INCOLLECTION{Mura-2008-frontmatter,
  AUTHOR    = {Bruno de Finetti},
  DOI       = {10.1007/978-1-4020-8202-3},
  EDITOR    = {Alberto Mura},
  PUBLISHER = {Springer},
  SERIES    = {Synthese Library},
  TITLE     = {Front Matter},
  VOLUME    = {340},
  YEAR      = {2008},
}

@BOOK{DeFinetti-1974/1975,
  AUTHOR    = {Bruno de Finetti},
  NOTE      = {Two volumes; translation of \cite{DeFinetti-1970-book}},
  PUBLISHER = {John Wiley \& Sons},
  TITLE     = {Theory of Probability},
  YEAR      = {1974-1975},
}

@ARTICLE{deFinetti-1981-BJPS,
  AUTHOR       = {Bruno de Finetti},
  DOI          = {10.1093/bjps},
  JOURNALTITLE = {The British Journal for the Philosophy of Science},
  LOCALFILE    = {article/deFinetti-1981-BJPS.pdf},
  NUMBER       = {1},
  PAGES        = {55–56},
  PUBLISHER    = {British Society for the Philosophy of Science},
  TITLE        = {The role of `Dutch books' and of `proper scoring rules'},
  URL          = {http://www.jstor.org/stable/687386},
  VOLUME       = {32},
  YEAR         = {1981},
}

@BOOK{DeFinetti-1972,
  ANNOTATION = {geringde kopie},
  AUTHOR     = {Bruno de Finetti},
  PUBLISHER  = {John Wiley \& Sons},
  TITLE      = {Probability, Induction and Statistics (The art of guessing)},
  YEAR       = {1972},
}

@INBOOK{DeFinetti-1972-notation,
  ANNOTATION = {Translation of \cite{DeFinetti-1967} by Leonard J. Savage geringde kopie},
  AUTHOR     = {Bruno de Finetti},
  BOOKTITLE  = {Probability, Induction and Statistics (The art of guessing)},
  PAGES      = {xviii–xxiv},
  PUBLISHER  = {John Wiley \& Sons},
  TITLE      = {A Useful Notation},
  YEAR       = {1972},
}

@BOOK{DeFinetti-1970-book,
  AUTHOR    = {Bruno de Finetti},
  NOTE      = {English translation: \cite{DeFinetti-1974/1975}},
  PUBLISHER = {Giulio Einaudi},
  TITLE     = {Teoria Delle Probabilità},
  YEAR      = {1970},
}

@ARTICLE{DeFinetti-1967,
  AUTHOR       = {Bruno de Finetti},
  JOURNALTITLE = {Revue Roumaine des Mathémathiques Pures et Appliquées},
  PAGES        = {1227–1233},
  TITLE        = {Quelques conventions qui semblent utiles},
  VOLUME       = {12},
  YEAR         = {1967},
}

@ARTICLE{DeFinetti-1937,
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Bruno de Finetti},
  JOURNALTITLE = {Annales de l'Institut Henri Poincaré},
  LOCALFILE    = {article/DeFinetti-1937.pdf},
  NOTE         = {English translation: \cite{deFinetti-1937-foresight}},
  NUMBER       = {1},
  PAGES        = {1–68},
  PUBLISHER    = {Institut Henri Poincaré},
  TITLE        = {La prévision: ses lois logiques, ses sources subjectives},
  URL          = {http://www.numdam.org/item?id=AIHP_1937__7_1_1_0},
  VOLUME       = {7},
  YEAR         = {1937},
}

@ARTICLE{DeFinetti-1933a,
  AUTHOR       = {Bruno de Finetti},
  JOURNALTITLE = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
  LOCALFILE    = {article/DeFinetti-1933a.pdf},
  PAGES        = {279–284},
  TITLE        = {Classi di numeri aleatori equivalenti},
  VOLUME       = {18},
  YEAR         = {1933},
}

@ARTICLE{DeFinetti-1933b,
  AUTHOR       = {Bruno de Finetti},
  JOURNALTITLE = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
  LOCALFILE    = {article/DeFinetti-1933b.pdf},
  PAGES        = {203–207},
  TITLE        = {La legge dei grandi numeri nel caso dei numeri aleatori equivalenti},
  VOLUME       = {18},
  YEAR         = {1933},
}

@ARTICLE{DeFinetti-1933c,
  AUTHOR       = {Bruno de Finetti},
  JOURNALTITLE = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
  LOCALFILE    = {article/DeFinetti-1933c.pdf},
  PAGES        = {279–284},
  TITLE        = {Sulla legge di distribuzione dei valori in una successione di numeri aleatori equivalenti},
  VOLUME       = {18},
  YEAR         = {1933},
}

@ARTICLE{DeFinetti-Jacob-1935,
  AUTHOR       = {Bruno de Finetti and M. Jacob},
  JOURNALTITLE = {Giornale dell'Istituto italiano degli attuari},
  LOCALFILE    = {article/DeFinetti-Jacob-1935.pdf},
  PAGES        = {303–319},
  TITLE        = {Sull'integrale di Stieltjes-Riemann},
  VOLUME       = {6},
  YEAR         = {1935},
}

@REPORT{Fink-1995-conjugate-compendium,
  AUTHOR      = {Daniel Fink},
  INSTITUTION = {Environmental Statistics Group, Department of Biology, Montana State Univeristy},
  LOCATION    = {Bozeman, Montana},
  TITLE       = {A Compendium of Conjugate Priors},
  TYPE        = {techreport},
  URL         = {http://www.people.cornell.edu/pages/df36/CONJINTRnewTEX.pdf},
  YEAR        = {1995},
}

@REPORT{Fioretti-2001-Shackle,
  ABSTRACT    = {Evidence Theory is a branch of mathematics that concerns the combination of empirical evidence in an individual's mind in order to construct a coherent picture of reality. Designed to deal with unexpected empirical evidence suggesting new possibilities, evidence theory has a lot in common with Shackle's idea of decision-making as a creative act. This essay investigates this connection in detail, pointing to the usefulness of evidence theory to formalise and extend Shackle's decision theory. In order to ease a proper framing of the issues involved, evidence theory is not only compared with Shackle's ideas but also with additive and sub-additive probability theories. Furthermore, the presentation of evidence theory does not refer to the original version only, but takes account of its most recent developments, too.},
  AUTHOR      = {Guido Fioretti},
  INSTITUTION = {Università di Firenze and ICER, Torino},
  TITLE       = {A mathematical theory of evidence for G. L. S. Shackle},
  TYPE        = {techreport},
  YEAR        = {2001},
}

@ARTICLE{Fishburn-1986,
  AUTHOR       = {Peter C. Fishburn},
  DOI          = {10.1214/ss/1177013611},
  JOURNALTITLE = {Statistical Science},
  LOCALFILE    = {article/Fishburn-1986.pdf},
  NUMBER       = {3},
  PAGES        = {335–345},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {The axioms of subjective probability},
  VOLUME       = {1},
  YEAR         = {1986},
}

@ARTICLE{Fishburn-1980,
  ABSTRACT     = {Stochastic dominance orders of all finite degrees are defined on the set of distribution functions on the nonnegative real numbers in terms of integrals of the distributions. It is proved that if F strictly nth-degree stochastically dominates G, and if the moments of F and G through order n are finite with $\mu \_{F}^{k}-∈t x^{k}dF(x)$, then $(\mu \_{F}^{1},…,\mu \_{F}^{n})\neq (\mu \_{G}^{n},…,\mu \_{G}^{n})$ and $(-1)^{k-1}\mu \_{F}^{k}>(-1)^{k-1}\mu \_{G}^{k}$ for the smallest k for which $\mu \_{F}^{k}\neq \mu \_{G}^{k}$.},
  AUTHOR       = {Peter C. Fishburn},
  ISSN         = {0364-765X},
  JOURNALTITLE = {Mathematics of Operations Research},
  LOCALFILE    = {article/Fishburn-1980.pdf},
  NUMBER       = {1},
  PAGES        = {94–100},
  PUBLISHER    = {INFORMS},
  TITLE        = {Stochastic dominance and moments of distributions},
  URL          = {http://www.jstor.org/stable/3689397},
  VOLUME       = {5},
  YEAR         = {1980},
}

@ARTICLE{Fishburn-LaValle-1998,
  ABSTRACT     = {The theory of subjective expected lexicographic utility brings together two classical developments in expected utility theory. The first is Hausner's theory of expected lexicographic utility in decision under risk. The second is a lottery-based theory of subjective expected utility in decision under uncertainty that was first axiomatized by Anscombe and Aumann. Our synthesis of the two produces representations of preference in decision underuncertainty in which utilities are finite-dimensional real vectors ordered lexicographicallyand subjective probabilities are real matrices. Axiomatizations of subjective expected lexicographic utility are described for finite and infinite sets of states. Procedures for assessing vector utilities and matrix probabilities are outlined.},
  AUTHOR       = {Peter C. Fishburn and Irving H. LaValle},
  DOI          = {10.1023/A:1018911830478},
  ISSN         = {0254-5330},
  JOURNALTITLE = {Annals of Operations Research},
  LOCALFILE    = {article/Fishburn-LaValle-1998.pdf},
  PAGES        = {183–206},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Subjective expected lexicographic utility: Axioms and assessment},
  VOLUME       = {80},
  YEAR         = {1998},
}

@ARTICLE{Fisher-1934,
  AUTHOR       = {R. A. Fisher},
  DOI          = {10.1098/rspa.1934.0050},
  JOURNALTITLE = {Proceedings of the Royal Society of London, A},
  LOCALFILE    = {article/Fisher-1934.pdf},
  PAGES        = {285–307},
  TITLE        = {Two new properties of mathematical likelihood},
  VOLUME       = {144},
  YEAR         = {1934},
}

@ARTICLE{Fodor-Marichal-Roubens-1995,
  AUTHOR       = {Janos Fodor and Jean-Luc Marichal and Marc Roubens},
  JOURNALTITLE = {IEEE Transactions on Fuzzy Systems},
  LOCALFILE    = {article/Fodor-Marichal-Roubens-1995.pdf},
  NUMBER       = {2},
  PAGES        = {236–240},
  TITLE        = {Characterization of the Ordered Weighted Averaging Operators},
  VOLUME       = {3},
  YEAR         = {1995},
}

@INPROCEEDINGS{Fortet-1951,
  AUTHOR    = {Robert M. Fortet},
  BOOKTITLE = {Congrès international de philosophie des sciences. 4: Calcul des probabilités},
  EDITOR    = {Raymond Bayer},
  LOCATION  = {Paris},
  NUMBER    = {1146},
  PAGES     = {35–47},
  PUBLISHER = {Hermann},
  SERIES    = {Actualités scientifiques et industrielles},
  TITLE     = {Faut-il élargir les axiomes du calcul des probabilités?},
  YEAR      = {1951},
}

@ARTICLE{Frechet-1948,
  AUTHOR       = {Maurice Fréchet},
  JOURNALTITLE = {Annales de l'Institut Henri Poincaré},
  LOCALFILE    = {article/Frechet-1948.pdf},
  NUMBER       = {4},
  PAGES        = {215–310},
  TITLE        = {Les éléments aléatoires de nature quelconque dans un espace distancié},
  URL          = {http://www.numdam.org/item?id=AIHP_1948__10_4_215_0},
  VOLUME       = {10},
  YEAR         = {1948},
}

@ARTICLE{Friedman-1997,
  ABSTRACT     = {The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {x\_1,....,x\_n}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like “naive” Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why “bagging/aggregating” classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance.},
  AUTHOR       = {Jerome H. Friedman},
  DOI          = {10.1023/A:1009778005914},
  JOURNALTITLE = {Data Mining and Knowledge Discovery},
  KEYWORDS     = {bagging; bias; classification; curse-of-dimensionality; naive Bayes; nearest-neighbors; variance},
  LOCALFILE    = {article/Friedman-1997.pdf},
  NUMBER       = {1},
  PAGES        = {55–77},
  PUBLISHER    = {Springer},
  TITLE        = {On Bias, Variance, 0/1—Loss, and the Curse-of-Dimensionality},
  VOLUME       = {1},
  YEAR         = {1997},
}

@BOOK{Friedman-1989feron,
  ANNOTATION = {geannoteerde uittreksels},
  AUTHOR     = {James W. Friedman},
  KEYWORDS   = {game theory; mathematical economics},
  PUBLISHER  = {Oxford University Press},
  TITLE      = {Game Theory with Applications to Economics},
  YEAR       = {1989},
}

@ARTICLE{Friedman-2004,
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Nir Friedman},
  DOI          = {10.1126/science.1094068},
  JOURNALTITLE = {Science},
  LOCALFILE    = {article/Friedman-2004.pdf},
  NUMBER       = {5659},
  PAGES        = {799–805},
  PUBLISHER    = {American Association for the Advancement of Science},
  TITLE        = {Inferring cellular networks using probabilistic graphical models},
  VOLUME       = {303},
  YEAR         = {2004},
}

@ARTICLE{Fuchs-Neumaier-2009,
  ABSTRACT     = {Robust design optimization methods applied to real life problems face some major difficulties: How to deal with the estimation of probability densities when data are sparse, how to cope with high dimensional problems and how to use valuable information in the form of unformalized expert knowledge. In this paper we introduce in detail the clouds formalism as a means to process available uncertainty information reliably, even if limited in amount and possibly lacking a formal description. This enables a worst-case analysis with confidence regions of relevant scenarios which can be involved in an optimization problem formulation for robust design.},
  AUTHOR       = {Martin Fuchs and Arnold Neumaier},
  DOI          = {10.1080/15598608.2009.10411922},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Clouds; Confidence regions; Design optimization; Potential clouds; Robust design; Uncertainty modeling},
  LOCALFILE    = {article/Fuchs-Neumaier-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {225–238},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Potential Based Clouds in Robust Design Optimization},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Fudenberg-Kreps-1993,
  ABSTRACT     = {We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.},
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Drew Fudenberg},
  DOI          = {10.1006/game.1993.1021},
  ISSN         = {0899-8256},
  JOURNALTITLE = {Games and Economic Behavior},
  LOCALFILE    = {article/Fudenberg-Kreps-1993.pdf},
  MONTH        = {07},
  NUMBER       = {3},
  PAGES        = {320–367},
  PUBLISHER    = {MIT Press},
  TITLE        = {Learning Mixed Equilibria},
  VOLUME       = {5},
  YEAR         = {1993},
}

@BOOK{Fudenberg-Levine-1998,
  ANNOTATION = {geannoteerde uittreksels},
  AUTHOR     = {Drew Fudenberg and David K. Levine},
  EDITOR     = {Ken Binmore},
  NUMBER     = {2},
  PUBLISHER  = {The MIT Press},
  SERIES     = {MIT Press Series on Economic Learning and Social Evolution},
  TITLE      = {The Theory of Learning in Games},
  YEAR       = {1998},
}

@ARTICLE{Fudenberg-Levine-1995,
  ABSTRACT     = {We study a variation of fictitious play, in which the probability of each action is an exponential function of that action's utility against the historical frequency of opponents' play. Regardless of the opponents' strategies, the utility received by an agent using this rule is nearly the best that could be achieved against the historical frequency. Such rules are approximately optimal in i.i.d. environments, and guarantee nearly the minmax regardless of opponents' behavior. Fictitious play shares these properties provided it switches ‘infrequently’ between actions. We also study the long-run outcomes when all players use consistent and cautious rules.},
  AUTHOR       = {Drew Fudenberg and David K. Levine},
  DOI          = {10.1016/0165-1889(94)00819-4},
  JOURNALTITLE = {Journal of Economic Dynamics and Control},
  KEYWORDS     = {Games; Learning; Rationality; consistency},
  LOCALFILE    = {article/Fudenberg-Levine-1995.pdf},
  NUMBER       = {5-7},
  PAGES        = {1065–1089},
  PUBLISHER    = {Elsevier},
  TITLE        = {Consistency and cautious fictitious play},
  VOLUME       = {19},
  YEAR         = {1995},
}

@ARTICLE{Fujimoto-Oshime-1994-Perron-Frobenius,
  ABSTRACT     = {By a pointed closed convex cone K \subset of \mathbb{R}^N with interior, an order is defined in \mathbb{R}^N. Let T: K\to K be a set-valued nondecreasing subhomogeneous map. The main purpose of this paper concerns the conditions on λ>0 under which λu ∈ T(u)+c, u ∈ K, is solvable for all c∈ K and how its solution depends on c. The homogenization of T around infinity is also introduced and is proved to leave the solvability condition for λ>0 unchanged.},
  AUTHOR       = {Takao Fujimoto and Yorimasa Oshime},
  DOI          = {10.1016/0304-4068(94)90028-0},
  JOURNALTITLE = {Journal of Mathematical Economics},
  KEYWORDS     = {Homo; Monotonicity; Nonlinear resolvent problems},
  LOCALFILE    = {article/Fujimoto-Oshime-1994-Perron-Frobenius.pdf},
  NUMBER       = {5},
  PAGES        = {475–498},
  TITLE        = {The nonlinear Perron-Frobenius problem for set-valued maps in a closed convex cone in R^N},
  VOLUME       = {23},
  YEAR         = {1994},
}

@MISC{Fukuda-presentation,
  AUTHOR    = {Komei Fukuda},
  LOCALFILE = {misc/Fukuda-presentation.pdf},
  TITLE     = {Vertex enumeration for polyhedra: algorithms and open problems},
  TYPE      = {Presentation slides},
}

@ARTICLE{Fukuda-2012-lecture,
  AUTHOR      = {Komei Fukuda},
  INSTITUTION = {ETH Zürich},
  LOCALFILE   = {misc/Fukuda-2012-lecture.pdf},
  TITLE       = {Polyhedral Computation},
  TYPE        = {Lecture notes},
  YEAR        = {2012},
}

@ARTICLE{Fukuda-2004-Minkowski-addition,
  ABSTRACT     = {A zonotope is the Minkowski addition of line segments in Rd. The zonotope construction problem is to list all extreme points of a zonotope given by its line segments. By duality, it is equivalent to the arrangement construction problem—that is, to generate all regions of an arrangement of hyperplanes. By replacing line segments with convex V-polytopes, we obtain a natural generalization of the zonotope construction problem: the construction of the Minkowski addition of k polytopes. Gritzmann and Sturmfels studied this general problem in various aspects and presented polynomial algorithms for the problem when one of the parameters k or d is fixed. The main objective of the present work is to introduce an efficient algorithm for variable d and k. Here we call an algorithm efficient or polynomial if it runs in time bounded by a polynomial function of both the input size and the output size. The algorithm is a natural extension of a known algorithm for the zonotope construction, based on linear programming and reverse search. It is compact, highly parallelizable and very easy to implement. This work has been motivated by the use of polyhedral computation for optimal tolerance determination in mechanical engineering.},
  AUTHOR       = {Komei Fukuda},
  DOI          = {10.1016/j.jsc.2003.08.007},
  JOURNALTITLE = {Journal of Symbolic Computation},
  KEYWORDS     = {Convex polytope; Efficient algorithm; Minkowski addition; Reverse search},
  LOCALFILE    = {article/Fukuda-2004-Minkowski-addition.pdf},
  PAGES        = {1261–1272},
  TITLE        = {From the zonotope construction to the Minkowski addition of convex polytopes},
  VOLUME       = {38},
  YEAR         = {2004},
}

@INPROCEEDINGS{Fukuda-Prodon-1996-cdd,
  AUTHOR    = {Komei Fukuda and Alain Prodon},
  BOOKTITLE = {Combinatorics and Computer Science},
  DOI       = {10.1007/3-540-61576-8_77},
  EDITOR    = {M. Deza and R. Euler and I. Manoussakis},
  PAGES     = {91–111},
  PUBLISHER = {Springer-Verlag},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Double description method revisited},
  URL       = {http://www.ifor.math.ethz.ch/~fukuda/cdd_home},
  VOLUME    = {1120},
  YEAR      = {1996},
}

@INPROCEEDINGS{Fuller-Zimmermann-1992,
  AUTHOR    = {Robert Fullér and Hans-Jürgen Zimmermann},
  BOOKTITLE = {Proceedings of 2nd International Workshop on Current Issues in Fuzzy Technologies},
  TITLE     = {Approximate Reasoning for Solving Fuzzy Linear Programming Problems},
  YEAR      = {1992},
}

@INPROCEEDINGS{1990-Gaag-CSN,
  ABSTRACT  = {Many AI researchers argue that probability theory is only capable of dealing with uncer­tainty in situations where a fully specified joint probability distribution is available, and conclude that it is not suitable for reasoning with uncertainty in AI systems. Probability intervals, however, constitute a means for expressing incompleteness of information. We present a method for computing probability intervals for probabilities of interest from a partial specification of a joint probability distribution and a method for updating such probability intervals as evidence becomes available.},
  AUTHOR    = {Linda C. van der Gaag},
  BOOKTITLE = {CSN90: Computer Science in the Netherlands, Proceedings},
  ISBN      = {90-6196-393-1},
  LOCALFILE = {inproceedings/1990-Gaag-CSN.pdf},
  PAGES     = {153–167},
  PUBLISHER = {Stichting Mathematisch Centrum},
  TITLE     = {Using probability intervals in plausible reasoning},
  VENUE     = {Amsterdam},
  VENUE2    = {Utrecht},
  VOLUME    = {1},
  YEAR      = {1990},
}

@INPROCEEDINGS{1990-Gaag-UAI,
  ABSTRACT  = {Many AI researchers argue that probability theory is only capable of dealing with uncertainty in situations where a full specification of a joint probability distribution is available, and conclude that it is not suitable for application in knowledge-based systems. Probability intervals, however, constitute a means for expressing incompleteness of information. We present a method for computing such probability intervals for probabilities of interest from a partial specification of a joint probability distribution. Our method improves on earlier approaches by allowing for independency relationships between statistical variables to be exploited.},
  AUTHOR    = {Linda C. van der Gaag},
  BOOKTITLE = {UAI-90: Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence},
  LOCALFILE = {inproceedings/1990-Gaag-UAI.pdf},
  LOCATION  = {Amsterdam, the Netherlands},
  PAGES     = {457–466},
  PUBLISHER = {Elsevier},
  TITLE     = {Computing probability intervals under independency constraints},
  YEAR      = {1990},
}

@INCOLLECTION{Van_der_Gaag+Bodlaender-2011,
  AUTHOR    = {Linda C. van der Gaag and Hans L. Bodlaender},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-642-22152-1_15},
  EDITOR    = {Weiru Liu},
  ISBN      = {978-3-642-22151-4},
  LOCALFILE = {inproceedings/Van_der_Gaag+Bodlaender-2011.pdf},
  PAGES     = {170–181},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {On stopping evidence gathering for diagnostic Bayesian networks},
  VOLUME    = {6717},
  YEAR      = {2011},
}

@INPROCEEDINGS{VanderGaag-Bodlaender-Feelders-2004,
  AUTHOR    = {Linda C. van der Gaag and Hans L. Bodlaender and Ad Feelders},
  BOOKTITLE = {UAI-04: Proceedings of the Twentieth Annual Conference on Uncertainty in Artificial Intelligence},
  EDITOR    = {M. Chickering and J. Halpern},
  LOCALFILE = {inproceedings/VanderGaag-Bodlaender-Feelders-2004.pdf},
  LOCATION  = {Arlington, Virginia},
  PAGES     = {569–576},
  PUBLISHER = {AUAI Press},
  TITLE     = {Monotonicity in Bayesian networks},
  YEAR      = {2004},
}

@INCOLLECTION{Van_der_Gaag+al-2010,
  AUTHOR    = {Linda C. van der Gaag and Janneke H. Bolt and Willie L. A. Loeffen and Armin R. W. Elbers},
  BOOKTITLE = {Computational Intelligence for Knowledge-Based Systems Design},
  DOI       = {10.1007/978-3-642-14049-5_69},
  EDITOR    = {Eyke Hüllermeier and Rudolf Kruse and Frank Hoffmann},
  ISBN      = {978-3-642-14048-8},
  LOCALFILE = {inproceedings/Van_der_Gaag+al-2010.pdf},
  PAGES     = {675–684},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Modelling patterns of evidence in Bayesian networks: A case-study in classical swine fever},
  VOLUME    = {6178},
  YEAR      = {2010},
}

@INPROCEEDINGS{VanderGaag-Renooij-2001,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij},
  BOOKTITLE = {UAI-01: Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
  EDITOR    = {Jack S. Breese and Daphne Koller},
  ISBN      = {1-55860-800-1},
  PAGES     = {530–537},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {Analysing sensitivity data from probabilistic networks},
  YEAR      = {2001},
}

@MISC{2009-Gaag+Renooij-syllabus,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij},
  LOCALFILE = {misc/2009-Gaag+Renooij-syllabus.pdf},
  NOTE      = {Syllabus Utrecht University},
  TITLE     = {Probabilistic Reasoning},
  URL       = {http://www.cs.uu.nl/docs/vakken/prob/literatuur.html},
  YEAR      = {2009},
}

@INPROCEEDINGS{VanderGaag-Renooij-2004,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij},
  BOOKTITLE = {IPMU : Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems},
  PAGES     = {1675–1682},
  TITLE     = {On the sensitivity of probabilistic networks to test reliability},
  VENUE     = {Perugia},
  YEAR      = {2004},
}

@INCOLLECTION{VanderGaag-Renooij-Coupe-2007,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Veerle M. H. Coupé},
  BOOKTITLE = {Advances in Probabilistic Graphical Models},
  DOI       = {10.1007/978-3-540-68996-6_5},
  LOCALFILE = {inbook/VanderGaag-Renooij-Coupe-2007.pdf},
  PAGES     = {103–124},
  PUBLISHER = {Springer},
  TITLE     = {Sensitivity analysis of probabilistic networks},
  VOLUME    = {124},
  YEAR      = {2007},
}

@INPROCEEDINGS{VanderGaag-etal-2009,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Ad Feelders and Arend de Groote and Marinus J. C. Eijkemans and Frank J. Broekmans and Bart C. J. M. Fauser},
  BOOKTITLE = {Machine Learning and Data Mining in Pattern Recognition},
  DOI       = {10.1007/978-3-642-03070-3_59},
  EDITOR    = {Petra Perner},
  ISBN      = {978-3-642-03069-7},
  PAGES     = {787–801},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Aligning Bayesian network classifiers with medical contexts.},
  VOLUME    = {5632},
  YEAR      = {2009},
}

@INCOLLECTION{Van_der_Gaag-etal-2012,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Hermi J. M. Schijf and Armin R. W. Elbers and Willie L. A. Loeffen},
  BOOKTITLE = {Advances in Computational Intelligence},
  DOI       = {10.1007/978-3-642-31718-7_16},
  EDITOR    = {Salvatore Greco and Bernadette Bouchon-Meunier and Giulianella Coletti and Mario Fedrizzi and Benedetto Matarazzo and Ronald R. Yager},
  ISBN      = {978-3-642-31717-0},
  LOCALFILE = {incollection/Van_der_Gaag-etal-2012.pdf},
  PAGES     = {151–160},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Communications in Computer and Information Science},
  TITLE     = {Experiences with eliciting probabilities from multiple experts},
  VOLUME    = {299},
  YEAR      = {2012},
}

@INPROCEEDINGS{VanderGaag-etal-2010-BNAIC,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Hermi J. M. Schijf and Armin R. W. Elbers and Willie L. A. Loeffen},
  BOOKTITLE = {BNAIC 2010: Proceedings of the 22nd Benelux Conference on Artificial Intelligence},
  LOCALFILE = {inproceedings/VanderGaag-etal-2010-BNAIC.pdf},
  TITLE     = {Probability assessments from multiple experts: qualitative information is more robust},
  VENUE     = {Luxembourg},
  YEAR      = {2010},
}

@INPROCEEDINGS{VanderGaag-etal-2009-ECSQUARU,
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Wilma Steeneveld and Henk Hogeveen},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-642-02906-6_45},
  EDITOR    = {Claudio Sossai and Gaetano Chemello},
  ISBN      = {978-3-642-02905-9},
  LOCALFILE = {inproceedings/VanderGaag-etal-2009-ECSQUARU.pdf},
  NOTE      = {Proceedings of ECSQUARU 2009},
  PAGES     = {518–529},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {When in doubt ... be indecisive},
  VOLUME    = {5590},
  YEAR      = {2009},
}

@ARTICLE{VanderGaag-etal-2002,
  ABSTRACT     = {With the help of two experts in gastrointestinal oncology from The Netherlands Cancer Institute, Antoni van Leeuwenhoekhuis, a decision-support system is being developed for patient-specific therapy selection for oesophageal cancer. The kernel of the system is a probabilistic network that describes the presentation characteristics of cancer of the oesophagus and the pathophysiological processes of invasion and metastasis. While the construction of the graphical structure of the network was relatively straightforward, probability elicitation with existing methods proved to be a major obstacle. To overcome this obstacle, we designed a new method for eliciting probabilities from experts that combines the ideas of transcribing probabilities as fragments of text and of using a scale with both numerical and verbal anchors for marking assessments. In this paper, we report experiences with our method in eliciting the probabilities required for the oesophagus network. The method allowed us to elicit many probabilities in reasonable time. To gain some insight in the quality of the probabilities obtained, we conducted a preliminary evaluation study of our network, using data from real patients. We found that for 85\% of the patients, the network predicted the correct cancer stage.},
  AUTHOR       = {Linda C. van der Gaag and Silja Renooij and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
  DOI          = {10.1016/S0933-3657(02)00012-X},
  ISSN         = {0933-3657},
  JOURNALTITLE = {Artificial Intelligence in Medicine},
  KEYWORDS     = {Elicitation of judgemental probabilities; Probabilistic networks},
  NUMBER       = {2},
  PAGES        = {123–148},
  TITLE        = {Probabilities for a probabilistic network: a case study in oesophageal cancer},
  VOLUME       = {25},
  YEAR         = {2002},
}

@INPROCEEDINGS{VanderGaag-etal-1999,
  ABSTRACT  = {In building Bayesian belief networks, the elicitation of all probabilities required can be a major obstacle. We learned the extent of this often-cited observation in the construction of the probabilistic part of a complex influence diagram in the field of cancer treatment. Based upon our negative experiences with existing methods, we designed a new method for probability elicitation from domain experts. The method combines various ideas, among which are the ideas of transcribing probabilities and of using a scale with both numerical and verbal anchors for marking assessments. In the construction of the probabilistic part of our influence diagram, the method proved to allow for the elicitation of many probabilities in little time.},
  AUTHOR    = {Linda C. van der Gaag and Silja Renooij and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
  BOOKTITLE = {UAI-99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
  KEYWORDS  = {belief networks; expert elicitation},
  LOCATION  = {San Francisco, California},
  PAGES     = {647–654},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {How to elicit many probabilities},
  YEAR      = {1999},
}

@ARTICLE{Van_der_Gaag+Tabachneck-Schijf-2010,
  ABSTRACT     = {The next development in building Bayesian networks will most likely entail constructing multi-purpose models that can be employed for varying tasks and by different types of user. In this position paper, we argue that the development of a special type of ontology to organize the knowledge involved in such a multi-purpose model is crucial for the management of the model’s content. This ontology should preserve all knowledge elicited for the construction of the model and be accessible to domain experts and knowledge engineers alike. Based on the different ways in which people learn and gain expertise, we further argue that knowledge elicitation will result in task-specific knowledge mostly, which is best stored in the format in which it is elicited. To support varying model views for different tasks and different types of user, we propose that the elicited knowledge be organized in a library-style ontology of separate modules.},
  AUTHOR       = {Linda C. van der Gaag and Hermina J. M. Tabachneck-Schijf},
  DOI          = {10.1016/j.ijar.2009.05.008},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian networks; Engineering; Task model views; Ontology},
  LOCALFILE    = {article/Van_der_Gaag+Tabachneck-Schijf-2010.pdf},
  NUMBER       = {2},
  PAGES        = {196–208},
  TITLE        = {Library-style ontologies to support varying model views},
  VOLUME       = {51},
  YEAR         = {2010},
}

@ARTICLE{Van_der_Gaag_Tabachneck-Schijf_Geenen-2009,
  ABSTRACT     = {In many realistic problem domains, the main variable of interest behaves monotonically in the observable variables, in the sense that higher values for the variable of interest become more likely with higher-ordered observations. This type of knowledge appears to naturally emerge from experts during knowledge elicitation, without explicit prompting from the knowledge engineer. The experts’ concept of monotonicity, however, may not correspond to the mathematical concept of monotonicity in Bayesian networks. We present a method that provides both for verifying whether or not a network exhibits the properties of monotonicity suggested by the experts and for studying the violated properties with the experts. We illustrate the application of our method for a real Bayesian network in veterinary science.},
  AUTHOR       = {Linda C. van der Gaag and Hermina J. M. Tabachneck-Schijf and Petra L. Geenen},
  DOI          = {10.1016/j.ijar.2008.04.008},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Bayesian networks; Knowledge engineering},
  LOCALFILE    = {article/Van_der_Gaag_Tabachneck-Schijf_Geenen-2009.pdf},
  NUMBER       = {3},
  PAGES        = {429–436},
  TITLE        = {Verifying monotonicity of Bayesian networks with domain experts},
  VOLUME       = {50},
  YEAR         = {2009},
}

@ARTICLE{Gaifman-2004,
  AUTHOR       = {Haim Gaifman},
  DOI          = {10.1023/B:SYNT.0000029944.99888.a7},
  JOURNALTITLE = {Synthese},
  LOCALFILE    = {article/Gaifman-2004.pdf},
  NUMBER       = {1-2},
  PAGES        = {97–119},
  TITLE        = {Reasoning with limited resources and assigning probabilities to arithmetical statements},
  VOLUME       = {140},
  YEAR         = {2004},
}

@INCOLLECTION{Garloff-Graf-1999,
  AUTHOR    = {J. Garloff and B. Graf},
  EDITOR    = {N. Munro},
  PAGES     = {339–352},
  PUBLISHER = {The Institution of Electrical Engineers (IEE)},
  TITLE     = {Solving strict polynomial inequalities by Bernstein expansion},
  YEAR      = {1999},
}

@ARTICLE{Gaertner-1999,
  ABSTRACT     = {We describe a new exact-arithmetic approach to linear programming when the number of variables n is much larger than the number of constraints m (or vice versa). The algorithm is an implementation of the simplex method which combines exact (multiple precision) arithmetic with inexact (floating point) arithmetic, where the number of exact arithmetic operations is small and usually bounded by a function of min(n,m). Combining this with a “partial pricing” scheme (based on a result by Clarkson) which is particularly tuned for the problems under consideration, we obtain a correct and practically efficient algorithm that even competes with the inexact state-of-the-art solver CPLEX1Trademark of CPLEX Optimization Inc. 1 for small values of min(n,m) and and is far superior to methods that use exact arithmetic in any operation. The main applications lie in computational geometry.},
  AUTHOR       = {Bernd Gärtner},
  DOI          = {10.1016/S0925-7721(99)00012-7},
  ISSN         = {0925-7721},
  JOURNALTITLE = {Computational Geometry},
  KEYWORDS     = {Linear programming; Simplex method; Exact arithmetic LP-type problems},
  NUMBER       = {2},
  PAGES        = {121–139},
  TITLE        = {Exact arithmetic at low cost – A case study in linear programming},
  VOLUME       = {13},
  YEAR         = {1999},
}

@ARTICLE{Gaubert-Gunawardena-2004-Perron,
  ABSTRACT     = {If A is a nonnegative matrix whose associated directed graph is strongly connected, the Perron-Frobenius theorem asserts that A has an eigenvector in the positive cone, (\mathbb{R}^+)^n. We associate a directed graph to any homogeneous, monotone function, f : (\mathbb{R}+)n \to (\mathbb{R}^+)^n, and show that if the graph is strongly connected, then f has a (nonlinear) eigenvector in (\mathbb{R}^+)^n. Several results in the literature emerge as corollaries. Our methods show that the Perron-Frobenius theorem is "really" about the boundedness of invariant subsets in the Hilbert projective metric. They lead to further existence results and open problems.},
  ANNOTATION   = {ook op papier als arXiv preprint},
  AUTHOR       = {Stéphane Gaubert and Jeremy Gunawardena},
  JOURNALTITLE = {Transactions of the American Mathematical Society},
  KEYWORDS     = {Collatz-Wielandt property; Hilbert projective met; Hilbert projective metric; nonexpansive function; nonlinear eigenvalue},
  LOCALFILE    = {article/Gaubert-Gunawardena-2004-Perron.pdf},
  NUMBER       = {12},
  PAGES        = {4931–4950},
  TITLE        = {The Perron-Frobenius theorem for homogeneous, monotone functions},
  URL          = {http://www.ams.org/journals/tran/2004-356-12/S0002-9947-04-03470-1},
  VOLUME       = {356},
  YEAR         = {2004},
}

@INPROCEEDINGS{Geenen-etal-2006,
  AUTHOR    = {Petra L. Geenen and Armin R. W. Elbers and Linda C. van der Gaag and Willie L. A. Loeffen},
  BOOKTITLE = {Proceedings of the Eleventh Symposium of the International Society for Veterinary Epidemiology and Economics},
  PAGES     = {667–669},
  TITLE     = {Development of a probabilistic network for clinical detection of classical swine fever},
  YEAR      = {2006},
}

@INPROCEEDINGS{Geenen-VanderGaag-2005,
  AUTHOR    = {Petra L. Geenen and Linda C. van der Gaag},
  BOOKTITLE = {Proceedings of the Third Bayesian Modelling Applications Workshop},
  TITLE     = {Developing a Bayesian network for clinical diagnosis in veterinary medicine: from the individual to the herd},
  VENUE     = {Edinburgh},
  YEAR      = {2005},
}

@ARTICLE{Geenen+al-2011,
  ABSTRACT     = {For diseases of which the clinical diagnosis is uncertain, naive Bayesian classifiers can be of assistance to the veterinary practitioner. These simple probabilistic models have proven to be very powerful for solving classification problems in a variety of domains, but are not yet widely applied within the veterinary domain. In this paper, naive Bayesian classifiers and methods for their construction are reviewed. We demonstrate how to construct full and selective classifiers from a data set and how to build such classifiers from information in the literature. As a case study, naive Bayesian classifiers to discriminate between classical swine fever (CSF)-infected and non-infected pig herds were constructed from data collected during the 1997/1998 CSF epidemic in the Netherlands. The resulting classifiers were studied in terms of their accuracy and compared with the optimally efficient diagnostic rule that was reported earlier by Elbers et al. (2002). The classifiers were found to have accuracies within the range of 67–70\% and performed comparable to or even better than the diagnostic rule on the available data. In contrast with the diagnostic rule, the classifiers had the advantage of taking both the presence and the absence of particular clinical signs into account, which resulted in more discriminative power. These results indicate that naive Bayesian classifiers are promising tools for solving diagnostic problems in the veterinary field.},
  AUTHOR       = {P. L. Geenen and Linda C. van der Gaag and Willie L. A. Loeffen and Armin R. W. Elbers},
  DOI          = {10.1016/j.rvsc.2010.08.006},
  ISSN         = {0034-5288},
  JOURNALTITLE = {Research in Veterinary Science},
  KEYWORDS     = {Naive Bayesian classifiers; Probabilistic models; Clinical diagnosis; Classical swine fever},
  LOCALFILE    = {article/Geenen+al-2011.pdf},
  NUMBER       = {1},
  PAGES        = {64–70},
  TITLE        = {Constructing naive Bayesian classifiers for veterinary medicine: A case study in the clinical diagnosis of classical swine fever},
  VOLUME       = {91},
  YEAR         = {2011},
}

@BOOK{Geisser-1993,
  AUTHOR    = {Seymour Geisser},
  NUMBER    = {55},
  PUBLISHER = {Chapman \& Hall},
  SERIES    = {Monographs on Statistics and Applied Probability},
  TITLE     = {Predictive Inference: An Introduction},
  YEAR      = {1993},
}

@ARTICLE{Genest-MacKay-1986,
  AUTHOR       = {Christian Genest and Jock MacKay},
  JOURNALTITLE = {The American Statistician},
  KEYWORDS     = {Archimedean copulas; Fréchet bounds; Kendall's tau; fixed marginals; singular distributions},
  LOCALFILE    = {article/Genest-MacKay-1986.pdf},
  NUMBER       = {4},
  PAGES        = {280–283},
  TITLE        = {The joy of copulas: bivariate distributions with uniform marginals},
  URL          = {http://www.jstor.org/stable/2684602},
  VOLUME       = {40},
  YEAR         = {1986},
}

@ARTICLE{Georgakopoulos-Kavvadias-Papadimitriou-1988,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {George Georgakopoulos and Dimitris Kavvadias and Christos H. Papadimitriou},
  DOI          = {10.1016/0885-064X(88)90006-4},
  JOURNALTITLE = {Journal of Complexity},
  LOCALFILE    = {article/Georgakopoulos-Kavvadias-Papadimitriou-1988.pdf},
  NUMBER       = {1},
  PAGES        = {1–11},
  PUBLISHER    = {Elsevier},
  TITLE        = {Probabilistic satisfiability},
  VOLUME       = {4},
  YEAR         = {1988},
}

@ARTICLE{Ghahramani-2001-HMM+BN-intro,
  ABSTRACT     = {We provide a tutorial on learning and inference in hidden Markov models in the context of the recent literature on Bayesian networks. This perspective makes it possible to consider novel generalizations of hidden Markov models with multiple hidden state variables, multiscale representations, and mixed discrete and continuous variables. Although exact inference in these generalizations is usually intractable, one can use approximate inference algorithms such as Markov chain sampling and variational methods. We describe how such methods are applied to these generalized hidden Markov models. We conclude this review with a discussion of Bayesian methods for model selection in generalized HMMs.},
  AUTHOR       = {Zoubin Ghahramani},
  DOI          = {10.1142/S0218001401000836},
  JOURNALTITLE = {International Journal of Pattern Recognition and Artificial Intelligence},
  KEYWORDS     = {Dynamic Bayesian networks; hidden Markov models},
  LOCALFILE    = {article/Ghahramani-2001-HMM+BN-intro.pdf},
  NUMBER       = {1},
  PAGES        = {9–42},
  TITLE        = {An introduction to hidden Markov models and Bayesian networks},
  VOLUME       = {15},
  YEAR         = {2001},
}

@ARTICLE{Gilbert-DeCooman-Kerre-2003,
  ABSTRACT     = {Probability assessments of events are often linguistic in nature. We model them by means of possibilistic probabilities (a version of Zadeh's fuzzy probabilities with a behavioural interpretation) with a suitable shape for practical implementation (on a computer). Employing the tools of interval analysis and the theory of imprecise probabilities we argue that the verification of coherence for these possibilistic probabilities, the corrections of non-coherent to coherent possibilistic probabilities and their extension to other events and gambles can be performed by finite and exact algorithms. The model can furthermore be transformed into an imprecise first-order model, useful for decision making and statistical inference.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {L. Gilbert and Gert de Cooman and Etienne E. Kerre},
  DOI          = {10.1007/s00500-002-0217-3},
  JOURNALTITLE = {Soft Computing},
  KEYWORDS     = {Fuzzy number; Fuzzy probability; Linguistic uncertainty; Lowest prevision; Possibility measure},
  LOCALFILE    = {article/Gilbert-DeCooman-Kerre-2003.pdf},
  PAGES        = {304–309},
  TITLE        = {Practical implementation of possibilistic probability mass functions},
  VOLUME       = {7},
  YEAR         = {2003},
}

@INCOLLECTION{1995-Gilio-algorithms,
  AUTHOR    = {Angelo Gilio},
  BOOKTITLE = {Mathematical models for handling partial knowledge in artificial intelligence},
  EDITOR    = {Giulianella Coletti and Didier Dubois and Romano Scozzafava},
  ISBN      = {0-306-45076-3},
  LOCALFILE = {incollection/1995-Gilio-algorithms.pdf},
  LOCATION  = {New York, NY, USA},
  PAGES     = {231–254},
  PUBLISHER = {Plenum Press},
  TITLE     = {Algorithms for precise and imprecise conditional probability assessments},
  YEAR      = {1995},
}

@ARTICLE{Gillett-et-al-2007,
  ABSTRACT     = {This article presents a probabilistic logic whose sentences can be interpreted as asserting the acceptability of gambles described in terms of an underlying logic. This probabilistic logic has a concrete syntax and a complete inference procedure, and it handles conditional as well as unconditional probabilities. It synthesizes Nilsson's probabilistic logic and Frisch and Haddawy's anytime inference procedure with Wilson and Moral's logic of gambles. Two distinct semantics can be used for our probabilistic logic: (1) the measure-theoretic semantics used by the prior logics already mentioned and also by the more expressive logic of Fagin, Halpern, and Meggido and (2) a behavioral semantics. Under the measure-theoretic semantics, sentences of our probabilistic logic are interpreted as assertions about a probability distribution over interpretations of the underlying logic. Under the behavioral semantics, these sentences are interpreted only as asserting the acceptability of gambles, and this suggests different directions for generalization.},
  ANNOTATION   = {Reasoning with Imprecise Probabilities},
  AUTHOR       = {Peter R. Gillett and Richard B. Scherl and Glenn Shafer},
  DOI          = {10.1016/j.ijar.2006.07.014},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Anytime deduction; Behavioral semantics; Gambles; Measure–theoretic; Probabilistic logic},
  LOCALFILE    = {article/Gillett-et-al-2007.pdf},
  NUMBER       = {3},
  PAGES        = {281–300},
  TITLE        = {A probabilistic logic based on the acceptability of gambles},
  VOLUME       = {44},
  YEAR         = {2007},
}

@ARTICLE{Giron-Rios-1980,
  ABSTRACT     = {In this paper the theoretical and practical implications of dropping-from the basic Bayesian coherence principles- the assumption of comparability of every pair of acts is examined. The resulting theory is shown to be still perfectly coherent and has Bayesian theory as a particular case. In particular we question the need of weakening or ruling out some of the axioms that constitute the coherence principles; what are their practical implications; how this drive to the notion of partial information or partial uncertainty in a certain sense; how this partial information is combined with sample information and how this relates to Bayesian methods. We also point out the relation of this approach to rational behaviour with the more (and apparently unrelated) general notion of domination structures as applied to multicrieria decision making.},
  AUTHOR       = {F. J. Girón and S. Rios},
  DOI          = {10.1007/BF02888345},
  JOURNALTITLE = {Trabajos de Estadística y de Investigación Operativa},
  LOCALFILE    = {article/Giron-Rios-1980.pdf},
  NUMBER       = {1},
  PAGES        = {17–38},
  TITLE        = {Quasi-Bayesian behaviour: a more realistic approach to decision making?},
  VOLUME       = {31},
  YEAR         = {1980},
}

@ARTICLE{Gneiting-Raftery-2007,
  ABSTRACT     = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  AUTHOR       = {Tilmann Gneiting and Adrian E. Raftery},
  DOI          = {10.1198/016214506000001437},
  JOURNALTITLE = {Journal of the American Statistical Association},
  KEYWORDS     = {Bayes factor; Bregman divergence; Brier score; Coherent; Continuous ranked probability score; Cross-validation; Entropy; Kernel score; Loss function; Minimum contrast estimation; Negative definite function; Prediction interval; Predictive distribution; Quantile forecast; Scoring rule; Skill score; Strictly proper; Utility function},
  LOCALFILE    = {article/Gneiting-Raftery-2007.pdf},
  NUMBER       = {477},
  PAGES        = {359–378},
  PUBLISHER    = {ASA},
  TITLE        = {Strictly proper scoring rules, prediction, and estimation},
  VOLUME       = {102},
  YEAR         = {2007},
}

@ARTICLE{Godfrey-Shipley-Gryz-2007,
  AUTHOR       = {Parke Godfrey and Ryan Shipley and Jarek Gryz},
  DOI          = {10.1007/s00778-006-0029-7},
  ISSN         = {1066-8888},
  JOURNALTITLE = {The VLDB Journal},
  LOCALFILE    = {article/Godfrey-Shipley-Gryz-2007.pdf},
  NUMBER       = {1},
  PAGES        = {5–28},
  PUBLISHER    = {Springer-Verlag},
  TITLE        = {Algorithms and analyses for maximal vector computation},
  VOLUME       = {16},
  YEAR         = {2007},
}

@ARTICLE{Goldberg-1991-float,
  ABSTRACT     = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
  AUTHOR       = {David Goldberg},
  DOI          = {10.1145/103162.103163},
  ISSN         = {0360-0300},
  JOURNALTITLE = {ACM Computing Surveys},
  LOCALFILE    = {article/Goldberg-1991-float.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {5–48},
  PUBLISHER    = {ACM},
  TITLE        = {What every computer scientist should know about floating-point arithmetic},
  URL          = {http://portal.acm.org/citation.cfm?doid=103162.103163},
  VOLUME       = {23},
  YEAR         = {1991},
}

@ARTICLE{Goldstein-1983,
  ABSTRACT     = {We prove that the result EX = E(E(X|Y)) is true, for bounded X, when the usual concept of conditional expectation or prevision is replaced by an alternative definition reflecting an individual's actual beliefs concerning X after observing Y. We discuss the importance of this result to subjectivist theory.},
  AUTHOR       = {Michael Goldstein},
  ISSN         = {0162-1459},
  JOURNALTITLE = {Journal of the American Statistical Association},
  KEYWORDS     = {coherence},
  LOCALFILE    = {article/Goldstein-1983.pdf},
  MONTH        = {12},
  NUMBER       = {384},
  PAGES        = {817–819},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {The prevision of a prevision},
  URL          = {http://www.jstor.org/stable/2288190},
  VOLUME       = {78},
  YEAR         = {1983},
}

@BOOK{Golub-VanLoan-1989,
  AUTHOR    = {Gene H. Golub and Charles F. {Van Loan}},
  EDITION   = {2},
  PUBLISHER = {Johns Hopkins University Press},
  SERIES    = {Johns Hopkins Series in the Mathematical Sciences},
  TITLE     = {Matrix Computation},
  YEAR      = {1989},
}

@MISC{Good-2003,
  AUTHOR = {I. J. Good},
  TITLE  = {The accumulation of imprecise weights of evidence},
  YEAR   = {2003},
}

@ARTICLE{Good-1952,
  ABSTRACT     = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
  AUTHOR       = {I. J. Good},
  ISSN         = {0035-9246},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Good-1952.pdf},
  NUMBER       = {1},
  PAGES        = {107–114},
  PUBLISHER    = {Blackwell Publishing for the Royal Statistical Society},
  TITLE        = {Rational decisions},
  URL          = {http://www.jstor.org/stable/2984087},
  VOLUME       = {14},
  YEAR         = {1952},
}

@ARTICLE{Goodhardt-Ehrenberg-Chatfield-1984,
  ABSTRACT     = {The Dirichlet is a stochastic model of purchase incidence and brand choice which parsimoniously integrates a wide range of already well-established empirical regularities.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {G. J. Goodhardt and A. S. C. Ehrenberg and C. Chatfield},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series A (General)},
  LOCALFILE    = {article/Goodhardt-Ehrenberg-Chatfield-1984.pdf},
  NUMBER       = {5},
  PAGES        = {621–655},
  TITLE        = {The Dirichlet: a comprehensive model of buying behaviour},
  URL          = {http://www.jstor.org/stable/2981696},
  VOLUME       = {147},
  YEAR         = {1984},
}

@ARTICLE{Goodnight-1979,
  ABSTRACT     = {The importance of the SWEEP operator in statistical computing is not so much that it is an inversion technique, but rather that it is a conceptual tool for understanding the least squares process. The SWEEP operator can be programmed to produce generalized inverses and create, as by-products, such items as the Forward Doolittle matrix, the Cholesky decomposition matrix, the Hermite canonical form matrix, the determinant of the original matrix, Type I sums of squares, the error sum of squares, a solution to the normal equations, and the general form of estimable functions. First, this tutorial describes the use of Gauss-Jordan elimination for least squares and continues with a description of a completely generalized sweep operator that computes and stores (X'X)-, (X'X)-X'X, (X'X)-X'Y, and Y'Y - Y'X(X'X)-X'Y, all in the space of a single upper triangular matrix.},
  AUTHOR       = {James H. Goodnight},
  ISSN         = {0003-1305},
  JOURNALTITLE = {The American Statistician},
  LOCALFILE    = {article/Goodnight-1979.pdf},
  NUMBER       = {3},
  PAGES        = {149–158},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {A Tutorial on the SWEEP Operator},
  URL          = {http://www.jstor.org/stable/2683825},
  VOLUME       = {33},
  YEAR         = {1979},
}

@ARTICLE{Grabisch-1995,
  AUTHOR       = {Michel Grabisch},
  JOURNALTITLE = {IEEE Transactions on Fuzzy Systems},
  LOCALFILE    = {article/Grabisch-1995.pdf},
  NUMBER       = {1},
  PAGES        = {96–109},
  TITLE        = {On Equivalence Classes of Fuzzy Connectives—The Case of Fuzzy Integrals},
  VOLUME       = {3},
  YEAR         = {1995},
}

@ARTICLE{Amara-1995-wavelets,
  ANNOTATION   = {op papier},
  AUTHOR       = {Amara Graps},
  JOURNALTITLE = {IEEE Computational Science and Engineering},
  NUMBER       = {2},
  PUBLISHER    = {IEEE Computer Society},
  TITLE        = {An Introduction to Wavelets},
  VOLUME       = {2},
  YEAR         = {1995},
}

@ARTICLE{1996-Greenberg-consistency,
  AUTHOR       = {Harvey J. Greenberg},
  DOI          = {10.1007/BF02284624},
  ISSN         = {1012-2443},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  LOCALFILE    = {articles/1996-Greenberg-consistency.pdf},
  NUMBER       = {1},
  PAGES        = {37–83},
  TITLE        = {Consistency, redundancy, and implied equalities in linear systems},
  VOLUME       = {17},
  YEAR         = {1996},
}

@BOOK{Greenleaf-1969,
  AUTHOR    = {F. P. Greenleaf},
  LOCATION  = {New York},
  OWNER     = {gert},
  PUBLISHER = {Van Nostrand},
  TITLE     = {Invariant Means on Topological Groups and Their Applications},
  YEAR      = {1969},
}

@REPORT{Griffiths-Ghahramani-2005,
  AUTHOR      = {Thomas L. Griffiths and Zoubin Ghahramani},
  INSTITUTION = {Gatsby Unit, University College London},
  NUMBER      = {GCNU TR 2005-001},
  TITLE       = {Infinite Latent Feature Models and the Indian Buffet Process},
  TYPE        = {techreport},
  YEAR        = {2005},
}

@BOOK{Grinstead-Snell-2006-probintro,
  ANNOTATION = {GNU FDL version, source available},
  AUTHOR     = {Charles M. Grinstead and J. Laurie Snell},
  PUBLISHER  = {American Mathematical Society},
  TITLE      = {Introduction to Probability},
  YEAR       = {2006},
}

@BOOK{Grunbaum-1967,
  AUTHOR    = {Branko Grünbaum},
  LOCATION  = {London},
  PUBLISHER = {Interscience Publishers},
  TITLE     = {Convex polytopes},
  YEAR      = {1967},
}

@ARTICLE{Grunbaum-Shepard-1969,
  AUTHOR       = {Branko Grünbaum and G. C. Shepard},
  DOI          = {10.1112/blms},
  JOURNALTITLE = {Bulletin of the London Mathematical Society},
  LOCALFILE    = {article/Grunbaum-Shepard-1969.pdf},
  PAGES        = {257–300},
  PUBLISHER    = {Springer Verlag},
  TITLE        = {Convex polytopes},
  VOLUME       = {1},
  YEAR         = {1969},
}

@ARTICLE{Grunwald-Halpern-2011,
  AUTHOR       = {Peter D. Grünwald and Joseph Y. Halpern},
  DOI          = {10.1613/jair.3374},
  JOURNALTITLE = {Journal of Artificial Intelligence Research},
  PAGES        = {393–426},
  TITLE        = {Making decisions using sets of probabilities: updating, time consistency, and calibration.},
  VOLUME       = {42},
  YEAR         = {2011},
}

@ARTICLE{GutierrezPena-Rueda-2003,
  ABSTRACT     = {Reference analysis, introduced by Bernardo (J. Roy. Statist. Soc. 41 (1979) 113) and further developed by Berger and Bernardo (On the development of reference priors (with discussion). In: J.M. Bernardo, J.O. Berger, A.P. Dawid, A.F.M. Smith (Eds.), Bayesian Statistics, Vol. 4, Clarendon Press, Oxford, pp. 35-60), has proved to be one of the most successful general methods to derive noninformative prior distributions. In practice, however, reference priors are typically difficult to obtain. In this paper we show how to find reference priors for a wide class of exponential family likelihoods.},
  AUTHOR       = {Eduardo Gutiérrez-Peña and R. Rueda},
  DOI          = {10.1016/S0378-3758(01)00281-6},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Affine dual foliations; Bayesian inference; Natural exponential family; Quadratic variance function; Reference prior; cut},
  LOCALFILE    = {article/GutierrezPena-Rueda-2003.pdf},
  NUMBER       = {1-2},
  PAGES        = {35–54},
  TITLE        = {Reference priors for exponential families},
  VOLUME       = {110},
  YEAR         = {2003},
}

@ARTICLE{GutierrezPena-Smith-1997-review,
  AUTHOR       = {Eduardo Gutiérrez-Peña and Adrian F. M. Smith},
  DOI          = {10.1007/BF02564426},
  JOURNALTITLE = {Test},
  LOCALFILE    = {article/GutierrezPena-Smith-1997-review.pdf},
  NUMBER       = {1},
  PAGES        = {1–90},
  PUBLISHER    = {Springer},
  TITLE        = {Exponential and Bayesian conjugate families: review and extensions},
  VOLUME       = {6},
  YEAR         = {1997},
}

@ARTICLE{GuitierrezPena-Smith-1995,
  ANNOTATION   = {met errata, ook op papier},
  AUTHOR       = {Eduardo Gutiérrez-Peña and Adrian F. M. Smith},
  JOURNALTITLE = {Journal of the American Statistical Association},
  KEYWORDS     = {Bayesian inference; Conjugate prior; Jeffreys's prior; Quadratic variance function},
  LOCALFILE    = {article/GutierrezPena-Smith-1995.pdf},
  NUMBER       = {432},
  PAGES        = {1347–1356},
  TITLE        = {Conjugate parameterizations for natural exponential families},
  URL          = {http://www.jstor.org/stable/2291525},
  VOLUME       = {90},
  YEAR         = {1995},
}

@ARTICLE{Ha-etal-1998,
  AUTHOR       = {Vu A. Ha and AnHai Doan and Van H. Vu and Peter Haddawy},
  DOI          = {10.1023/A:1018936829318},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  LOCALFILE    = {article/Ha-etal-1998.pdf},
  PAGES        = {1–21},
  TITLE        = {Geometric foundations for interval-based probabilities},
  VOLUME       = {24},
  YEAR         = {1998},
}

@ARTICLE{Haddad-Moreaux-2007,
  ABSTRACT     = {Performance evaluation of complex systems is a critical issue and bounds computation provides confidence about service quality, reliability, etc. of such systems. The stochastic ordering theory has generated a lot of works on bounds computation. Maximal lower and minimal upper bounds of a Markov chain by a st-monotone one exist and can be efficiently computed. In the present work, we extend simultaneously this last result in two directions. On the one hand, we handle the case of a maximal monotone lower bound of a family of Markov chains where the coefficients are given by numerical intervals. On the other hand, these chains are sub-chains associated to sub-stochastic matrices. We prove the existence of this maximal bound and we provide polynomial time algorithms to compute it both for discrete and continuous Markov chains. Moreover, it appears that the bounding sub-chain of a family of strictly sub-stochastic ones is not necessarily strictly sub-stochastic. We establish a characterization of the families of sub-chains for which these bounds are strictly sub-stochastic. Finally, we show how to apply these results to a classical model of repairable system. A forthcoming paper will present detailed numerical results and comparison with other methods.},
  AUTHOR       = {Serge Haddad and Patrice Moreaux},
  DOI          = {10.1016/j.ejor.2005.08.016},
  ISSN         = {0377-2217},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {Markov process; Stochastic bound; Stochastic process; Strong stochastic ordering; Sub-Markov chain},
  LOCALFILE    = {article/Haddad-Moreaux-2007.pdf},
  NUMBER       = {2},
  PAGES        = {999–1015},
  TITLE        = {Sub-stochastic matrix analysis for bounds computation–Theoretical results},
  VOLUME       = {176},
  YEAR         = {2007},
}

@ARTICLE{1965-Hailperin-bounds,
  AUTHOR       = {Theodore Hailperin},
  ISSN         = {00029890},
  JOURNALTITLE = {The American Mathematical Monthly},
  LANGUAGE     = {English},
  LOCALFILE    = {article/1965-Hailperin-bounds.pdf},
  NUMBER       = {4},
  PAGES        = {343–359},
  TITLE        = {Best Possible Inequalities for the Probability of a Logical Function of Events},
  URL          = {http://www.jstor.org/stable/2313491},
  VOLUME       = {72},
  YEAR         = {1965},
}

@ARTICLE{Haldane-1948,
  AUTHOR       = {J. B. S. Haldane},
  JOURNALTITLE = {Biometrika},
  LOCALFILE    = {article/Haldane-1948.pdf},
  NUMBER       = {3–4},
  PAGES        = {297–300},
  TITLE        = {The precision of observed values of small frequencies},
  URL          = {http://www.jstor.org/stable/2332350},
  VOLUME       = {35},
  YEAR         = {1948},
}

@ARTICLE{Haldane-1945,
  AUTHOR       = {J. B. S. Haldane},
  JOURNALTITLE = {Biometrika},
  LOCALFILE    = {article/Haldane-1945.pdf},
  NUMBER       = {3},
  PAGES        = {222–225},
  TITLE        = {On a method of estimating frequencies},
  URL          = {http://www.jstor.org/stable/2332299},
  VOLUME       = {33},
  YEAR         = {1945},
}

@ARTICLE{Hall-2006-sensitivity-indices,
  ABSTRACT     = {An uncertainty-based sensitivity index represents the contribution that uncertainty in model input Xi makes to the uncertainty in model output Y. This paper addresses the situation where the uncertainties in the model inputs are expressed as closed convex sets of probability measures, a situation that exists when inputs are expressed as intervals or sets of intervals with no particular distribution specified over the intervals, or as probability distributions with interval-valued parameters. Three different approaches to measuring uncertainty, and hence uncertainty-based sensitivity, are explored. Variance-based sensitivity analysis (VBSA) estimates the contribution that each uncertain input, acting individually or in combination, makes to variance in the model output. The partial expected value of perfect information (partial EVPI), quantifies the (financial) value of learning the true numeric value of an input. For both of these sensitivity indices the generalization to closed convex sets of probability measures yields lower and upper sensitivity indices. Finally, the use of relative entropy as an uncertainty-based sensitivity index is introduced and extended to the imprecise setting, drawing upon recent work on entropy measures for imprecise information.},
  AUTHOR       = {Jim W. Hall},
  DOI          = {10.1016/j.ress.2005.11.042},
  ISSN         = {0951-8320},
  JOURNALTITLE = {Reliability Engineering \& System Safety},
  KEYWORDS     = {Coherent lower and upper probabilities; Entropy-based sensitivity indices; Generalized information theory; Partial expected value of perfect information; Variance-based sensitivity indices},
  LOCALFILE    = {article/Hall-2006-sensitivity-indices.pdf},
  MONTH        = {10},
  NUMBER       = {10-11},
  PAGES        = {1443–1451},
  PUBLISHER    = {Elsevier},
  TITLE        = {Uncertainty-based sensitivity indices for imprecise probability distributions},
  VOLUME       = {91},
  YEAR         = {2006},
}

@ARTICLE{Hall-Lawry-2004-approx,
  ABSTRACT     = {Random set theory provides a convenient mechanism for representing uncertain knowledge including probabilistic and set-based information, and extending it through a function. This paper focuses upon the situation when the available information is in terms of coherent lower and upper probabilities, which are encountered, for example, when a probability distribution is specified by interval parameters. We propose an Iterative Rescaling Method (IRM) for constructing a random set with corresponding belief and plausibility measures that are a close outer approximation to the lower and upper probabilities. The approach is compared with the discrete approximation method of Williamson and Downs (sometimes referred to as the p-box), which generates a closer approximation to lower and upper cumulative probability distributions but in most cases a less accurate approximation to the lower and upper probabilities on the remainder of the power set. Four combination methods are compared by application to example random sets generated using the IRM.},
  AUTHOR       = {Jim W. Hall and Jonathan Lawry},
  DOI          = {10.1016/j.ress.2004.03.005},
  JOURNALTITLE = {Reliability Engineering \& System Safety},
  KEYWORDS     = {Coherent lower and upper probabilities; Dempster–Shafer theory; Iterative rescaling method; Möbius inversion; Random set theory; p-Box},
  LOCALFILE    = {article/Hall-Lawry-2004-approx.pdf},
  PAGES        = {89–101},
  TITLE        = {Generation, combination and extension of random set approximations to coherent lower and upper probabilities},
  VOLUME       = {85},
  YEAR         = {2004},
}

@ARTICLE{Halmos-1970-howto,
  AUTHOR       = {P. R. Halmos},
  JOURNALTITLE = {L’Enseignement mathématique},
  LOCALFILE    = {article/Halmos-1970-howto.pdf},
  NUMBER       = {2},
  TITLE        = {How to write mathematics},
  VOLUME       = {XVIII},
  YEAR         = {1970},
}

@ARTICLE{Halpern-2010,
  ABSTRACT     = {The relationship between Popper spaces (conditional probability spaces that satisfy some regularity conditions), lexicographic probability systems (LPS's), and nonstandard probability spaces (NPS's) is considered. If countable additivity is assumed, Popper spaces and a subclass of LPS's are equivalent; without the assumption of countable additivity, the equivalence no longer holds. If the state space is finite, LPS's are equivalent to NPS's. However, if the state space is infinite, NPS's are shown to be more general than LPS's.},
  AUTHOR       = {Joseph Y. Halpern},
  DOI          = {10.1016/j.geb.2009.03.013},
  ISSN         = {0899-8256},
  JOURNALTITLE = {Games and Economic Behavior},
  LOCALFILE    = {article/Halpern-2010.pdf},
  NUMBER       = {1},
  PAGES        = {155–179},
  TITLE        = {Lexicographic probability, conditional probability, and nonstandard probability},
  VOLUME       = {68},
  YEAR         = {2010},
}

@ARTICLE{Halpern-Koller-2004,
  ABSTRACT     = {Non-deductive reasoning systems are often representation dependent: representing the same situation in two di erent ways may cause such a system to return two different answers. Some have viewed this as a significant problem. For example, the principle of maximum entropy has been subjected to much criticism due to its representation dependence. There has, however, been almost no work investigating representation dependence. In this paper, we formalize this notion and show that it is not a problem specific to maximum entropy. In fact, we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment, in a precise sense. Moreover, we show that representation independence is incompatible with even a weak default assumption of independence. We then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata, and provide a construction of a family of inference procedures that provides such restricted representation independence, using relative entropy.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Joseph Y. Halpern and Daphne Koller},
  DOI          = {10.1613/jair.1292},
  JOURNALTITLE = {Journal of Artificial Intelligence Research},
  LOCALFILE    = {article/Halpern-Koller-2004.pdf},
  PAGES        = {319–356},
  TITLE        = {Representation Dependence in Probabilistic Inference},
  VOLUME       = {21},
  YEAR         = {2004},
}

@ARTICLE{Hammer-1955,
  AUTHOR       = {Preston C. Hammer},
  DOI          = {10.1215/S0012-7094-55-02209-2},
  JOURNALTITLE = {Duke Mathematical Journal},
  LOCALFILE    = {article/Hammer-1955.pdf},
  NUMBER       = {1},
  PAGES        = {103–106},
  PUBLISHER    = {Duke University Press},
  TITLE        = {Maximal convex sets},
  VOLUME       = {22},
  YEAR         = {1955},
}

@ARTICLE{Hampel2009,
  ABSTRACT     = {The paper gives a short survey about the occurrence (sometimes hidden in the background) of nonadditive probabilities in statistics. It starts with the original meaning of ?probability? in statistics in the Ars Conjectandi by Jakob (James) Bernoulli, and the ensuing misunderstanding which gave the term its present meaning. One chapter is about robustness theory, its use of (nonadditive) Choquet-capacities, and an attempt to clarify some widespread misunderstandings about it, which have consequences for the use of upper and lower probabilities. Also the uncertainty about model choice (including the conflict between purely mathematical reasoning and good statistical practice) and treatment of outliers is briefly discussed. The partial arbitrariness of additivity both in Bayes' famous Scholium and in modern Bayes theory is outlined. The infamous and almost forgotten fiducial probabilities can actually be corrected and find their place in a more general paradigm using upper and lower probabilities. Finally, a new (?) qualitative theory of inference is mentioned which may contain some essentials of inductive reasoning in real life.},
  AUTHOR       = {Frank Hampel},
  DOI          = {10.1080/15598608.2009.10411908},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Ars Conjectandi; Bayes’ Scholium; Inaccuracy and uncertainty; Misleading logic in data analysis; Misunder- standings about robustness theory; Nonadditive probabilites; Original meaning of probability; Proper fiducial probabilities; Qualitative reasoning in real life.},
  LOCALFILE    = {article/Hampel-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {11–23},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Nonadditive Probabilities in Statistics},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Harsanyi-1982-comment,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {John C. Harsanyi},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Harsanyi-1982-comment.pdf},
  NUMBER       = {2},
  PAGES        = {120–124},
  TITLE        = {Subjective probability and the theory of games: Comments on Kadane and Larkey's paper},
  URL          = {http://www.jstor.org/stable/2631295},
  VOLUME       = {28},
  YEAR         = {1982},
}

@ARTICLE{Harsanyi-1982-rejoinder,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {John C. Harsanyi},
  DOI          = {10.1287/mnsc.28.2.124a},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Harsanyi-1982-rejoinder.pdf},
  NUMBER       = {2},
  PAGES        = {124–125},
  PUBLISHER    = {INFORMS},
  TITLE        = {Rejoinder to professors Kadane and Larkey},
  VOLUME       = {28},
  YEAR         = {1982},
}

@ARTICLE{Hart-MasColell-2001,
  AUTHOR       = {Sergiu Hart and Andreu Mas-Colell},
  DOI          = {10.1006/jeth.2000.2746},
  JOURNALTITLE = {Journal of Economic Theory},
  LOCALFILE    = {article/Hart-MasColell-2001.pdf},
  PAGES        = {26–54},
  TITLE        = {A General Class of Adaptive Strategies},
  VOLUME       = {98},
  YEAR         = {2001},
}

@BOOK{Hartfiel-1998-book,
  AUTHOR    = {Darald J. Hartfiel},
  NUMBER    = {1695},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Mathematics},
  TITLE     = {Markov Set-Chains},
  URL       = {http://books.google.com/books?id=79wZAQAAIAAJ},
  YEAR      = {1998},
}

@ARTICLE{Hartfiel-1991,
  ABSTRACT     = {Let T be a non-empty subset of n x n stochastic matrices. Define T2={A1A2 | A1,A2∈T}, T3= {A1A2A3 | A1,A2,A3∈T}, ⋯. The sequence T1,T2,⋯ is called a Markov set-chain. An important problem in this area is to determine when such a set-chain converges. This paper gives a notion of a sequential limiting set and shows how it can be used to obtain a result on set-chain convergence.},
  AUTHOR       = {Darald J. Hartfiel},
  ISSN         = {0021-9002},
  JOURNALTITLE = {Journal of Applied Probability},
  LOCALFILE    = {article/Hartfiel-1991.pdf},
  NUMBER       = {4},
  PAGES        = {910–913},
  PUBLISHER    = {Applied Probability Trust},
  TITLE        = {Sequential limits in Markov set-chains},
  URL          = {http://www.jstor.org/stable/3214695},
  VOLUME       = {28},
  YEAR         = {1991},
}

@ARTICLE{Hartfiel-Seneta-1994,
  ABSTRACT     = {In the theory of homogeneous Markov chains, states are classified according to their connectivity to other states and this classification leads to a classification of the Markov chains themselves. In this paper we classify Markov set-chains analogously, particularly into ergodic, regular, and absorbing Markov set-chains. A weak law of large numbers is developed for regular Markov set-chains. Examples are used to illustrate analysis of behavior of Markov set-chains.},
  AUTHOR       = {Darald J. Hartfiel and E. Seneta},
  ISSN         = {0001-8678},
  JOURNALTITLE = {Advances in Applied Probability},
  LOCALFILE    = {article/Hartfiel-Seneta-1994.pdf},
  NUMBER       = {4},
  PAGES        = {947–964},
  PUBLISHER    = {Applied Probability Trust},
  TITLE        = {On the theory of Markov set-chains},
  URL          = {http://www.jstor.org/stable/1427899},
  VOLUME       = {26},
  YEAR         = {1994},
}

@ARTICLE{Hartman-Watson-1974,
  AUTHOR       = {Philip Hartman and Geoffrey S. Watson},
  DOI          = {10.1214/aop/1176996606},
  JOURNALTITLE = {The Annals of Probability},
  LOCALFILE    = {article/Hartman-Watson-1974.pdf},
  MONTH        = {08},
  NUMBER       = {4},
  PAGES        = {593–607},
  TITLE        = {“Normal” distribution functions on spheres and the modified Bessel functions},
  VOLUME       = {2},
  YEAR         = {1974},
}

@BOOK{Hatcher-2002,
  AUTHOR    = {Allan Hatcher},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Algebraic Topology},
  YEAR      = {2002},
}

@ARTICLE{Heath-Sudderth-1976-exchangeability,
  ABSTRACT     = {A simple proof is given for de Finetti's theorem that every sequence of exchangeable 0-1 random variables is a probability mixture of sequences of independent, identically distributed variables. The proof can easily be presented to seniors or first year graduate students of mathematical statistics and should aid them in understanding the relationship between the classical and the Bayesian point of view.},
  AUTHOR       = {David Heath and William Sudderth},
  JOURNALTITLE = {The American Statistician},
  LOCALFILE    = {article/Heath-Sudderth-1976-exchangeability.pdf},
  NUMBER       = {4},
  PAGES        = {188–189},
  TITLE        = {De Finetti's theorem on exchangeable variables},
  URL          = {http://www.jstor.org/stable/2683760},
  VOLUME       = {30},
  YEAR         = {1976},
}

@ARTICLE{Helsper+Van_der_Gaag-2007,
  ABSTRACT     = {Building a probabilistic network for a real-life domain of application is a hard and time-consuming process, which is generally performed with the help of domain experts. As the scope and, hence, the size and complexity of networks are increasing, the need for proper management of the elicited domain knowledge becomes apparent. To study the usefulness of ontologies for this purpose, we constructed an ontology for the domain of oesophageal cancer, based on a real-life probabilistic network for the staging of cancer of the oesophagus and the knowledge elicited for its construction. In this paper, we describe the various components of our ontology and outline the benefits of using ontologies in engineering probabilistic networks.},
  AUTHOR       = {Eveline M. Helsper and Linda C. van der Gaag},
  DOI          = {10.1017/S0269888907001038},
  ISSN         = {1469-8005},
  JOURNALTITLE = {The Knowledge Engineering Review},
  LOCALFILE    = {article/Helsper+Van_der_Gaag-2007.pdf},
  MONTH        = {2},
  NUMBER       = {1},
  PAGES        = {67–86},
  TITLE        = {Ontologies for probabilistic networks: a case study in the oesophageal-cancer domain},
  VOLUME       = {22},
  YEAR         = {2007},
}

@INPROCEEDINGS{2004-Helsper++-acquisition,
  AUTHOR    = {Eveline M. Helsper and Linda C. van der Gaag and F. Groenendaal},
  BOOKTITLE = {Engineering Knowledge in the Age of the Semantic Web},
  EDITOR    = {E. Motta and N. R. Shadbolt and A. Stutt and N. Gibbins},
  PAGES     = {280–292},
  PUBLISHER = {Springer},
  TITLE     = {Designing a procedure for the acquisition of probability constraints for Bayesian networks},
  YEAR      = {2004},
}

@INCOLLECTION{Henk-RichterGebert-Ziegler-1997,
  AUTHOR    = {Martin Henk and Jürgen Richter-Gebert and Günter M. Ziegler},
  BOOKTITLE = {Handbook of Discrete and Computational Geometry},
  EDITOR    = {J. E. Goodman and J. O'Rourke},
  PAGES     = {243–270},
  PUBLISHER = {CRC Press},
  TITLE     = {Basic properties of convex polytopes},
  URL       = {http://fma2.math.uni-magdeburg.de/~henk/preprints/henk; richter-gebert ziegler&basic properties of convex polytopes.pdf},
  YEAR      = {1997},
}

@ARTICLE{Hill-1968-An,
  ABSTRACT     = {A Bayesian approach to inference about the percentiles and other characteristics of a finite population is proposed. The approach does not depend upon, though it need not exclude, the use of parametric models. Some related questions concerning the existence of exchangeable distributions are considered. It is shown that there are no countably additive exchangeable distributions on the space of observations which give ties probability 0 and for which a next observation is conditionally equally likely to fall in any of the open intervals between successive order statistics of a given sample.},
  AUTHOR       = {Bruce M. Hill},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Hill-1968-An.pdf},
  NUMBER       = {322},
  PAGES        = {677–691},
  TITLE        = {Posterior distribution of percentiles: Bayes' theorem for sampling from a population},
  URL          = {http://www.jstor.org/stable/2284038},
  VOLUME       = {63},
  YEAR         = {1968},
}

@ARTICLE{Hipp-1974,
  AUTHOR       = {Christian Hipp},
  DOI          = {10.1214/aos/1176342879},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Hipp-1974.pdf},
  MONTH        = {11},
  NUMBER       = {6},
  PAGES        = {1283–1292},
  TITLE        = {Sufficient statistics and exponential families},
  VOLUME       = {2},
  YEAR         = {1974},
}

@REPORT{kam981,
  AUTHOR      = {Milan Hladík},
  INSTITUTION = {KAM-DIMATIA Series},
  NUMBER      = {981},
  TITLE       = {Interval linear programming: A survey},
  TYPE        = {techreport},
  YEAR        = {2010},
}

@REPORT{Hofbauer-1995,
  AUTHOR      = {Josef Hofbauer},
  INSTITUTION = {Collegium Budapest},
  TITLE       = {Stability for the Best Response Dynamics},
  TYPE        = {techreport},
  YEAR        = {1995},
}

@ARTICLE{Hofbauer-Sigmund-2003,
  AUTHOR       = {Josef Hofbauer and Karl Sigmund},
  JOURNALTITLE = {Bulletin of the American Mathematical Society},
  NUMBER       = {4},
  PAGES        = {479–519},
  TITLE        = {Evolutionary game dynamics},
  VOLUME       = {40},
  YEAR         = {2003},
}

@BOOK{Holmes-1975,
  AUTHOR    = {Richard B. Holmes},
  LOCATION  = {New York},
  NUMBER    = {24},
  PUBLISHER = {Springer-Verlag},
  SERIES    = {Graduate Texts in Mathematics},
  TITLE     = {Geometric Functional Analysis and its Applications},
  YEAR      = {1975},
}

@BOOK{Hsu-1995-sysensig,
  AUTHOR    = {Hwei Piao Hsu},
  ISBN      = {0-07-030641-9},
  LOCALFILE = {book/Hsu-1995-sysensig.pdf},
  PUBLISHER = {McGraw-Hill},
  SERIES    = {Schaum's Outline Series},
  TITLE     = {Schaum's outline of theory and problems of signals and systems},
  YEAR      = {1995},
}

@ARTICLE{Hsu-et-al-2005,
  ABSTRACT     = {Much is known about how people make decisions under varying levels of probability (risk). Less is known about the neural basis of decision-making when probabilities are uncertain because of missing information (ambiguity). In decision theory, ambiguity about probabilities should not affect choices. Using functional brain imaging, we show that the level of ambiguity in choices correlates positively with activation in the amygdala and orbitofrontal cortex, and negatively with a striatal system. Moreover, striatal activity correlates positively with expected reward. Neurological subjects with orbitofrontal lesions were insensitive to the level of ambiguity and risk in behavioral choices. These data suggest a general neural circuit responding to degrees of uncertainty, contrary to decision theory.},
  AUTHOR       = {Ming Hsu and Meghana Bhatt and Ralph Adolphs and Daniel Tranel and Colin F. Camerer},
  DOI          = {10.1126/science.1115327},
  JOURNALTITLE = {Science},
  LOCALFILE    = {article/Hsu-et-al-2005.pdf},
  MONTH        = {12},
  NUMBER       = {5754},
  PAGES        = {1680–1683},
  PUBLISHER    = {American Association for the Advancement of Science},
  TITLE        = {Neural systems responding to degrees of uncertainty in human decision-making},
  VOLUME       = {310},
  YEAR         = {2005},
}

@ARTICLE{Huang-Huang-Tsai-2006-Hilbert-metric,
  ABSTRACT     = {The purpose of this paper is to study the eigenvalue problems for a class of positive nonlinear operators. Using projective metric techniques and the contraction mapping principle, we establish existence, uniqueness and continuity results for positive eigensolutions of a particular type of positive nonlinear operator. In addition, we prove the existence of a unique fixed point of the operator with explicit norm-estimates. Applications to nonlinear systems of equations and to matrix equations are considered.},
  AUTHOR       = {Min-Jei Huang and Chao-Ya Huang and Tzong-Mo Tsai},
  DOI          = {10.1016/j.laa.2005.08.024},
  JOURNALTITLE = {Linear Algebra and its Applications},
  KEYWORDS     = {Concave operator; Cone; Eigenvalue problem; Hilbert’s projective metric; Matrix equation; Nonlinear system of equations},
  LOCALFILE    = {article/Huang-Huang-Tsai-2006-Hilbert-metric.pdf},
  NUMBER       = {1},
  PAGES        = {202–211},
  TITLE        = {Applications of Hilbert's projective metric to a class of positive nonlinear operators},
  VOLUME       = {413},
  YEAR         = {2006},
}

@ARTICLE{1974-Huber+Strassen-correction,
  AUTHOR       = {Peter J. Huber and Volker Strassen},
  DOI          = {10.1214/aos/1176342630},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/1974-Huber+Strassen-correction.pdf},
  NOTE         = {Correction of: \cite{1973-Huber+Strassen}},
  NUMBER       = {1},
  PAGES        = {223–224},
  TITLE        = {Note: Correction to Minimax Tests and the Neyman-Pearson Lemma for Capacities},
  VOLUME       = {2},
  YEAR         = {1974},
}

@ARTICLE{1973-Huber+Strassen,
  AUTHOR       = {Peter J. Huber and Volker Strassen},
  DOI          = {10.1214/aos/1176342363},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Huber-Strassen-1973.pdf},
  NOTE         = {Correction: \cite{1974-Huber+Strassen-correction}},
  NUMBER       = {2},
  PAGES        = {251–263},
  TITLE        = {Minimax tests and the Neyman-Pearson lemma for capacities},
  VOLUME       = {1},
  YEAR         = {1973},
}

@BOOK{Hume-1739,
  AUTHOR    = {David Hume},
  EDITION   = {Annotated},
  EDITOR    = {David Fate Norton and Mary J. Norton},
  PUBLISHER = {Oxford University Press},
  SERIES    = {Oxford Philosophical Texts},
  TITLE     = {A treatise of human nature},
  YEAR      = {1739},
}

@REPORT{Hutter-2006,
  AUTHOR      = {Marcus Hutter},
  INSTITUTION = {IDSIA},
  LOCATION    = {Manno-Lugano, Switzerland},
  NUMBER      = {IDSIA-03-06},
  TITLE       = {On the Foundations of Universal Sequence Prediction},
  TYPE        = {techreport},
  YEAR        = {2006},
}

@BOOK{Huygens-1920,
  AUTHOR    = {Christiaan Huygens},
  PUBLISHER = {Martinus Nijhoff},
  TITLE     = {Oevres complètes de Christiaan Huygens},
  VOLUME    = {14},
  YEAR      = {1920},
}

@BOOK{Huzurbazar-1976,
  AUTHOR    = {Vasant S. Huzurbazar},
  EDITOR    = {Anant M. Kshirsagar},
  LOCATION  = {New York},
  PUBLISHER = {Marcel Dekker},
  SERIES    = {Statistics: Textbooks and Monographs},
  TITLE     = {Sufficient Statistics: Selected Contributions},
  VOLUME    = {19},
  YEAR      = {1976},
}

@ARTICLE{Ide-Cozman-2008,
  ABSTRACT     = {This paper presents a family of algorithms for approximate inference in credal networks (that is, models based on directed acyclic graphs and set-valued probabilities) that contain only binary variables. Such networks can represent incomplete or vague beliefs, lack of data, and disagreements among experts; they can also encode models based on belief functions and possibilistic measures. All algorithms for approximate inference in this paper rely on exact inferences in credal networks based on polytrees with binary variables, as these inferences have polynomial complexity. We are inspired by approximate algorithms for Bayesian networks; thus the Loopy 2U algorithm resembles Loopy Belief Propagation, while the Iterated Partial Evaluation and Structured Variational 2U algorithms are, respectively, based on Localized Partial Evaluation and variational techniques.},
  AUTHOR       = {Jaime Shinsuke Ide and Fabio Gagliardi Cozman},
  DOI          = {10.1016/j.ijar.2007.09.003},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {2U algorithm; Credal networks; Loopy Belief Propagation; Variational methods},
  LOCALFILE    = {article/Ide-Cozman-2008.pdf},
  NUMBER       = {1},
  PAGES        = {275–296},
  TITLE        = {Approximate algorithms for credal networks with binary variables},
  VOLUME       = {48},
  YEAR         = {2008},
}

@ARTICLE{Inuiguchi-2007,
  ABSTRACT     = {In this paper, we treat fuzzy linear programming problems with uncertain parameters whose ranges are specified as fuzzy polytopes. The problem is formulated as a necessity measure optimization model. It is shown that the problem can be reduced to a semi-infinite programming problem and solved by a combination of a bisection method and a relaxation procedure. An algorithm in which the bisection method and the relaxation procedure converge simultaneously is proposed. A simple numerical example is given to illustrate the solution procedure.},
  AUTHOR       = {Masahiro Inuiguchi},
  DOI          = {10.1016/j.fss.2007.04.004},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {Bisection method; Fuzzy polytope; Necessity measure; Possibilistic linear programming; Relaxation procedure; Semi-infinite programming},
  LOCALFILE    = {article/Inuiguchi-2007.pdf},
  NUMBER       = {17},
  PAGES        = {1882–1891},
  TITLE        = {Necessity measure optimization in linear programming problems with fuzzy polytopes},
  VOLUME       = {158},
  YEAR         = {2007},
}

@ARTICLE{Inuiguchi-2006,
  AUTHOR       = {Masahiro Inuiguchi},
  EPRINT       = {10338.dmlcz/135726},
  EPRINTTYPE   = {hdl},
  JOURNALTITLE = {Kybernetika},
  KEYWORDS     = {bender; fuzzy linear programming; necessity measure; oblique fuzzy vector; s},
  LOCALFILE    = {article/Inuiguchi-2006.pdf},
  NUMBER       = {4},
  PAGES        = {441–452},
  PUBLISHER    = {THE ACADEMY OF SCIENCES OF THE CZECH REPUBLIC},
  TITLE        = {A necessity measure optimization approach to linear programming problems with oblique fuzzy vectors},
  URL          = {http://hdl.handle.net/10338.dmlcz/135726},
  VOLUME       = {42},
  YEAR         = {2006},
}

@ARTICLE{Inuiguchi-Ramik-2000,
  ABSTRACT     = {In this paper, we review some fuzzy linear programming methods and techniques from a practical point of view. In the first part, the general history and the approach of fuzzy mathematical programming are introduced. Using a numerical example, some models of fuzzy linear programming are described. In the second part of the paper, fuzzy mathematical programming approaches are compared to stochastic programming ones. The advantages and disadvantages of fuzzy mathematical programming approaches are exemplified in the setting of an optimal portfolio selection problem. Finally, some newly developed ideas and techniques in fuzzy mathematical programming are briefly reviewed.},
  AUTHOR       = {Masahiro Inuiguchi and Jaroslav Ramík},
  DOI          = {10.1016/S0165-0114(98)00449-7},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {Fuzzy constraint; Fuzzy goal; Fuzzy mathematical programming; Necessity measure; Portfolio selection; Possibility measure; Simplex method; Stochastic programming},
  LOCALFILE    = {article/Inuiguchi-Ramik-2000.pdf},
  NUMBER       = {1},
  PAGES        = {3–28},
  TITLE        = {Possibilistic linear programming: a brief review of fuzzy mathematical programming and a comparison with stochastic programming in portfolio selection problem},
  VOLUME       = {111},
  YEAR         = {2000},
}

@ARTICLE{Inuiguchi-Sakawa-1997,
  ABSTRACT     = {In this paper, we focus on a treatment of a linear programming problem with an interval objective function. From the viewpoint of the achievement rate, a new solution concept, the maximin achievement rate solution, is proposed. Nice properties of this solution are shown: a maximin achievement rate solution is necessarily optimal when a necessarily optimal solution exists, and if not, then it is still a possibly optimal solution. An algorithm for a maximin achievement rate solution is proposed based on a relaxation procedure together with a simplex method. A numerical example is given to demonstrate the proposed solution algorithm.},
  AUTHOR       = {Masahiro Inuiguchi and M. Sakawa},
  ISSN         = {0160-5682},
  JOURNALTITLE = {Journal of the Operational Research Society},
  KEYWORDS     = {fractional programming},
  LOCALFILE    = {article/Inuiguchi-Sakawa-1997.pdf},
  NUMBER       = {1},
  PAGES        = {25–33},
  PUBLISHER    = {Palgrave Macmillan Journals on behalf of the Operational Research Society},
  TITLE        = {An achievement rate approach to linear programming problems with an interval objective function},
  URL          = {http://www.jstor.org/stable/3009940},
  VOLUME       = {48},
  YEAR         = {1997},
}

@ARTICLE{Ioannidis-et-al-2010-afraid,
  AUTHOR       = {J. P. A. Ioannidis and Athina Tatsioni and F. B. Karassa},
  DOI          = {10.1111/j.1365-2362.2010.02272.x},
  ISSN         = {1365-2362},
  JOURNALTITLE = {European Journal of Clinical Investigation},
  LOCALFILE    = {article/Ioannidis-et-al-2010-afraid.pdf},
  NUMBER       = {4},
  PAGES        = {285–287},
  TITLE        = {Who is afraid of reviewers' comments?: Or, why anything can be published and anything can be cited},
  VOLUME       = {40},
  YEAR         = {2010},
}

@ARTICLE{1977-Isermann-MOLPadj,
  ABSTRACT     = {This paper presents an algorithm for the enumeration of the set of all efficient solutions for a linear multiple objective program. The procedure first finds whether an efficient solution exists for a given linear multiple objective program and then determines an initial efficient basic solution. In a second step the set of all efficient basic solutions and of all efficient extreme rays is established. Finally, the set of all efficient solutions is constructed as a union of a minimal number of convex sets of efficient solutions. The procedure developed in this paper will be illustrated by a numerical example.},
  AUTHOR       = {Heinz Isermann},
  EPRINT       = {3008921},
  EPRINTTYPE   = {jstor},
  ISSN         = {00303623},
  JOURNALTITLE = {Operational Research Quarterly},
  LOCALFILE    = {article/Isermann-1977.pdf},
  NUMBER       = {3},
  PAGES        = {711–725},
  PUBLISHER    = {Palgrave Macmillan Journals on behalf of the Operational Research Society},
  TITLE        = {The enumeration of the set of all efficient solutions for a linear multiple objective program},
  URL          = {http://www.jstor.org/stable/3008921},
  VOLUME       = {28},
  YEAR         = {1977},
}

@BOOK{Ito-Kunisch-2008,
  AUTHOR    = {Kazufumi Ito and Karl Kunisch},
  PUBLISHER = {SIAM},
  TITLE     = {Lagrange Multiplier Approach to Variational Problems and Applications},
  YEAR      = {2008},
}

@THESIS{Jaeger-1995,
  AUTHOR      = {Manfred Jaeger},
  INSTITUTION = {Universität des Saarlandes},
  LOCATION    = {Saarbrücken},
  TITLE       = {Default Reasoning about Probabilities},
  TYPE        = {phdthesis},
  YEAR        = {1995},
}

@ARTICLE{Jaffray-1989,
  AUTHOR       = {Jean-Yves Jaffray},
  DOI          = {10.1016/0167-6377(89)90010-2},
  JOURNALTITLE = {Operations Research Letters},
  LOCALFILE    = {article/Jaffray-1989.pdf},
  NUMBER       = {2},
  PAGES        = {107–112},
  PUBLISHER    = {Elsevier},
  TITLE        = {Linear utility theory for belief functions},
  VOLUME       = {8},
  YEAR         = {1989},
}

@ARTICLE{Jamison-Lodwick-2001,
  ABSTRACT     = {In this paper we begin with a standard form of the linear programming problem. We replace each constant in the problem with a fuzzy number. We then reformat the objective and constraints into an unconstrained fuzzy function by penalizing the objective for possible constraint violations. The range of this fuzzy function lies in the space of fuzzy numbers. The objective is then redefined as optimizing the expected midpoint of the image of this fuzzy function. We show that this objective defines a concave function which, therefore, can be maximized globally. We present an algorithm for finding the optimum.},
  AUTHOR       = {K. David Jamison and Weldon A. Lodwick},
  DOI          = {10.1016/S0165-0114(99)00082-2},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  KEYWORDS     = {Fuzzy function; Fuzzy number; Linear programming; possibility distribution},
  LOCALFILE    = {article/Jamison-Lodwick-2001.pdf},
  NUMBER       = {1},
  PAGES        = {97–110},
  TITLE        = {Fuzzy linear programming using a penalty method},
  VOLUME       = {119},
  YEAR         = {2001},
}

@ARTICLE{Jaumard-Hansende-Aragao-1991,
  AUTHOR       = {Brigitte Jaumard and Pierre Hansen and Marcus Poggi de Aragão},
  DOI          = {10.1287/ijoc.3.2.135},
  JOURNALTITLE = {ORSA Journal on Computing},
  NUMBER       = {2},
  PAGES        = {135–148},
  PUBLISHER    = {INFORMS},
  TITLE        = {Column Generation Methods for Probabilistic Logic},
  VOLUME       = {3},
  YEAR         = {1991},
}

@BOOK{Jaynes-2003,
  AUTHOR    = {E. T. Jaynes},
  EDITOR    = {J. Larry Bretthorst},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Probability Theory: The Logic of Science},
  YEAR      = {2003},
}

@BOOK{Jeffreys-1983,
  AUTHOR    = {Harold Jeffreys},
  EDITION   = {corrected},
  PUBLISHER = {Oxford University Press},
  TITLE     = {Theory of Probability},
  YEAR      = {1983},
}

@INCOLLECTION{Jimenez-etal-2005,
  AUTHOR    = {F. Jiménez and G. Sánchez and J. Cadenas and A. Gómez-Skarmeta and J. Verdegay},
  BOOKTITLE = {Computational Intelligence, Theory and Applications},
  DOI       = {10.1007/3-540-31182-3_66},
  EDITOR    = {Bernd Reusch},
  ISBN      = {978-3-540-22807-3},
  LOCALFILE = {incollection/Jimenez-etal-2005.pdf},
  PAGES     = {713–722},
  PUBLISHER = {Springer},
  SERIES    = {Advances in Soft Computing},
  TITLE     = {Nonlinear Optimization with Fuzzy Constraints by Multi-Objective Evolutionary Algorithms},
  VOLUME    = {33},
  YEAR      = {2005},
}

@INPROCEEDINGS{John-Langley-1995,
  AUTHOR    = {George H. John and Pat Langley},
  BOOKTITLE = {UAI-95: Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
  EDITOR    = {Philippe Besnard and Steve Hanks},
  PAGES     = {338–345},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {Estimating Continuous Distributions in Bayesian Classifiers},
  YEAR      = {1995},
}

@ARTICLE{Johnson-1967,
  ABSTRACT     = {It is shown that the uniqueness of relationship between a regression function and a prior distribution, in Bates-Neyman type models, previously established for linear regressions and gamma prior distributions, is of much more general application.},
  AUTHOR       = {Norman L. Johnson},
  ISSN         = {0162-1459},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Johnson-1967.pdf},
  NUMBER       = {317},
  PAGES        = {288–289},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {Note on a uniqueness relation in certain accident proneness models},
  URL          = {http://www.jstor.org/stable/2282931},
  VOLUME       = {62},
  YEAR         = {1967},
}

@BOOK{Johnson-Kemp-Kotz-2005,
  AUTHOR    = {Norman L. Johnson and Adrienne W. Kemp and Samuel Kotz},
  EDITION   = {3},
  PUBLISHER = {Wiley},
  TITLE     = {Univariate Discrete Distributions},
  YEAR      = {2005},
}

@BOOK{Johnson-Kotz-Balakrishnan-1997,
  AUTHOR    = {Norman L. Johnson and Samuel Kotz and N. Balakrishnan},
  PUBLISHER = {Wiley},
  SERIES    = {Wiley Series in Probability and Statistics},
  TITLE     = {Discrete Multivariate Distributions},
  YEAR      = {1997},
}

@ARTICLE{Johnson-1932,
  AUTHOR       = {W. E. Johnson},
  ISSN         = {0026-4423},
  JOURNALTITLE = {Mind},
  LOCALFILE    = {article/Johnson-1932.pdf},
  NUMBER       = {164},
  PAGES        = {409–423},
  PUBLISHER    = {Oxford University Press on behalf of the Mind Association},
  TITLE        = {Probability: The Deductive and Inductive Problems},
  URL          = {http://www.jstor.org/stable/2250183},
  VOLUME       = {41},
  YEAR         = {1932},
}

@BOOK{Johnson-1924,
  AUTHOR    = {W. E. Johnson},
  LOCATION  = {Cambridge, United Kingdom},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Logic, Part III},
  YEAR      = {1924},
}

@REPORT{Jones-Kerrigan-Maciejowski-2004,
  AUTHOR      = {Colin N. Jones and Eric C. Kerrigan and Jan M. Maciejowski},
  INSTITUTION = {Department of Engineering, University of Cambridge},
  LOCALFILE   = {techreport/Jones-Kerrigan-Maciejowski-2004.pdf},
  MONTH       = {6},
  NUMBER      = {CUED/F-INFENG/TR.463},
  TITLE       = {Equality Set Projection: A new algorithm for the projection of polytopes in halfspace representation},
  TYPE        = {techreport},
  URL         = {http://www-control.eng.cam.ac.uk/~cnj22/docs/resp_mar_04_15.pdf},
  YEAR        = {2004},
}

@MANUAL{DeJongh-html,
  AUTHOR = {Hans de Jong},
  TITLE  = {Handleiding HTML},
  URL    = {http://www.handleidinghtml.nl},
}

@ARTICLE{Jordan-2004-graphical,
  AUTHOR       = {Michael I. Jordan},
  DOI          = {10.1214/088342304000000026},
  JOURNALTITLE = {Statistical Science},
  LOCALFILE    = {article/Jordan-2004-graphical.pdf},
  NUMBER       = {1},
  PAGES        = {140–155},
  TITLE        = {Graphical Models},
  VOLUME       = {19},
  YEAR         = {2004},
}

@ARTICLE{Kadane-Larkey-1982,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Joseph B. Kadane and Patrick D. Larkey},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Kadane-Larkey-1982.pdf},
  NUMBER       = {2},
  PAGES        = {113–120},
  TITLE        = {Subjective probability and the theory of games},
  URL          = {http://www.jstor.org/stable/2631294},
  VOLUME       = {28},
  YEAR         = {1982},
}

@ARTICLE{Kadane-Larkey-1982-reply,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Joseph B. Kadane and Patrick D. Larkey},
  DOI          = {10.1287/mnsc.28.2.124},
  JOURNALTITLE = {Management Science},
  LOCALFILE    = {article/Kadane-Larkey-1982-reply.pdf},
  NUMBER       = {2},
  PAGES        = {124},
  PUBLISHER    = {INFORMS},
  TITLE        = {Reply to Professor Harsanyi},
  URL          = {http://repository.cmu.edu/statistics/38},
  VOLUME       = {28},
  YEAR         = {1982},
}

@BOOK{Kadane-Schervish-Seidenfeld-1999,
  ABSTRACT  = {This important collection of essays is a synthesis of foundational studies in Bayesian decision theory and statistics. An overarching topic of the collection is understanding how the norms for Bayesian decision making should apply in settings with more than one rational decision maker and then tracing out some of the consequences of this turn for Bayesian statistics. There are four principal themes to the collection: cooperative, non-sequential decisions; the representation and measurement of 'partially ordered' preferences; non-cooperative, sequential decisions; and pooling rules and Bayesian dynamics for sets of probabilities. The volume will be particularly valuable to philosophers concerned with decision theory, probability, and statistics, statisticians, mathematicians, and economists.},
  AUTHOR    = {Joseph B. Kadane and Mark J. Schervish and Teddy Seidenfeld},
  PUBLISHER = {Cambridge University Press},
  SERIES    = {Cambridge studies in probability, induction, and decision theory},
  TITLE     = {Rethinking the foundations of statistics},
  URL       = {http://books.google.com/books?id=SsBPTDFwnpoC},
  YEAR      = {1999},
}

@ARTICLE{Kadane-Schervish-Seidenfeld-1996,
  ABSTRACT     = {When can a Bayesian select an hypothesis H and design an experiment (or a sequence of experiments) to make certain that, given the experimental outcome(s), the posterior probability of H will be greater than its prior probability? We discuss an elementary result that establishes sufficient conditions under which this reasoning to a foregone conclusion cannot occur. We illustrate how when the sufficient conditions fail, because probability is finitely but not countably additive, it may be that a Bayesian can design an experiment to lead his/her posterior probability into a foregone conclusion. The problem has a decision theoretic version in which a Bayesian might rationally pay not to see the outcome of certain cost-free experiments, which we discuss from several perspectives. Also, we relate this issue in Bayesian hypothesis testing to various concerns about "optional stopping."},
  AUTHOR       = {Joseph B. Kadane and Mark J. Schervish and Teddy Seidenfeld},
  ISSN         = {0162-1459},
  JOURNALTITLE = {Journal of the American Statistical Association},
  KEYWORDS     = {coherence; finite additivity; sequential tests; stopping rules; value of information},
  LOCALFILE    = {article/Kadane-Schervish-Seidenfeld-1996-foregone.pdf},
  MONTH        = {09},
  NUMBER       = {435},
  PAGES        = {1228–1235},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {Reasoning to a foregone conclusion},
  URL          = {http://www.jstor.org/stable/2291741},
  VOLUME       = {91},
  YEAR         = {1996},
}

@ARTICLE{Kadane-Wolfson-1998-elicitation,
  ABSTRACT     = {Elicitation of expert opinion is becoming increasingly important in the elicitation of prior distributions. In this paper, the psychology of elicitation and the currently available methods are briefly reviewed, but the primary discussion is on the distinction between 'general' elicitation methods for a class of problems and 'application-specific' methods which are useful only once. Examples of both types of elicitation are given, along with a discussion about general versus application-specific methods, and predictive versus structural elicitation.},
  AUTHOR       = {Joseph B. Kadane and Lara J. Wolfson},
  ISSN         = {0039-0526},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  LOCALFILE    = {article/Kadane-Wolfson-1998-elicitation.pdf},
  NUMBER       = {1},
  PAGES        = {3–19},
  PUBLISHER    = {Blackwell Publishing for the Royal Statistical Society},
  TITLE        = {Experiences in Elicitation},
  URL          = {http://www.jstor.org/stable/2988424},
  VOLUME       = {47},
  YEAR         = {1998},
}

@ARTICLE{Kakutani-1937,
  AUTHOR       = {Shizuo Kakutani},
  DOI          = {10.2183/pjab1912.13.93},
  JOURNALTITLE = {Proceedings of the Imperial Academy},
  LOCALFILE    = {article/Kakutani-1937.pdf},
  NUMBER       = {4},
  PAGES        = {93–94},
  TITLE        = {Ein Beweis des Satzes von M. Eidelheit über konvexe Mengen},
  VOLUME       = {13},
  YEAR         = {1937},
}

@BOOK{Kallenberg-2005,
  AUTHOR    = {Olav Kallenberg},
  PUBLISHER = {Springer},
  SERIES    = {probability and Its Applications},
  TITLE     = {Probabilistic Symmetries and Invariance Principles},
  YEAR      = {2005},
}

@ARTICLE{Kaplan-etal-2011,
  ABSTRACT     = {During early stages of development, regulatory proteins bind DNA and control the expression of nearby genes, thereby driving spatial and temporal patterns of gene expression during development. But the biochemical forces that determine where these regulatory proteins bind are poorly understood. We gathered experimental data on the activities of several key regulators of early development of the fruit fly (Drosophila melanogaster) and developed a computational method to predict where and how strongly they will bind. We find that competition, cooperativity, and other interactions among individual regulatory proteins have a limited effect on their binding, while the global accessibility of DNA to protein binding has a significant impact on the binding of all factors. Our results suggest a practical method for predicting regulatory binding by combining experimental DNA accessibility assays with computational algorithms to determine where will binding occur among the accessible regions of the genome.},
  AUTHOR       = {Tommy Kaplan and Li Xiao-Yong and Peter J. Sabo and Sean Thomas and John A Stamatoyannopoulos and Mark D. Biggin and Michael B. Eisen},
  DOI          = {10.1371/journal.pgen.1001290},
  JOURNALTITLE = {PLoS Genetics},
  LOCALFILE    = {article/Kaplan-etal-2011.pdf},
  NUMBER       = {2},
  PAGES        = {e1001290},
  PUBLISHER    = {Public Library of Science},
  TITLE        = {Quantitative Models of the Mechanisms That Control Genome-Wide Patterns of Transcription Factor Binding during Early Drosophila Development},
  VOLUME       = {7},
  YEAR         = {2011},
}

@REPORT{Katzoff-1964,
  ANNOTATION   = {Second Edition ook op papier},
  AUTHOR       = {S. Katzoff},
  ORGANIZATION = {NASA},
  TITLE        = {Clarity in technical reporting},
  TYPE         = {techreport},
  YEAR         = {1964},
}

@REPORT{Kaymak-Sousa-2001,
  ABSTRACT    = {Many practical optimization problems are characterized by some flexibility in the problem constraints, where this flexibility can be exploited for additional trade-off between improving the objective function and satisfying the constraints. Especially in decision making, this type of flexibility could lead to workable solutions, where the goals and the constraints specified by different parties involved in the decision making are traded off against one another and satisfied to various degrees. Fuzzy sets have proven to be a suitable representation for modeling this type of soft constraints. Conventionally, the fuzzy optimization problem in such a setting is defined as the simultaneous satisfaction of the constraints and the goals. No additional distinction is assumed to exist amongst the constraints and the goals. This report proposes an extension of this model for satisfying the problem constraints and the goals, where preference for different constraints and goals can be specified by the decision-maker. The difference in the preference for the constraints is represented by a set of associated weight factors, which influence the nature of trade-off between improving the optimization objectives and satisfying various constraints. Simultaneous weighted satisfaction of various criteria is modeled by using the recently proposed weighted extensions of Archimedean) fuzzy t-norms. The weighted satisfaction of the problem constraints and goals are demonstrated by using a simple general, and it can also be applied to fuzzy mathematical programming problems and multi-objective fuzzy optimization.},
  AUTHOR      = {U. Kaymak and J. M. Sousa},
  INSTITUTION = {Erasmus Universiteit Amsterdam},
  NUMBER      = {ERS-2001-19-LIS},
  TITLE       = {Weighted constraints in fuzzy optimization},
  TYPE        = {techreport},
  URL         = {http://repub.eur.nl/res/pub/85},
  YEAR        = {2001},
}

@BOOK{Kemeny-Snell-1976-markov,
  AUTHOR    = {John G. Kemeny and J. Laurie Snell},
  EDITION   = {2},
  PUBLISHER = {Springer},
  SERIES    = {Undergraduate Texts in Mathematics},
  TITLE     = {Finite Markov Chains},
  URL       = {http://books.google.com/books?id=0bTK5uWzbYwC},
  YEAR      = {1976},
}

@BOOK{Kemeny-Snell-Thompson-1974-finitemathintro,
  AUTHOR    = {John G. Kemeny and J. Laurie Snell and Gerald L. Thompson},
  EDITION   = {3},
  PUBLISHER = {Prentice-Hall},
  TITLE     = {Introduction to Finite Mathematics},
  YEAR      = {1974},
}

@BOOK{Keynes-1921,
  AUTHOR    = {John Maynard Keynes},
  PUBLISHER = {Macmillan},
  TITLE     = {A Treatise on Probability},
  URL       = {http://www.gutenberg.org/ebooks/32625},
  YEAR      = {1921},
}

@ARTICLE{Kieler-etal-1995-pregnancy,
  ABSTRACT     = {Pregnancy length was calculated from ultrasonic measurement of the fetal biparietal diameter (BPD) in the second trimester and compared with the corresponding length calculated from the last menstrual period (LMP) in 86.5 women with exactly 28-day cycles and spontaneous onset of labor. The effects of maternal age, parity, smoking and sex of the child on BPD-based pregnancy length were evaluated in 1998 women with spontaneous onset of labor. The mean pregnancy length was 280.6 days when based on BPD and 283.4 days when based on LIMP. The corresponding median values were 281 days and 284 days, respectively. Probably because of more preterm deliveries, women below 20 years of age had a significantly shorter pregnancy length than women 20–34 years of age. Women carrying girls had a significantly shorter pregnancy length than women carrying boys. There was a tendency to shorter pregnancy lengths among multiparae and among smokers. Copyright © 1995 International Society of Ultrasound in Obstetrics and Gynecology},
  AUTHOR       = {H. Kieler and O. Axelsson and S. Nilsson and U. Waldenström},
  DOI          = {10.1046/j.1469-0705.1995.06050353.x},
  ISSN         = {1469-0705},
  JOURNALTITLE = {Ultrasound in Obstetrics and Gynecology},
  KEYWORDS     = {human pregnancy length; ultrasound biometry},
  LOCALFILE    = {article/Kieler-etal-1995-pregnancy.pdf},
  NUMBER       = {5},
  PAGES        = {353–357},
  PUBLISHER    = {Blackwell Science Ltd.},
  TITLE        = {The length of human pregnancy as calculated by ultrasonographic measurement of the fetal biparietal diameter},
  VOLUME       = {6},
  YEAR         = {1995},
}

@INPROCEEDINGS{Kjaerulff-VanderGaag-2000,
  AUTHOR    = {Uffe Kjærulff and Linda C. van der Gaag},
  BOOKTITLE = {UAI-00: Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
  EDITOR    = {Craig Boutilier and Moisés Goldszmidt},
  ISBN      = {1-55860-709-9},
  PAGES     = {317–325},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {Making sensitivity analysis computationally efficient.},
  YEAR      = {2000},
}

@ARTICLE{Klee-1956,
  AUTHOR       = {Klee, Jr., Victor L.},
  JOURNALTITLE = {Mathematica Scandinavica},
  LOCALFILE    = {article/Klee-1956.pdf},
  PAGES        = {54–64},
  TITLE        = {The structure of semispaces},
  URL          = {http://www.mscand.dk/article.php?id=1449},
  VOLUME       = {4},
  YEAR         = {1956},
}

@ARTICLE{Klee-1955,
  AUTHOR       = {Klee, Jr., Victor L.},
  ISSN         = {0002-9939},
  JOURNALTITLE = {Proceedings of the American Mathematical Society},
  LOCALFILE    = {article/Klee-1955.pdf},
  NUMBER       = {2},
  PAGES        = {313–318},
  PUBLISHER    = {American Mathematical Society},
  TITLE        = {Separation Properties of Convex Cones},
  URL          = {http://www.jstor.org/stable/2032366},
  VOLUME       = {6},
  YEAR         = {1955},
}

@ARTICLE{Klee-1951,
  AUTHOR       = {Klee, Jr., Victor L.},
  DOI          = {10.1215/S0012-7094-51-01835-2},
  JOURNALTITLE = {Duke Mathematical Journal},
  LOCALFILE    = {article/Klee-1951.pdf},
  NUMBER       = {2},
  PAGES        = {443–466},
  TITLE        = {Convex sets in linear spaces},
  VOLUME       = {18},
  YEAR         = {1951},
}

@MISC{Knuth-Larrabee-Roberts-1987,
  ANNOTATION = {Report based on a Stanford University course},
  AUTHOR     = {Donald E. Knuth and Tracy Larrabee and Paul M. Roberts},
  TITLE      = {Mathematical Writing},
  YEAR       = {1987},
}

@BOOK{Koller-Friedman-2009,
  AUTHOR    = {Daphne Koller and Nir Friedman},
  ISBN      = {978-0-262-01319-2},
  PUBLISHER = {MIT Press},
  SERIES    = {Adaptive Computation and Machine Learning},
  TITLE     = {Probabilistic Graphical Models},
  YEAR      = {2009},
}

@BOOK{Kolmogorov-1956,
  ANNOTATION = {English translation of Grundbegriffe der Wahrscheinlichkeit[s]rechnung, 1933},
  AUTHOR     = {A. N. Kolmogorov},
  EDITION    = {Second},
  EDITOR     = {Nathan Morisson},
  LOCATION   = {New York},
  PUBLISHER  = {Chelsea publishing company},
  TITLE      = {Foundations of the theory of probability},
  YEAR       = {1956},
}

@ARTICLE{Koopman-1940-ams,
  AUTHOR       = {B. O. Koopman},
  JOURNALTITLE = {Bulletin of the American Mathematical Society},
  LOCALFILE    = {article/Koopman-1940-ams.pdf},
  NUMBER       = {10},
  PAGES        = {763–774},
  TITLE        = {The bases of probability},
  URL          = {http://projecteuclid.org/euclid.bams/1183503229},
  VOLUME       = {46},
  YEAR         = {1940},
}

@ARTICLE{Koopman-1940-axioms,
  AUTHOR       = {B. O. Koopman},
  ISSN         = {0003-486X},
  JOURNALTITLE = {The Annals of Mathematics},
  LOCALFILE    = {article/Koopman-1940-axioms.pdf},
  NUMBER       = {2},
  PAGES        = {269–292},
  TITLE        = {The axioms and algebra of intuitive probability},
  URL          = {http://www.jstor.org/stable/1969003},
  VOLUME       = {41},
  YEAR         = {1940},
}

@ARTICLE{Koopman-1936,
  AUTHOR       = {B. O. Koopman},
  JOURNALTITLE = {Transactions of the American Mathematical Society},
  LOCALFILE    = {article/Koopman-1936.pdf},
  NUMBER       = {3},
  PAGES        = {399–409},
  TITLE        = {On distributions admitting a sufficient statistic},
  URL          = {http://www.ams.org/journals/tran/1936-039-03/S0002-9947-1936-1501854-3/S0002-99 47-1936-1501854-3.pdf},
  VOLUME       = {39},
  YEAR         = {1936},
}

@BOOK{Kotz-Balakrishnan-Johnson-2000,
  AUTHOR    = {Samuel Kotz and N. Balakrishnan and Norman L. Johnson},
  EDITION   = {second},
  PUBLISHER = {Wiley},
  SERIES    = {Wiley Series in Probability and Statistics},
  TITLE     = {Continuous Multivariate Distributions},
  VOLUME    = {1: Models},
  YEAR      = {2000},
}

@ARTICLE{Kozine-Krymsky-2009,
  ABSTRACT     = {This paper describes how one can compute interval-valued statistical measures given limited information about the underlying distribution. The particular focus is on a bounded derivative of a probability density function and its combination with other available statistical evidence for computing quantities of interest. To be able to utilise the evidence about the derivative it is suggested to adapt the ?conventional? problem statement to variational calculus and the way to do so is demonstrated. A number of examples are given throughout the paper.},
  AUTHOR       = {Igor O. Kozine and Victor Krymsky},
  DOI          = {10.1080/15598608.2009.10411909},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bounded derivative; Bounded probability distribution; Interval- valued measures; Natural extension; Variational calculus},
  LOCALFILE    = {article/Kozine-Krymsky-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {25–38},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Bounded Densities and Their Derivatives: Extension to other Domains},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Kozine-Utkin-2002,
  ABSTRACT     = {The requirement that precise state and transition probabilities be available is often not realistic because of cost, technical difficulties or the uniqueness of the situation under study. Expert judgements, generic data, heterogeneous and partial information on the occurrences of events may be sources of the probability assessments. All this source information cannot produce precise probabilities of interest without having to introduce drastic assumptions often of quite an arbitrary nature. in this paper the theory of interval-valued coherent previsions is employed to generalise discrete Markov chains to interval-valued probabilities. A general procedure of interval-valued probability elicitation is analysed as well. In addition, examples are provided.},
  ANNOTATION   = {ook offprint},
  AUTHOR       = {Igor O. Kozine and Lev V. Utkin},
  DOI          = {10.1023/A:1014745904458},
  JOURNALTITLE = {Reliable Computing},
  LOCALFILE    = {article/Kozine-Utkin-2002.pdf},
  PAGES        = {97–113},
  TITLE        = {Interval-Valued Finite Markov Chains},
  VOLUME       = {8},
  YEAR         = {2002},
}

@ARTICLE{Krantz-Kunreuther-2007,
  ABSTRACT     = {We propose a constructed-choice model for general decision making. The model departs from utility theory and prospect theory in its treatment of multiple goals and it suggests several different ways in which context can affect choice. It is particularly instructive to apply this model to protective decisions, which are often puzzling. Among other anomalies, people insure against non-catastrophic events, underinsure against catastrophic risks, and allow extraneous factors to influence insurance purchases and other protective decisions. Neither expected-utility theory nor prospect theory can explain these anomalies satisfactorily. To apply this model to the above anomalies, we consider many different insurance-related goals, organized in a taxonomy, and we consider the effects of context on goals, resources, plans and decision rules. The paper concludes by suggesting some prescriptions for improving individual decision making with respect to protective measures.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {David H. Krantz and Howard C. Kunreuther},
  JOURNALTITLE = {Judgment and Decision Making},
  KEYWORDS     = {catastrophic risk; decision making; goals; insurance; plans; prospect theory; protective behavior; utility theory},
  LOCALFILE    = {article/Krantz-Kunreuther-2007.pdf},
  MONTH        = {06},
  NUMBER       = {3},
  PAGES        = {137–168},
  TITLE        = {Goals and plans in decision making},
  URL          = {http://journal.sjdm.org/jdm7303b.pdf},
  VOLUME       = {2},
  YEAR         = {2007},
}

@ARTICLE{Krein-Milman-1940,
  AUTHOR       = {M. Krein and D. Milman},
  JOURNALTITLE = {Studia Mathematica},
  LOCALFILE    = {article/Krein-Milman-1940.pdf},
  PAGES        = {133–138},
  TITLE        = {On extreme points of regular convex sets},
  VOLUME       = {9},
  YEAR         = {1940},
}

@ARTICLE{Kreinovich-Xiang-Ferson-2006,
  ABSTRACT     = {In many real-life situations, we only have partial information about the actual probability distribution. For example, under Dempster-Shafer uncertainty, we only know the masses m1, ... ,mn assigned to different sets S1, ... ,Sn, but we do not know the distribution within each set Si. Because of this uncertainty, there are many possible probability distributions consistent with our knowledge; different distributions have, in general, different values of standard statistical characteristics such as mean and variance. It is therefore desirable, given a Dempster-Shafer knowledge base, to compute the ranges and of possible values of mean E and of variance V. In their recent paper, Langewisch and Choobineh show how to compute these ranges in polynomial time. In particular, they reduce the problem of computing to the problem of minimizing a convex quadratic function, a problem which can be solved in time O(n2 [middle dot] log(n)). We show that the corresponding quadratic optimization problem can be actually solved faster, in time O(n [middle dot] log(n)); thus, we can compute the bounds V and in time O(n [middle dot] log(n)).},
  AUTHOR       = {Vladik Kreinovich and Gang Xiang and Scott Ferson},
  DOI          = {10.1016/j.ijar.2005.12.001},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Kreinovich-Xiang-Ferson-2006.pdf},
  NUMBER       = {3},
  PAGES        = {212–227},
  TITLE        = {Computing mean and variance under Dempster-Shafer uncertainty: Towards faster algorithms},
  VOLUME       = {42},
  YEAR         = {2006},
}

@THESIS{Kriegler-2005,
  AUTHOR      = {Elmar Kriegler},
  INSTITUTION = {Universität Potsdam},
  TITLE       = {Imprecise Probability Analysis for Integrated Assessment of Climate Change},
  TYPE        = {phdthesis},
  YEAR        = {2005},
}

@ARTICLE{Krogh-etal-1994,
  ABSTRACT     = {Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the [alpha]-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both.},
  AUTHOR       = {Anders Krogh and Michael Brown and I. Saira Mian and Kimmen Sjölander and David Haussler},
  DOI          = {10.1006/jmbi.1994.1104},
  ISSN         = {0022-2836},
  JOURNALTITLE = {Journal of Molecular Biology},
  KEYWORDS     = {EF-hand; globin; hidden Markov models; kinase},
  LOCALFILE    = {article/Krogh-etal-1994.pdf},
  NUMBER       = {5},
  PAGES        = {1501–1531},
  TITLE        = {Hidden Markov models in computational biology: Applications to protein modeling},
  VOLUME       = {235},
  YEAR         = {1994},
}

@INCOLLECTION{2008-Kroupa-cores,
  AUTHOR    = {Tomáš Kroupa},
  BOOKTITLE = {Soft Methods for Handling Variability and Imprecision},
  DOI       = {10.1007/978-3-540-85027-4_37},
  EDITOR    = {Didier Dubois and María Asunción Lubiano and Henri Prade and María Ángeles Gil and Przemysław Grzegorzewski and Olgierd Hryniewicz},
  ISBN      = {978-3-540-85026-7},
  KEYWORDS  = {Coherent upper probability; Possibility measure; Core},
  LOCALFILE = {inproceedings/2008-Kroupa-cores.pdf},
  PAGES     = {306–312},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Advances in Soft Computing},
  TITLE     = {Geometry of Cores of Submodular Coherent Upper Probabilities and Possibility Measures},
  VOLUME    = {48},
  YEAR      = {2008},
}

@ARTICLE{Kschischang-Frey-Loeliger-2001,
  ABSTRACT     = {Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of ldquo;local rdquo; functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative ldquo;turbo rdquo; decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms},
  AUTHOR       = {F. R. Kschischang and B. J. Frey and H.-A. Loeliger},
  DOI          = {10.1109/18.910572},
  ISSN         = {0018-9448},
  JOURNALTITLE = {IEEE Transactions on Information Theory},
  KEYWORDS     = {Bayesian networks; FFT algorithms; HMM; Kalman filter; Kalman filters; Viterbi algorithm; Viterbi decoding; artificial intelligence; belief networks; belief propagation algorithm; bipartite graph; computational rule; digital communication; digital communications; factor graphs; factorization; fast Fourier transform; fast Fourier transforms; forward/backward algorithm; functional analysis; generic message-passing algorithm; global function; global functions; graph theory; hidden Markov models; iterative decoding; iterative turbo decoding algorithm; local functions; marginal functions; message passing; signal processing; sum-product algorithm; turbo codes},
  LOCALFILE    = {article/Kschischang-Frey-Loeliger-2001.pdf},
  NUMBER       = {2},
  PAGES        = {498–519},
  TITLE        = {Factor graphs and the sum-product algorithm},
  VOLUME       = {47},
  YEAR         = {2001},
}

@ARTICLE{Kubis-2002,
  ABSTRACT     = {The purpose of this paper is to investigate some separation properties of sets with axiomatically defined convexity structures. We state a general separation theorem for pairs of convexities, improving some known results. As an application, we discuss separation properties of lattices, real vector spaces and modules.},
  AUTHOR       = {Wiesław Kubiś},
  DOI          = {10.1007/PL00012529},
  ISSN         = {0047-2468},
  JOURNALTITLE = {Journal of Geometry},
  KEYWORDS     = {Mathematics; Statistics},
  LOCALFILE    = {article/Kubis-2002.pdf},
  LOCATION     = {Basel},
  NUMBER       = {1},
  PAGES        = {110–119},
  PUBLISHER    = {Birkhäuser},
  TITLE        = {Separation properties of convexity spaces},
  VOLUME       = {74},
  YEAR         = {2002},
}

@ARTICLE{Kunreuther-et-al-2002,
  ABSTRACT     = {This paper reviews the state of the art of research on individual decision-making in high-stakes, low-probability settings. A central theme is that resolving high-stakes decisions optimally poses a formidable challenge not only to naïve decision makers, but also to users of more sophisticated tools, such as decision analysis. Such decisions are difficult to make because precise information about probabilities is not available, and the dynamics of the decision are complex. When faced with such problems, naïve decision-makers fall prey to a wide range of potentially harmful biases, such as failing to recognize a high-stakes problem, ignoring the information about probabilities that does exist, and responding to complexity by accepting the status quo. A proposed agenda for future research focuses on how the process and outcomes of high-stakes decision making might be improved.},
  AUTHOR       = {Howard C. Kunreuther and Robert Meyer and Richard Zeckhauser and Paul Slovic and Barry Schwartz and Christian Schade and Mary Frances Luce and Steven Lippman and David H. Krantz and Barbara Kahn and Robin Hogarth},
  DOI          = {10.1023/A:1020287225409},
  JOURNALTITLE = {Marketing Letters},
  KEYWORDS     = {decision biases; decision heuristics; decision making under certainty; high-stakes decisions; risky decision making},
  LOCALFILE    = {article/Kunreuther-et-al-2002.pdf},
  NUMBER       = {3},
  PAGES        = {259–268},
  PUBLISHER    = {Springer},
  TITLE        = {High stakes decision making: Normative, descriptive and prescriptive considerations},
  VOLUME       = {13},
  YEAR         = {2002},
}

@BOOK{Kuznetsov-1991,
  AUTHOR   = {Kuznetsov},
  LOCATION = {Moscow},
  TITLE    = {No Title},
  YEAR     = {1991},
}

@MISC{Kvasnica-Grieder-Baotic-2006-MPT,
  AUTHOR = {M. Kvasnica and P. Grieder and M. Baotić},
  TITLE  = {Multi-Parametric Toolbox (MPT), version 2.6.3},
  URL    = {http://control.ee.ethz.ch/~mpt},
  YEAR   = {2006},
}

@INCOLLECTION{Kwisthout+Bodlaender+Van_der_Gaag-2011,
  AUTHOR    = {Johan H. P. Kwisthout and Hans L. Bodlaender and Linda C. van der Gaag},
  BOOKTITLE = {SOFSEM 2011: Theory and Practice of Computer Science},
  DOI       = {10.1007/978-3-642-18381-2_30},
  EDITOR    = {Ivana Černá and Tibor Gyimóthy and Juraj Hromkovič and Keith Jefferey and Rastislav Králović and Marko Vukolić and Stefan Wolf},
  ISBN      = {978-3-642-18380-5},
  LOCALFILE = {inproceedings/Kwisthout+Bodlaender+Van_der_Gaag-2011.pdf},
  PAGES     = {356–367},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {The complexity of finding kth most probable explanations in probabilistic networks},
  VOLUME    = {6543},
  YEAR      = {2011},
}

@INPROCEEDINGS{Kwisthout+Bodlaender+Van_der_Gaag-2010,
  AUTHOR    = {Johan H. P. Kwisthout and Hans L. Bodlaender and Linda C. van der Gaag},
  BOOKTITLE = {Proceedings of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence},
  ISBN      = {978-1-60750-605-8},
  LOCATION  = {Amsterdam, the Netherlands},
  PAGES     = {237–242},
  PUBLISHER = {IOS Press},
  TITLE     = {The necessity of bounded treewidth for efficient inference in Bayesian networks},
  YEAR      = {2010},
}

@ARTICLE{Kyburg-Pittarelli-1996,
  ABSTRACT     = {Problems for strict and convex Bayesianism are discussed. A set-based Bayesianism generalizing convex Bayesianism and intervalism is proposed. This approach abandons not only the strict Bayesian requirement of a unique real-valued probability function in any decision-making context but also the requirement of convexity for a set-based representation of uncertainty. Levi's E-admissibility decision criterion is retained and is shown to be applicable in the nonconvex case},
  AUTHOR       = {Kyburg, Jr., Henry E. and M. Pittarelli},
  DOI          = {10.1109/3468.487958},
  ISSN         = {1083-4427},
  JOURNALTITLE = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on},
  KEYWORDS     = {E-admissibility decision criterion; convex Bayesianism; decision-making; intervalism; set-based Bayesianism; set-based uncertainty representation; strict Bayesianism; Bayes methods; decision theory; probability; set theory; uncertainty handling},
  LOCALFILE    = {article/Kyburg-Pittarelli-1996.pdf},
  MONTH        = {5},
  NUMBER       = {3},
  PAGES        = {324–339},
  TITLE        = {Set-based Bayesianism},
  VOLUME       = {26},
  YEAR         = {1996},
}

@ARTICLE{Lambrakis1969,
  AUTHOR       = {D. P. Lambrakis},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Ericson-1969.pdf},
  NUMBER       = {2},
  PAGES        = {234–245},
  TITLE        = {Experiments with mixtures: an alternative to the simplex-lattice design},
  URL          = {http://www.jstor.org/stable/2984207},
  VOLUME       = {31},
  YEAR         = {1969},
}

@ARTICLE{Lange-1995,
  AUTHOR       = {Kenneth Lange},
  DOI          = {10.1007/BF01441156},
  JOURNALTITLE = {Genetica},
  LOCALFILE    = {article/Lange-1995.pdf},
  NUMBER       = {1},
  PAGES        = {107–117},
  PUBLISHER    = {Springer},
  TITLE        = {Applications of the Dirichlet distribution to forensic match probabilities},
  VOLUME       = {96},
  YEAR         = {1995},
}

@BOOK{Laplace-1825-essai,
  AUTHOR    = {Pierre-Simon Laplace},
  EDITION   = {5},
  LOCATION  = {Paris},
  PUBLISHER = {Bachelier},
  TITLE     = {Essai philosophique sur les probabilités},
  URL       = {http://books.google.com/books?id=Ovo3AAAAMAAJ},
  YEAR      = {1825},
}

@BOOK{Laplace-1920-theorie,
  AUTHOR    = {Pierre-Simon Laplace},
  EDITION   = {3},
  PUBLISHER = {Gauthier-Villars},
  SERIES    = {Oeuvres complètes de Laplace},
  TITLE     = {Théorie analytique des probabilités},
  VOLUME    = {7},
  YEAR      = {1820},
}

@ARTICLE{Laskey-1995,
  ABSTRACT     = {When eliciting a probability model from experts, knowledge engineers may compare the results of the model with expert judgment on test scenarios, then adjust model parameters to bring the behavior of the model more in line with the experts intuition. This paper presents a methodology for analytic computation of sensitivity values in Bayesian network models. Sensitivity values are partial derivatives of output probabilities with respect to parameters being varied in the sensitivity analysis. They measure the impact of small changes in a network parameter on a target probability value or distribution. Sensitivity values can be used to focus knowledge elicitation effort on those parameters having the most impact on outputs of concern. Analytic sensitivity values are computed for an example and compared to sensitivity analysis by direct variation of parameters},
  AUTHOR       = {K. B. Laskey},
  DOI          = {10.1109/21.384252},
  ISSN         = {0018-9472},
  JOURNALTITLE = {IEEE Transactions on Systems, Man and Cybernetics},
  KEYWORDS     = {Bayesian networks; knowledge elicitation; knowledge engineering; probability assessments; sensitivity analysis; symbolic reasoning; target probability value; uncertainty representation; Bayes methods; inference mechanisms; knowledge acquisition; probability; sensitivity analysis},
  MONTH        = {06},
  NUMBER       = {6},
  PAGES        = {901–909},
  TITLE        = {Sensitivity analysis for probability assessments in Bayesian networks},
  VOLUME       = {25},
  YEAR         = {1995},
}

@MISC{Lauritzen-2004a,
  ANNOTATION = {Transparanten},
  AUTHOR     = {Steffen L. Lauritzen},
  TITLE      = {Sufficiency and Unbiased Estimation},
  YEAR       = {2004},
}

@MISC{Lauritzen-2004b,
  ANNOTATION = {Transparanten},
  AUTHOR     = {Steffen L. Lauritzen},
  TITLE      = {Exponential Families of Distributions},
  YEAR       = {2004},
}

@INPROCEEDINGS{Lee-Varaiya-2000-sysensig,
  ANNOTATION = {ook op papier},
  AUTHOR     = {Edward A. Lee and Pravin Varaiya},
  BOOKTITLE  = {Proceedings of the First Signal Processing Workshop},
  TITLE      = {Introducing signals and syetems – The Berkeley approach},
  YEAR       = {2000},
}

@INPROCEEDINGS{Lemmer-Kyburg-1991,
  AUTHOR    = {John F. Lemmer and Kyburg, Jr., Henry E.},
  BOOKTITLE = {AAAI-91 Proceedings},
  PAGES     = {488–493},
  TITLE     = {Conditions for the existence of belief functions corresponding to intervals of beliefs},
  YEAR      = {1991},
}

@BOOK{Letac-1992,
  ANNOTATION = {Hoofdstukken 1, 2 en 4 kopies},
  AUTHOR     = {Gérard Letac},
  NUMBER     = {50},
  PUBLISHER  = {Conselho Nacional de Desenvolvimento Cientifico e Tecnológico, Instituto de Matemática Pura e Aplicada (IMPA)},
  SERIES     = {Monografias de Matemática},
  TITLE      = {Lectures on natural exponential families and their variance functions},
  YEAR       = {1992},
}

@BOOK{Levi-1980,
  AUTHOR    = {Isaac Levi},
  LOCATION  = {London},
  PUBLISHER = {MIT Press},
  TITLE     = {The Enterprise of Knowledge},
  YEAR      = {1980},
}

@ARTICLE{Levi-1977,
  AUTHOR       = {Isaac Levi},
  JOURNALTITLE = {The Journal of Philosophy},
  LOCALFILE    = {article/Levi-1977.pdf},
  NUMBER       = {1},
  PAGES        = {5–29},
  TITLE        = {Direct inference},
  URL          = {http://www.jstor.org/stable/2025732},
  VOLUME       = {74},
  YEAR         = {1977},
}

@ARTICLE{Levi-1974,
  AUTHOR       = {Isaac Levi},
  JOURNALTITLE = {The Journal of Philosophy},
  LOCALFILE    = {article/Levi-1974.pdf},
  NUMBER       = {13},
  PAGES        = {391–418},
  TITLE        = {On indeterminate probabilities},
  URL          = {http://www.jstor.org/stable/2025161},
  VOLUME       = {71},
  YEAR         = {1974},
}

@BOOK{Liu-2007-uncertain-programming,
  AUTHOR  = {Baoding Liu},
  EDITION = {2},
  TITLE   = {Theory and Practice of Uncertain Programming},
  YEAR    = {2007},
}

@BOOK{Liu-2007-uncertainty-theory,
  AUTHOR  = {Baoding Liu},
  EDITION = {3},
  TITLE   = {Uncertainty Theory},
  YEAR    = {2007},
}

@ARTICLE{Liu-Mueller-2003,
  AUTHOR       = {Xueli Liu and Hans-Georg Müller},
  DOI          = {10.1093/bioinformatics},
  JOURNALTITLE = {Bioinformatics},
  LOCALFILE    = {article/Liu-Mueller-2003.pdf},
  NUMBER       = {15},
  PAGES        = {1937–1944},
  TITLE        = {Modes and clustering for time-warped gene expression profile data},
  VOLUME       = {19},
  YEAR         = {2003},
}

@ARTICLE{Lo-1986-finite-sampling,
  AUTHOR       = {Albert Y. Lo},
  DOI          = {10.1214/aos/1176350061},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Lo-1986-finite-sampling.pdf},
  NUMBER       = {3},
  PAGES        = {1226–1233},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Bayesian statistical inference for sampling a finite population},
  VOLUME       = {14},
  YEAR         = {1986},
}

@ARTICLE{Lodwick-Bachman-2005,
  ABSTRACT     = {Fuzzy and possibilistic optimization methods are demonstrated to be effective tools in solving large-scale problems. In particular, an optimization problem in radiation therapy with various orders of complexity from 1000 to 62,250 constraints for fuzzy and possibilistic linear and nonlinear programming implementations possessing (1) fuzzy or soft inequalities, (2) fuzzy right-hand side values, and (3) possibilistic right-hand side is used to demonstrate that fuzzy and possibilistic optimization methods are tractable and useful. We focus on the uncertainty in the right side of constraints which arises, in the context of the radiation therapy problem, from the fact that minimal and maximal radiation tolerances are ranges of values, with preferences within the range whose values are based on research results, empirical findings, and expert knowledge, rather than fixed real numbers. The results indicate that fuzzy/possibilistic optimization is a natural and effective way to model various types of optimization under uncertainty problems and that large fuzzy and possibilistic optimization problems can be solved efficiently.},
  AUTHOR       = {Weldon A. Lodwick and Katherine Bachman},
  DOI          = {10.1007/s10700-005-3663-4},
  ISSN         = {1568-4539},
  JOURNALTITLE = {Fuzzy Optimization and Decision Making},
  LOCALFILE    = {article/Lodwick-Bachman-2005.pdf},
  PAGES        = {257–278},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Solving Large-Scale Fuzzy and Possibilistic Optimization Problems},
  VOLUME       = {4},
  YEAR         = {2005},
}

@MISC{Lohne-2012,
  AUTHOR = {Andreas Löhne},
  MONTH  = {11},
  TITLE  = {bensolve, version 1.2},
  URL    = {http://ito.mathematik.uni-halle.de/~loehne/index_en_dl.php},
  YEAR   = {2012},
}

@ARTICLE{Luce-VonWinterfeld-1994,
  ABSTRACT     = {Descriptive and normative modeling of decision making under risk and uncertainty have grown apart over the past decade. Psychological models attempt to accommodate the numerous violations of rationality axioms, including independence and transitivity. Meanwhile, normatively oriented decision analysts continue to insist on the applied usefulness of the subjective expected utility (SEU) model. As this gap has widened, two facts have remained largely unobserved. First, most people in real situations attempt to behave in accord with the most basic rationality principles, even though they are likely to fail in more complex situations. Second, the SEU model is likely to provide consistent and rational answers to decision problems within a given problem structure, but may not be invariant across structures. Thus, people may be more rational than the psychological literature gives them credit for, and applications of the SEU model may be susceptible to some violations of invariance principles. This paper attempts to search out the common ground between the normative, descriptive, and prescriptive modeling by exploring three types of axioms concerning structural rationality, preference rationality, and quasi-rationality. Normatively the first two are mandatory and the last, suspect. Descriptively, all have been questioned, but often the inferences involved have confounded preference and structural rationality. We propose a prescriptive view that entails full compliance with preference rationality, modifications of structural rationality, and acceptance of quasi-rationality to the extent of granting a primary role to the status quo and the decomposition of decision problems into gains and losses.},
  AUTHOR       = {R. Duncan Luce and Detlof von Winterfeldt},
  DOI          = {10.1287/mnsc.40.2.263},
  JOURNALTITLE = {Management Science},
  KEYWORDS     = {Decision Analysis; Prescriptive Utility; Rank-Dependent Utility; Sign-Dependent Utility},
  LOCALFILE    = {article/Luce-VonWinterfeld-1994.pdf},
  NUMBER       = {2},
  PAGES        = {263–279},
  TITLE        = {What common ground exists for descriptive, prescriptive, and normative utility theories?},
  URL          = {http://www.jstor.org/stable/2632765},
  VOLUME       = {40},
  YEAR         = {1994},
}

@INPROCEEDINGS{Maass-2003-ISIPTA,
  AUTHOR    = {Sebastian Maaß},
  BOOKTITLE = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
  EDITOR    = {Jean-Marc Bernard and Teddy Seidenfeld and Marco Zaffalon},
  LOCATION  = {Waterloo, Ontario, Canada},
  PAGES     = {372–382},
  PUBLISHER = {Carleton Scientific},
  SERIES    = {Proceedings in Informatics},
  TITLE     = {Continuous Linear Representations of Coherent Lower Previsions},
  VENUE     = {Lugano, Switzerland},
  VOLUME    = {18},
  YEAR      = {2003},
}

@THESIS{Maass-2003-PhD,
  AUTHOR      = {Sebastian Maaß},
  INSTITUTION = {Universität Bremen},
  LOCALFILE   = {phdthesis/Maass-2003.pdf},
  TITLE       = {Exact functionals, functionals preserving linear inequalities, Lévy's metric},
  TYPE        = {phdthesis},
  YEAR        = {2003},
}

@MISC{Manski-2003,
  AUTHOR = {Charles F. Manski},
  TITLE  = {Partial identification of probability distributions},
  YEAR   = {2003},
}

@ARTICLE{Mantel-1976-tails,
  AUTHOR       = {Nathan Mantel},
  JOURNALTITLE = {The American Statistician},
  LOCALFILE    = {article/Mantel-1976-tails.pdf},
  NUMBER       = {1},
  PAGES        = {14–17},
  TITLE        = {Tails of Distributions},
  URL          = {http://www.jstor.org/stable/2682880},
  VOLUME       = {30},
  YEAR         = {1976},
}

@ARTICLE{Mardia-ElAtoum-1976,
  ABSTRACT     = {The main aim of this note is to give a theoretical discussion of Bayesian inference for the von Mises-Fisher distribution. The choice of particular priors is considered and the admissibility of certain Bayesian estimators studied. For the multisample case estimators are given. Some problems of testing hypotheses are summarized in the form of posterior odds against the null hypothesis.},
  AUTHOR       = {K. V. Mardia and S. A. M. El-Atoum},
  DOI          = {10.1093/biomet},
  JOURNALTITLE = {Biometrika},
  KEYWORDS     = {Bayesian directional data analysis; Loss function},
  LOCALFILE    = {article/Mardia-ElAtoum-1976.pdf},
  NUMBER       = {1},
  PAGES        = {203–206},
  PUBLISHER    = {Biometrika Trust},
  TITLE        = {Bayesian inference for the von Mises-Fisher distribution},
  VOLUME       = {63},
  YEAR         = {1976},
}

@BOOK{Martin-1966,
  AUTHOR    = {J. J. Martin},
  EDITOR    = {David B. Hertz},
  NUMBER    = {13},
  PUBLISHER = {Wiley},
  SERIES    = {Publications in Operations Research},
  TITLE     = {Bayesian Decision problems and Markov Chains},
  YEAR      = {1966},
}

@ARTICLE{Matheiss-Rubin-1980-vertexenum,
  ABSTRACT     = {This paper surveys the literature on methods for finding all vertices of convex polytopes, contrasting the main features of each method and providing computational results for representative methods.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {T. H. Matheiss and David S. Rubin},
  ISSN         = {0364-765X},
  JOURNALTITLE = {Mathematics of Operations Research},
  LOCALFILE    = {article/Matheiss-Rubin-1980.pdf},
  NUMBER       = {2},
  PAGES        = {167–185},
  TITLE        = {A survey and comparison of methods for finding all vertices of convex polyhedral sets},
  URL          = {http://www.jstor.org/stable/3689148},
  VOLUME       = {5},
  YEAR         = {1980},
}

@ARTICLE{Mazaheri-Nasri-2007,
  AUTHOR       = {H. Mazaheri and M. Nasri},
  JOURNALTITLE = {International Mathematical Forum},
  LOCALFILE    = {article/Mazaheri-Nasri-2007.pdf},
  NUMBER       = {16},
  PAGES        = {747–751},
  TITLE        = {Complemented Subspaces in the Normed Spaces},
  URL          = {http://www.m-hikari.com/imf-password2007/13-16-2007/mazaheriIMF13-16-2007-3.pdf},
  VOLUME       = {2},
  YEAR         = {2007},
}

@ARTICLE{McKinney-1962,
  AUTHOR       = {Richard L. McKinney},
  DOI          = {10.1090/S0002-9947-1962-0147879-X},
  JOURNALTITLE = {Transactions of the American Mathematical Society},
  LOCALFILE    = {article/McKinney-1962.pdf},
  PAGES        = {131–148},
  TITLE        = {Positive bases for linear spaces},
  URL          = {http://www.ams.org/journals/tran/1962-103-01/S0002-9947-1962-0147879-X/S0002-99 47-1962-0147879-X.pdf},
  VOLUME       = {103},
  YEAR         = {1962},
}

@BOOK{Menezes-VanOorschot-Vanstone-1996,
  AUTHOR    = {Alfred J. Menezes and Paul C. {Van Oorschot} and Scott A. Vanstone},
  PUBLISHER = {CRC Press},
  TITLE     = {Handbook of Applied Cryptography},
  YEAR      = {1996},
}

@THESIS{Mevel-1997,
  AUTHOR      = {Laurent Mevel},
  INSTITUTION = {Université de Rennes},
  TITLE       = {Statistique asymptotique pour les modèles de Markov cachés},
  TYPE        = {phdthesis},
  YEAR        = {1997},
}

@INPROCEEDINGS{Mevel-Finesso-2000,
  ANNOTATION = {ook op papier},
  AUTHOR     = {Laurent Mevel and Lorenzo Finesso},
  BOOKTITLE  = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
  TITLE      = {Bayesian estimation of Hidden Markov Models},
  YEAR       = {2000},
}

@ARTICLE{Miller-1980-gamma,
  ABSTRACT     = {This paper presents a Bayesian analysis of shape, scale, and mean of the two-parameter gamma distribution. Attention is given to conjugate and "non-informative" priors, to simplifications of the numerical analysis of posterior distributions, and to comparison of Bayesian and classical inferences.},
  AUTHOR       = {Robert B. Miller},
  ISSN         = {0040-1706},
  JOURNALTITLE = {Technometrics},
  KEYWORDS     = {Bayesian analysis; Gamma distribution},
  LOCALFILE    = {article/Miller-1980-gamma.pdf},
  NUMBER       = {1},
  PAGES        = {65–69},
  PUBLISHER    = {American Statistical Association and American Society for Quality},
  TITLE        = {Bayesian analysis of the two-parameter Gamma distribution},
  URL          = {http://www.jstor.org/stable/1268384},
  VOLUME       = {22},
  YEAR         = {1980},
}

@ARTICLE{Milne-1993,
  ABSTRACT     = {Taking as starting point two familiar interpretations of probability, we develop these in a perhaps unfamiliar way to arrive ultimately at an improbable claim concerning the proper axiomatization of probability theory: the domain of definition of a point-valued probability distribution is an orthomodular partially ordered set. Similar claims have been made in the light of quantum mechanics but here the motivation is intrinsically probabilistic. This being so the main task is to investigate what light, if any, this sheds on quantum mechanics. In particular it is important to know under what conditions these point-valued distributions can be thought of as derived from distribution-pairs of upper and lower probabilities on boolean algebras. Generalising known results this investigation unsurprisingly proves unrewarding. In the light of this failure the next topic investigated is how these generalized probability distributions are to be interpreted.},
  AUTHOR       = {Peter Milne},
  DOI          = {10.1007/BF01049259},
  JOURNALTITLE = {Journal of Philosophical Logic},
  PAGES        = {129–168},
  TITLE        = {The foundations of probability and quantum mechanics},
  VOLUME       = {22},
  YEAR         = {1993},
}

@REPORT{Minka-2001,
  AUTHOR = {Thomas P. Minka},
  TITLE  = {Bayesian linear regression},
  TYPE   = {techreport},
  YEAR   = {2001},
}

@ARTICLE{Miranda-2008-survey,
  ABSTRACT     = {This paper presents a summary of Peter Walley's theory of coherent lower previsions. We introduce three representations of coherent assessments: coherent lower and upper previsions, closed and convex sets of linear previsions, and sets of desirable gambles. We show also how the notion of coherence can be used to update our beliefs with new information, and a number of possibilities to model the notion of independence with coherent lower previsions. Next, we comment on the connection with other approaches in the literature: de Finetti's and Williams' earlier work, Kuznetsov's and Weischelberger's work on interval-valued probabilities, Dempster-Shafer theory of evidence and Shafer and Vovk's game-theoretic approach. Finally, we present a brief survey of some applications and summarize the main strengths and challenges of the theory.},
  AUTHOR       = {Enrique Miranda},
  DOI          = {10.1016/j.ijar.2007.12.001},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Avoiding sure loss; Coherence; Conditional lower previsions; Desirability; Imprecision; Independence; Subjective probability},
  LOCALFILE    = {article/Miranda-2008-survey.pdf},
  NUMBER       = {2},
  PAGES        = {628–658},
  TITLE        = {A survey of the theory of coherent lower previsions},
  VOLUME       = {48},
  YEAR         = {2008},
}

@REPORT{Miranda-2008-updating,
  ANNOTATION  = {ook op papier},
  AUTHOR      = {Enrique Miranda},
  INSTITUTION = {Department of Statistics and Operations Research, Rey Juan Carlos University},
  TITLE       = {Updating coherent previsions on finite spaces},
  TYPE        = {techreport},
  YEAR        = {2008},
}

@ARTICLE{Miranda-DeCooman-2007-margext,
  ABSTRACT     = {We generalise Walley's Marginal Extension Theorem to the case of any finite number of conditional lower previsions. Unlike the procedure of natural extension, our marginal extension always provides the smallest (most conservative) coherent extensions. We show that they can also be calculated as lower envelopes of marginal extensions of conditional linear (precise) previsions. Finally, we use our version of the theorem to study the so-called forward irrelevant product and forward irrelevant natural extension of a number of marginal lower previsions.},
  AUTHOR       = {Enrique Miranda and Gert de Cooman},
  DOI          = {10.1016/j.ijar.2006.12.009},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Coherence; Epistemic irrelevance; Forward irrelevance; Forward irrelevant natural extension; Forward irrelevant product; Imprecise probabilities; Lower previsions; Marginal extension; Natural extension},
  LOCALFILE    = {article/Miranda-DeCooman-2007-margext.pdf},
  NUMBER       = {1},
  PAGES        = {188–225},
  TITLE        = {Marginal extension in the theory of coherent lower previsions},
  VOLUME       = {46},
  YEAR         = {2007},
}

@ARTICLE{Miranda-DeCooman-2003,
  ABSTRACT     = {Numerical possibility measures can be interpreted as systems of upper betting rates for events. As such, they have a special part in the unifying behavioural theory of imprecise probabilities, proposed by Walley. On this interpretation, they should arguably satisfy certain rationality, or consistency, requirements, such as avoiding sure loss and coherence. Using a version of Walley's notion of epistemic independence suitable for possibility measures, we study in detail what these rationality requirements tell us about the construction of independent product possibility measures from given marginals, and we obtain necessary and sufficient conditions for a product to satisfy these criteria. In particular, we show that the well-known minimum and product rules for forming independent joint distributions from marginal ones, are only coherent when at least one of these distributions assume just the values zero and one.},
  ANNOTATION   = {reprint},
  AUTHOR       = {Enrique Miranda and Gert de Cooman},
  DOI          = {10.1016/S0888-613X(02)00087-7},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Coherence; Conditioning; Epistemic independence; Independent product; Possibility theory; Upper probability},
  LOCALFILE    = {article/Miranda-DeCooman-2003.pdf},
  PAGES        = {23–42},
  TITLE        = {Epistemic independence in numerical possibility theory},
  VOLUME       = {32},
  YEAR         = {2003},
}

@ARTICLE{Miranda-DeCooman-Couso-2005-multivalued,
  ABSTRACT     = {We discuss how lower previsions induced by multi-valued mappings fit into the framework of the behavioural theory of imprecise probabilities, and show how the notions of coherence and natural extension from that theory can be used to prove and generalise existing results in an elegant and straightforward manner. This provides a clear example for their explanatory and unifying power.},
  ANNOTATION   = {uitgebreide versie, ook op papier},
  AUTHOR       = {Enrique Miranda and Gert de Cooman and Inés Couso},
  DOI          = {10.1016/j.jspi.2004.03.005},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Coherence; Conditioning; Evidence theory; Imprecise probabilities; Lower inverse; Lower prevision; Multi-valued mapping; Natural extension; Random set},
  LOCALFILE    = {article/Miranda-DeCooman-Couso-2005-multivalued.pdf},
  NUMBER       = {1},
  PAGES        = {173–197},
  TITLE        = {Lower previsions induced by multi-valued mappings},
  VOLUME       = {133},
  YEAR         = {2005},
}

@INPROCEEDINGS{Miranda-Troffaes-destercke-2008-SMPS,
  ANNOTATION = {extended version on paper; available from authors},
  AUTHOR     = {Enrique Miranda and Matthias C. M. Troffaes and Sébastien Destercke},
  BOOKTITLE  = {SMPS},
  DOI        = {10.1007/978-3-540-85027-4_29},
  EDITOR     = {Didier Dubois and María Asunción Lubiano and Henri Prade and María Ángeles Gil and Przemysław Grzegorzewski and Olgierd Hryniewicz},
  ISBN       = {978-3-540-85026-7},
  PAGES      = {235–242},
  PUBLISHER  = {Springer},
  SERIES     = {Advances in Soft Computing},
  TITLE      = {Generalised p-Boxes on Totally Ordered Spaces},
  VOLUME     = {48},
  YEAR       = {2008},
}

@ARTICLE{Miranda-Zaffalon-2011,
  ABSTRACT     = {We detail the relationship between sets of desirable gambles and conditional lower previsions. The former is one the most general models of uncertainty. The latter corresponds to Walley’s celebrated theory of imprecise probability. We consider two avenues: when a collection of conditional lower previsions is derived from a set of desirable gambles, and its converse. In either case, we relate the properties of the derived model with those of the originating one. Our results constitute basic tools to move from one formalism to the other, and thus to take advantage of work done in the two fronts.},
  AUTHOR       = {Enrique Miranda and Marco Zaffalon},
  DOI          = {10.1007/s10472-011-9231-4},
  ISSN         = {1012-2443},
  ISSUE        = {3},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  KEYWORD      = {Computer Science},
  KEYWORDS     = {coherence; equal expressivity; lower and upper previsions; mathematics subject classifications; natural extension; sets of desirable gambles},
  LOCALFILE    = {article/Miranda-Zaffalon-2011.pdf},
  PAGES        = {251–309},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Notes on desirability and conditional lower previsions},
  VOLUME       = {60},
  YEAR         = {2011},
}

@ARTICLE{2009-Miranda+Zaffalon-cohgraphs,
  ABSTRACT     = {We study the consistency of a number of probability distributions, which are allowed to be imprecise. To make the treatment as general as possible, we represent those probabilistic assessments as a collection of conditional lower previsions. The problem then becomes proving Walley's (strong) coherence of the assessments. In order to maintain generality in the analysis, we assume to be given nearly no information about the numbers that make up the lower previsions in the collection. Under this condition, we investigate the extent to which the above global task can be decomposed into simpler and more local ones. This is done by introducing a graphical representation of the conditional lower previsions that we call the coherence graph: we show that the coherence graph allows one to isolate some subsets of the collection whose coherence is sufficient for the coherence of all the assessments; and we provide a polynomial-time algorithm that finds the subsets efficiently. We show some of the implications of our results by focusing on three models and problems: Bayesian and credal networks, of which we prove coherence; the compatibility problem, for which we provide an optimal graphical decomposition; probabilistic satisfiability, of which we show that some intractable instances can instead be solved efficiently by exploiting coherence graphs.},
  AUTHOR       = {Enrique Miranda and Marco Zaffalon},
  DOI          = {10.1016/j.artint.2008.09.001},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Walley's strong and weak coherence; Coherent lower previsions; Graphical models; Probabilistic logic; Satisfiability},
  LOCALFILE    = {article/2009-Miranda+Zaffalon-cohgraphs.pdf},
  NUMBER       = {1},
  PAGES        = {104–144},
  TITLE        = {Coherence graphs},
  VOLUME       = {173},
  YEAR         = {2009},
}

@ARTICLE{Miranda-Combarro-Gil-2006-extreme,
  ABSTRACT     = {Non-additive measures are a valuable tool to model many different problems arising in real situations. However, two important difficulties appear in their practical use: the complexity of the measures and their identification from sample data. For the first problem, additional conditions are imposed, leading to different subfamilies of non-additive measures. Related to the second point, in this paper we study the set of vertices of some families of non-additive measures, namely k-additive measures and p-symmetric measures. These extreme points are necessary in order to properly apply a new method of identification of non-additive measures based on genetic algorithms, whose cross-over operator is the convex combination. We solve the problem through techniques of Linear Programming.},
  AUTHOR       = {Pedro Miranda and Elías F. Combarro and Pedro Gil},
  DOI          = {10.1016/j.ejor.2005.03.005},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {Decision analysis; Genetic algorithms; Linear programming; Multiple criteria analysis; Non-additive measures; Vertices; k-additivity; p-symmetry},
  LOCALFILE    = {article/Miranda-Combarro-Gil-2006-extreme.pdf},
  NUMBER       = {3},
  PAGES        = {1865–1884},
  TITLE        = {Extreme points of some families of non-additive measures},
  VOLUME       = {174},
  YEAR         = {2006},
}

@ARTICLE{Miranda-Grabisch-Gil-2002,
  AUTHOR       = {Pedro Miranda and Michel Grabisch and Pedro Gil},
  DOI          = {10.1142/S0218488502001867},
  EPRINT       = {0804.2642},
  EPRINTTYPE   = {arXiv},
  JOURNALTITLE = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  LOCALFILE    = {article/Miranda-Grabisch-Gil-2002.pdf},
  NUMBER       = {Supplementary Issue 1},
  PAGES        = {105–123},
  TITLE        = {p-Symmetric fuzzy measures},
  VOLUME       = {10},
  YEAR         = {2002},
}

@BOOK{VonMises-1996-humanaction,
  AUTHOR    = {Ludwig von Mises},
  EDITION   = {4},
  ISBN      = {0-930073-18-5},
  LOCALFILE = {book/VonMises-1996-hmanaction.pdf},
  LOCATION  = {San Francisco, California},
  PUBLISHER = {Fox \& Wilkes},
  TITLE     = {Human Action: A Treatise on Economics},
  YEAR      = {1996},
}

@ARTICLE{Montgomery-Coolen-Hart-2009,
  ABSTRACT     = {Refined risk assessments should increase realism compared with the first tier deterministic risk assessment. This may involve using probabilistic methods which account separately for uncertainty and variability. Analysts use cumulative distribution functions to represent variability, and bounds around these to illustrate uncertainty. In probability bounds analysis, parametric probability boxes (p-boxes) are usually formed using intervals for each parameter. In this paper a Bayesian framework is adopted, which takes account of dependencies between parameters. Bayesian p-boxes use imprecision represented by bounds to summarise the uncertainty surrounding the risk distribution parameters.},
  AUTHOR       = {Victoria J. Montgomery and Frank P. A. Coolen and Andy D. M. Hart},
  DOI          = {10.1080/15598608.2009.10411912},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bayesian methods; Cumulative distribution functions; Highest posterior density regions; Risk assessment; Probability boxes},
  LOCALFILE    = {article/Montgomery-Coolen-Hart-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {69–83},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Bayesian Probability Boxes in Risk Assessment},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Moral-2005-desir,
  ABSTRACT     = {This paper studies graphoid properties for epistemic irrelevance in sets of desirable gambles. For that aim, the basic operations of conditioning and marginalization are expressed in terms of variables. Then, it is shown that epistemic irrelevance is an asymmetric graphoid. The intersection property is verified in probability theory when the global probability distribution is positive in all the values. Here it is always verified due to the handling of zero probabilities in sets of gambles. An asymmetrical D-separation principle is also presented, by which this type of independence relationships can be represented in directed acyclic graphs.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Serafín Moral},
  DOI          = {10.1007/s10472-005-9011-0},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  KEYWORDS     = {Desirable gambles; conditioning; epistemic independence; epistemic irrelevance; imprecise probabilities},
  LOCALFILE    = {article/Moral-2005-desir.pdf},
  PAGES        = {197–214},
  TITLE        = {Epistemic irrelevance on sets of desirable gambles},
  VOLUME       = {45},
  YEAR         = {2005},
}

@INPROCEEDINGS{Moral-Wilson-1995,
  AUTHOR    = {Serafín Moral and Nic Wilson},
  BOOKTITLE = {Mathematical Models for Handling Partial Knowledge in Artificial Intelligence},
  EDITOR    = {Giulianella Coletti and Didier Dubois and Romano Scozzafava},
  LOCALFILE = {inproceedings/Moral-Wilson-1995.pdf},
  LOCATION  = {New York},
  PAGES     = {113–128},
  PUBLISHER = {Plenum},
  TITLE     = {Revision Rules for Convex Sets of Probabilities},
  YEAR      = {1995},
}

@INCOLLECTION{1964-Morishima-PerronFrobenius,
  AUTHOR    = {Michio Morishima},
  BOOKTITLE = {Equilibrium, stability, and growth},
  CHAPTER   = {Appendix},
  EDITION   = {1967},
  LOCALFILE = {inbook/1964-Morishima-PerronFrobenius.pdf},
  NOTE      = {ook op papier},
  PAGES     = {195–215},
  PUBLISHER = {Oxford University Press},
  TITLE     = {Generalizations of the Perron–Frobenius Theorem for nonnegative square matrices},
  YEAR      = {1964},
}

@ARTICLE{Morris-1982,
  AUTHOR       = {Carl N. Morris},
  DOI          = {10.1214/aos/1176346158},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Morris-1982.pdf},
  NUMBER       = {1},
  PAGES        = {65–80},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Natural exponential families with quadratic variance functions},
  VOLUME       = {10},
  YEAR         = {1982},
}

@ARTICLE{Mosimann-1962,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {James E. Mosimann},
  JOURNALTITLE = {Biometrika},
  LOCALFILE    = {article/Mosimann-1962.pdf},
  NUMBER       = {1/2},
  PAGES        = {65–82},
  TITLE        = {On the compound multinomial distribution, the multivariate $\beta$-distribution, and correlations among proportions},
  URL          = {http://www.jstor.org/stable/2333468},
  VOLUME       = {49},
  YEAR         = {1962},
}

@ARTICLE{Moslehian-2006,
  ARXIVID      = {math/0501048},
  AUTHOR       = {Mohammad Sal Moslehian},
  EPRINT       = {0501048},
  EPRINTCLASS  = {math},
  EPRINTTYPE   = {arXiv},
  JOURNALTITLE = {Trends in Mathematics},
  KEYWORDS     = {Complemented subspace; L1-predual space; Schauder basis; basis; complementary minimal subspace; prime space; quasi-complemented subspace; sequence spaces; weakly complemented subspace},
  LOCALFILE    = {article/Moslehian-2006.pdf},
  NUMBER       = {1},
  PAGES        = {91–98},
  TITLE        = {A survey of the complemented subspace problem},
  VOLUME       = {9},
  YEAR         = {2006},
}

@ARTICLE{Mraz-1998,
  ABSTRACT     = {The paper deals with computing the exact upper and lower bounds of optimal values forlinear programming problems whose coefficients vary in given intervals. The theoreticalbackground for calculating these bounds is described and corresponding algorithms aregiven. A comparison with other approaches, some applications and a software package arementioned.},
  AUTHOR       = {František Mráz},
  DOI          = {10.1023/A:1018985914065},
  ISSN         = {0254-5330},
  JOURNALTITLE = {Annals of Operations Research},
  KEYWORDS     = {inexact data; interval coefficients; linear programming problem},
  LOCALFILE    = {article/Mraz-1998.pdf},
  PAGES        = {51–62},
  PUBLISHER    = {Springer},
  TITLE        = {Calculating the exact bounds of optimal values in LP with interval coefficients},
  VOLUME       = {81},
  YEAR         = {1998},
}

@INCOLLECTION{Mukerji-Tallon-2003,
  AUTHOR    = {Sujoy Mukerji and Jean-Marc Tallon},
  BOOKTITLE = {Uncertainty in Economic Theory: A collection of essays in honor of David Schmeidler's 65th birthday},
  EDITOR    = {I. Gilboa},
  PUBLISHER = {Routledge},
  TITLE     = {An overview of economic applications of David Schmeidler's models of decision making under uncertainty},
  YEAR      = {2004},
}

@ARTICLE{Munch-etal-2006,
  ABSTRACT     = {BACKGROUND: Genomic tiling micro arrays have great potential for identifying previously undiscovered coding as well as non-coding transcription. To-date, however, analyses of these data have been performed in an ad hoc fashion. RESULTS: We present a probabilistic procedure, ExpressHMM, that adaptively models tiling data prior to predicting expression on genomic sequence. A hidden Markov model (HMM) is used to model the distributions of tiling array probe scores in expressed and non-expressed regions. The HMM is trained on sets of probes mapped to regions of annotated expression and non-expression. Subsequently, prediction of transcribed fragments is made on tiled genomic sequence. The prediction is accompanied by an expression probability curve for visual inspection of the supporting evidence. We test ExpressHMM on data from the Cheng et al. (2005) tiling array experiments on ten Human chromosomes [1]. Results can be downloaded and viewed from our web site [2]. CONCLUSION: The value of adaptive modelling of fluorescence scores prior to categorisation into expressed and non-expressed probes is demonstrated. Our results indicate that our adaptive approach is superior to the previous analysis in terms of nucleotide sensitivity and transfrag specificity.},
  AUTHOR       = {Kasper Munch and Paul Gardner and Peter Arctander and Anders Krogh},
  DOI          = {10.1186/1471-2105-7-239},
  ISSN         = {1471-2105},
  JOURNALTITLE = {BMC Bioinformatics},
  LOCALFILE    = {article/Munch-etal-2006.pdf},
  NUMBER       = {1},
  PAGES        = {239},
  PUBLISHER    = {BioMed Central Ltd},
  TITLE        = {A hidden Markov model approach for determining expression from genomic tiling micro arrays},
  VOLUME       = {7},
  YEAR         = {2006},
}

@ARTICLE{Munch-Krogh-2006,
  ABSTRACT     = {BACKGROUND: The number of sequenced eukaryotic genomes is rapidly increasing. This means that over time it will be hard to keep supplying customised gene finders for each genome. This calls for procedures to automatically generate species-specific gene finders and to re-train them as the quantity and quality of reliable gene annotation grows. RESULTS: We present a procedure, Agene, that automatically generates a species-specific gene predictor from a set of reliable mRNA sequences and a genome. We apply a Hidden Markov model (HMM) that implements explicit length distribution modelling for all gene structure blocks using acyclic discrete phase type distributions. The state structure of the each HMM is generated dynamically from an array of sub-models to include only gene features represented in the training set. CONCLUSION: Acyclic discrete phase type distributions are well suited to model sequence length distributions. The performance of each individual gene predictor on each individual genome is comparable to the best of the manually optimised species-specific gene finders. It is shown that species-specific gene finders are superior to gene finders trained on other species.},
  AUTHOR       = {Kasper Munch and Anders Krogh},
  DOI          = {10.1186/1471-2105-7-263},
  ISSN         = {1471-2105},
  JOURNALTITLE = {BMC Bioinformatics},
  LOCALFILE    = {article/Munch-Krogh-2006.pdf},
  NUMBER       = {1},
  PAGES        = {263},
  PUBLISHER    = {BioMed Central Ltd},
  TITLE        = {Automatic generation of gene finders for eukaryotic species},
  VOLUME       = {7},
  YEAR         = {2006},
}

@REPORT{Murphy-2001-graphical,
  AUTHOR      = {Kevin P. Murphy},
  INSTITUTION = {University of British Columbia},
  TITLE       = {An introduction to graphical models},
  TYPE        = {techreport},
  URL         = {http://people.cs.ubc.ca/~murphyk/Papers/intro_gm.pdf},
  YEAR        = {2001},
}

@ARTICLE{Nash-1951,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {John Nash},
  JOURNALTITLE = {The Annals of Mathematics},
  LOCALFILE    = {article/Nash-1951.pdf},
  NUMBER       = {2},
  PAGES        = {286–295},
  TITLE        = {Non-cooperative games},
  URL          = {http://www.jstor.org/stable/1969529},
  VOLUME       = {54},
  YEAR         = {1951},
}

@ARTICLE{Nau-2006-shape,
  ABSTRACT     = {Incomplete preferences provide the epistemic foundation for models of imprecise subjective probabilities and utilities that are used in robust Bayesian analysis and in theories of bounded rationality. This paper presents a simple axiomatization of incomplete preferences and characterizes the shape of their representing sets of probabilities and utilities. Deletion of the completeness assumption from the axiom system of Anscombe and Aumann yields preferences represented by a convex set of state-dependent expected utilities, of which at least one must be a probability/utility pair. A strengthening of the state-independence axiom is needed to obtain a representation purely in terms of a set of probability/utility pairs.},
  AUTHOR       = {Robert F. Nau},
  DOI          = {10.1214/009053606000000740},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {Axioms of decision theory; Bayesian robustness; coherence; imprecise probabilities and utilities; partial order; state-dependent utility},
  LOCALFILE    = {article/Nau-2006-shape.pdf},
  NUMBER       = {5},
  PAGES        = {2430–2448},
  TITLE        = {The shape of incomplete preferences},
  VOLUME       = {34},
  YEAR         = {2006},
}

@ARTICLE{Nau-1992,
  ABSTRACT     = {This paper presents a quasi-Bayesian model of subjective uncertainty in which beliefs which are represented by lower and upper probabilities qualified by numerical confidence weights. The representation is derived from a system of axioms of binary preferences which differs from standard axiom systems insofar as completeness is not assumed and transitivity is weakened. Confidence-weighted probabilities may be elicited through the acceptance of bets with limited stakes, a generalization of the operational method of de Finetti. The model is applicable to the reconciliation of inconsistent probability judgments and to the sensitivity analysis of Bayesian decision models.},
  AUTHOR       = {Robert F. Nau},
  DOI          = {10.1214/aos/1176348888},
  ISSN         = {0090-5364},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {coherence; confidence-weighted probabilities; fuzzy sets; incompletenes; lower and upper probabilities; second-order probabilities; subjective probability},
  LOCALFILE    = {article/Nau-1992.pdf},
  MONTH        = {12},
  NUMBER       = {4},
  PAGES        = {1737–1767},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Indeterminate probabilities on finite sets},
  VOLUME       = {20},
  YEAR         = {1992},
}

@REPORT{Nesterov-Palma-2000,
  ANNOTATION  = {geannoteerde reprint},
  AUTHOR      = {Yu. Nesterov and André {De Palma}},
  INSTITUTION = {Center for Operations Research \& Econometrics, Université catholique de Louvain},
  NUMBER      = {2000/27},
  TITLE       = {Stable dynamics in transportation systems},
  TYPE        = {CORE discussion paper},
  YEAR        = {2000},
}

@INCOLLECTION{Neumaier-2004,
  AUTHOR    = {Arnold Neumaier},
  EDITOR    = {A. Iserles},
  PAGES     = {271–369},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Complete Search in Continuous Global Optimization and Constraint Satisfaction},
  YEAR      = {2004},
}

@ARTICLE{Neumaier-2003-surprise,
  ABSTRACT     = {This paper presents a new approach to fuzzy modeling based on the concept of surprise. The new concept is related to the traditional membership function by an antitone transformation. Advantages of the surprise approach include: 1. It has a consistent semantic interpretation. 2. It allows the joint use of quantitative and qualitative knowledge, using simple rules of logic. 3. It is a direct extension of (and allows combination with) the least-squares approach to reconciling conflicting approximate numerical data. 4. It is ideally suited to optimization under imprecise or conflicting goals, specified by a combination of soft and hard interval constraints. 5. It gives a straightforward approach to constructing families of functions consistent with fuzzy associative memories as used in fuzzy control, with tuning parameters (reflecting linguistic ambiguity) that can be adapted to available performance data.},
  AUTHOR       = {Arnold Neumaier},
  DOI          = {10.1016/S0165-0114(02)00248-8},
  ISSN         = {0165-0114},
  JOURNALTITLE = {Fuzzy Sets and Systems},
  LOCALFILE    = {article/Neumaier-2003-surprise.pdf},
  NUMBER       = {1},
  PAGES        = {21–38},
  TITLE        = {Fuzzy modeling in terms of surprise},
  VOLUME       = {135},
  YEAR         = {2003},
}

@ARTICLE{vonNeumann-1928,
  AUTHOR       = {John von Neumann},
  DOI          = {10.1007/BF01448847},
  JOURNALTITLE = {Mathematische Annalen},
  LOCALFILE    = {article/vonNeumann-1928.pdf},
  NUMBER       = {1},
  PAGES        = {295–320},
  TITLE        = {Zur Theorie der Gesellschaftsspiele},
  VOLUME       = {100},
  YEAR         = {1928},
}

@BOOK{vonNeumann-Morgenstern-1944,
  AUTHOR    = {John von Neumann and Oskar Morgenstern},
  PUBLISHER = {Princeton University Press},
  TITLE     = {Theory of Games and Economic Behavior},
  YEAR      = {1944},
}

@INPROCEEDINGS{Nilim-ElGhaoui-2003-robust-Markov-NIPS,
  AUTHOR    = {Arnab Nilim and Laurent {El Ghaoui}},
  BOOKTITLE = {NIPS},
  EDITOR    = {Sebastian Thrun and Lawrence K. Saul and Bernhard Schölkopf},
  ISBN      = {0-262-20152-6},
  PUBLISHER = {MIT Press},
  TITLE     = {Robustness in Markov Decision Problems with Uncertain Transition Matrices},
  YEAR      = {2003},
}

@REPORT{Nilim-ElGhaoui-2004-robust-Markov,
  AUTHOR      = {Arnab Nilim and Laurent {El Ghaoui}},
  INSTITUTION = {Department of Electrical Engineering and Computer Sciences},
  LOCATION    = {Berkeley, California},
  NUMBER      = {M04/26},
  SCHOOL      = {University of California Berkeley},
  TITLE       = {Robust Markov Decision Processes with Uncertain Transition Matrices},
  TYPE        = {UCB ERL MEMO},
  YEAR        = {2004},
}

@ARTICLE{Northrop-1936-prob-in-QM,
  AUTHOR       = {F. S. C. Northrop},
  JOURNALTITLE = {Philosophy of Science},
  LOCALFILE    = {article/Northrop-1936-prob-in-QM.pdf},
  NUMBER       = {2},
  PAGES        = {215–232},
  TITLE        = {The Philosophical Significance of the Concept of Probability in Quantum Mechanics},
  URL          = {http://www.jstor.org/stable/184348},
  VOLUME       = {3},
  YEAR         = {1936},
}

@ARTICLE{Nowak-May-Sigmund-1995connor,
  AUTHOR       = {Martin A. Nowak and Robert M. May and Karl Sigmund},
  JOURNALTITLE = {Scientific American},
  PAGES        = {76–81},
  TITLE        = {The Arithmetics of Mutual Help},
  YEAR         = {1995},
}

@BOOK{Nuutila-1995-transcl,
  AUTHOR    = {Esko Nuutila},
  NUMBER    = {74},
  PUBLISHER = {Finnish Academy of Technology},
  SERIES    = {Acta Polytechnica Scandinavica, Mathematics and Computing in Engineering Series},
  TITLE     = {Efficient Transitive Closure Computation in Large Digraphs},
  URL       = {http://www.cs.hut.fi/~enu/thesis.html},
  YEAR      = {1995},
}

@ARTICLE{OHara-OHara-1999,
  AUTHOR       = {K. O'Hara and J. O'Hara},
  DOI          = {10.1046/j.1464-410x.1999.0830s1079.x},
  ISSN         = {1464-4096},
  JOURNALTITLE = {BJU international},
  KEYWORDS     = {Age Distribution; Circumcision; Coitus; Female; Humans; Interpersonal Relations; Male; Male: adverse effects; Male: psychology; Orgasm; Sexual Behavior; Sexual Partners; Sexual Partners: psychology},
  LOCALFILE    = {article/OHara-OHara-1999.pdf},
  MONTH        = {01},
  NUMBER       = {Supplement 1},
  PAGES        = {79–84},
  PMID         = {10349418},
  TITLE        = {The effect of male circumcision on the sexual enjoyment of the female partner.},
  VOLUME       = {83},
  YEAR         = {1999},
}

@BOOK{Oliphant-2006-numpy,
  AUTHOR    = {Travis E. Oliphant},
  PUBLISHER = {Trelgol Publishing},
  TITLE     = {Guide to NumPy},
  URL       = {http://www.tramy.us/numpybook.pdf},
  YEAR      = {2006},
}

@ARTICLE{Oshime-1983,
  AUTHOR       = {Yorimasa Oshime},
  JOURNALTITLE = {Journal of mathematics of Kyoto university},
  LOCALFILE    = {article/Oshime-1983-Perron.pdf},
  NUMBER       = {4},
  PAGES        = {803–830},
  TITLE        = {An extension of Morishima's nonlinear Perron-Frobenius theorem},
  URL          = {http://projecteuclid.org/euclid.kjm/1250521436},
  VOLUME       = {23},
  YEAR         = {1983},
}

@ARTICLE{Ovaere-Deschrijver-Kerre-2009,
  ABSTRACT     = {In this paper we present a new approach to handle uncertainty in the Finite Element Method. As this technique is widely used to tackle real-life design problems, it is also very prone to parameter-uncertainty. It is hard to make a good decision regarding design optimization if no claim can be made with respect to the outcome of the simulation. We propose an approach that combines several techniques in order to offer a total order on the possible design choices, taking the inherent fuzziness into account. Additionally we propose a more efficient ordering procedure to build a total order on fuzzy numbers.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Koen Ovaere and Glad Deschrijver and Etienne E. Kerre},
  DOI          = {10.1007/s12543-009-0002-4},
  JOURNALTITLE = {Fuzzy Information and Engineering},
  KEYWORDS     = {fuzzy decision making; fuzzy finite element method; fuzzy ordering},
  LOCALFILE    = {article/Ovaere-deschrijver-Kerre-2009.pdf},
  NUMBER       = {1},
  PAGES        = {27–36},
  PUBLISHER    = {Springer},
  TITLE        = {Application of fuzzy decision making to the fuzzy finite element method},
  VOLUME       = {1},
  YEAR         = {2009},
}

@BOOK{Pearl-1988,
  AUTHOR    = {Judea Pearl},
  LOCATION  = {San Francisco, California},
  PUBLISHER = {Morgan Kaufmann},
  TITLE     = {Probabilistic Reasoning in Intelligent Systems},
  YEAR      = {1988},
}

@ARTICLE{Pearl-1988-intervals,
  ABSTRACT     = {The apparent failure of individual probabilistic expressions to distinguish between uncertainty and ignorance, and between certainty and confidence, have swayed researchers to seek alternative formalisms, where confidence measures are provided explicit notation. This paper summarizes how a causal networks formulation of probabilities facilitates the representation of confidence measures as an integral part of a knowledge system that does not require the use of higher order probabilities. We also examine whether Dempster-Shafer intervals represent confidence about probabilities.},
  AUTHOR       = {Judea Pearl},
  DOI          = {10.1016/0888-613X(88)90117-X},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Dempster-Shafer theory; ca; interval probability},
  LOCALFILE    = {article/Pearl-1988-intervals.pdf},
  NUMBER       = {3},
  PAGES        = {211–216},
  PUBLISHER    = {Elsevier},
  TITLE        = {On probability intervals},
  VOLUME       = {2},
  YEAR         = {1988},
}

@ARTICLE{Pelessoni-Vicig-2009,
  ABSTRACT     = {In this paper we consider some bounds for lower previsions that are either coherent or, more generally, centered convex. We focus on bounds concerning the classical product and Bayes' rules, discussing first weak product rules and some of their implications for coherent lower previsions. We then generalise a well-known lower bound, which is a (weak) version for events and coherent lower probabilities of Bayes' theorem, to the case of random variables and (centered) convex previsions. We obtain a family of bounds and show that one of them is undominated in all cases. Some applications are outlined, and it is shown that 2-monotonicity, which ensures that the bound is sharp in the case of events, plays a much more limited role in this general framework.},
  AUTHOR       = {Renato Pelessoni and Paolo Vicig},
  DOI          = {10.1080/15598608.2009.10411913},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {2-monotonicity; Bayes’ theorem; Centered convex previsions; Conditional lower previsions; Product rule},
  LOCALFILE    = {article/Pelessoni-Vicig-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {85–101},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Bayes' Theorem Bounds for Convex Lower Previsions},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Pelessoni-Vicig-2005-convex,
  ABSTRACT     = {Two classes of imprecise previsions, which we termed convex and centered convex previsions, are studied in this paper in a framework close to Walley's and Williams' theory of imprecise previsions. We show that convex previsions are related with a concept of convex natural extension, which is useful in correcting a large class of inconsistent imprecise probability assessments, characterised by a condition of avoiding unbounded sure loss. Convexity further provides a conceptual framework for some uncertainty models and devices, like unnormalised supremum preserving functions. Centered convex previsions are intermediate between coherent previsions and previsions avoiding sure loss, and their not requiring positive homogeneity is a relevant feature for potential applications. We discuss in particular their usage in (financial) risk measurement. In a final part we introduce convex imprecise previsions in a conditional environment and investigate their basic properties, showing how several of the preceding notions may be extended and the way the generalised Bayes rule applies.},
  AUTHOR       = {Renato Pelessoni and Paolo Vicig},
  DOI          = {10.1016/j.ijar.2004.10.007},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Convex conditional imprecise previsions; Convex imprecise previsions; Convex natural extension; Generalised Bayes rule; Risk measures},
  LOCALFILE    = {article/Pelessoni-Vicig-2005-convex.pdf},
  NUMBER       = {2-3},
  PAGES        = {297–319},
  TITLE        = {Uncertainty modelling and conditioning with convex imprecise previsions},
  VOLUME       = {39},
  YEAR         = {2005},
}

@ARTICLE{2003-Pelessoni+Vicig-risk,
  ABSTRACT     = {In this paper the theory of coherent imprecise previsions is applied to risk measurement. We introduce the notion of coherent risk measure defined on an arbitrary set of risks, showing that it can be considered a special case of coherent upper prevision. We also prove that our definition generalizes the notion of coherence for risk measures defined on a linear space of random numbers, given in literature. Consistency properties of Value-at-Risk (VaR), currently one of the most used risk measures, are investigated too, showing that it does not necessarily satisfy a weaker notion of consistency called 'avoiding sure loss'. We introduce sufficient conditions for VaR to avoid sure loss and to be coherent. Finally we discuss ways of modifying incoherent risk measures into coherent ones.},
  AUTHOR       = {Renato Pelessoni and Paolo Vicig},
  DOI          = {10.1142/S0218488503002156},
  JOURNALTITLE = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  KEYWORDS     = {Imprecise prevision; Value-at-Risk; avoiding sure loss condition; coherent risk measure},
  LOCALFILE    = {article/2003-Pelessoni+Vicig-risk.pdf},
  NUMBER       = {4},
  PAGES        = {393–412},
  TITLE        = {Imprecise previsions for risk measurement},
  VOLUME       = {11},
  YEAR         = {2003},
}

@ARTICLE{Pericchi-Walley-1991,
  AUTHOR       = {Luis Raúl Pericchi and Peter Walley},
  JOURNALTITLE = {International Statistical Review},
  LOCALFILE    = {article/Pericchi-Walley-1991.pdf},
  NUMBER       = {1},
  PAGES        = {1–23},
  TITLE        = {Robust Bayesian credible intervals and prior ignorance},
  URL          = {http://www.jstor.org/stable/1403571},
  VOLUME       = {58},
  YEAR         = {1991},
}

@ARTICLE{Peterson-1972-Radon,
  AUTHOR       = {B. B. Peterson},
  JOURNALTITLE = {The American Mathematical Monthly},
  LOCALFILE    = {article/Peterson-1972-Radon.pdf},
  NUMBER       = {9},
  PAGES        = {949–963},
  TITLE        = {The geometry of Radon's theorem},
  URL          = {http://www.jstor.org/stable/2318065},
  VOLUME       = {79},
  YEAR         = {1972},
}

@INCOLLECTION{Piatti-et-al-2010-CNtutorial,
  ANNOTATION = {ook op papier},
  AUTHOR     = {Alberto Piatti and Alessandro Antonucci and Marco Zaffalon},
  BOOKTITLE  = {Advances in Mathematics Research},
  EDITOR     = {Albert R. Baswell},
  LOCATION   = {New York},
  PUBLISHER  = {Nova Publishers},
  TITLE      = {Building Knowledge-Based Systems by Credal Networks: A Tutorial},
  VOLUME     = {11},
  YEAR       = {2010},
}

@INPROCEEDINGS{2010-Pieters+Gaag,
  AUTHOR    = {Barbara F. I. Pieters and Linda C. van der Gaag},
  BOOKTITLE = {Proceedings of the 22nd Benelux Conference on Artificial Intelligence},
  LOCALFILE = {inproceedings/2010-Pieters+Gaag.pdf},
  TITLE     = {On lurking dependencies and naive Bayesian classifiers},
  YEAR      = {2010},
}

@INCOLLECTION{Pieters+Van_der_Gaag+Feelders-2011,
  AUTHOR    = {Barbara F. I. Pieters and Linda C. van der Gaag and Ad Feelders},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-642-22152-1_36},
  EDITOR    = {Weiru Liu},
  ISBN      = {978-3-642-22151-4},
  LOCALFILE = {inproceedings/Pieters+Van_der_Gaag+Feelders-2011.pdf},
  PAGES     = {422–433},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {When learning naive Bayesian classifiers preserves monotonicity},
  VOLUME    = {6717},
  YEAR      = {2011},
}

@ARTICLE{Pinkus-2005-approx,
  ABSTRACT     = {Approximation theory is concerned with the ability to approximate functions by simpler and more easily calculated functions. The first question we ask in approximation theory concerns the possibility of approximation. Is the given family of functions from which we plan to approximate dense in the set of functions we wish to approximate? In this work, we survey some of the main density results and density methods.},
  ARXIVID      = {math/0501328},
  AUTHOR       = {Allan Pinkus},
  EPRINT       = {0501328},
  EPRINTCLASS  = {math},
  EPRINTTYPE   = {arXiv},
  JOURNALTITLE = {Surveys in Approximation Theory},
  LOCALFILE    = {article/Pinkus-2005-approx.pdf},
  PAGES        = {1–45},
  TITLE        = {Density in Approximation Theory},
  URL          = {http://www.math.technion.ac.il/sat/papers/1},
  VOLUME       = {1},
  YEAR         = {2005},
}

@ARTICLE{Polya-1930,
  ANNOTATION   = {Bevat deel over urneproblemen},
  AUTHOR       = {G. Pólya},
  JOURNALTITLE = {Annales de l'Institut Henri Poincaré},
  LOCALFILE    = {article/Polya-1930.pdf},
  NUMBER       = {2},
  PAGES        = {117–161},
  TITLE        = {Sur quelques points de la théorie des probabilités},
  URL          = {http://www.numdam.org/item?id=AIHP_1930__1_2_117_0},
  VOLUME       = {1},
  YEAR         = {1930},
}

@BOOK{Prautzsch-Boehm-Paluszny-2002-Bezier,
  ANNOTATION = {boek bij Gert},
  AUTHOR     = {Hartmut Prautzsch and Wolfgang Boehm and Marco Paluszny},
  PUBLISHER  = {Springer},
  SERIES     = {Mathematics and visualization},
  TITLE      = {Bézier and B-spline Techniques},
  YEAR       = {2002},
}

@MISC{Quaeghebeur-mcohconstraints,
  AUTHOR     = {Erik Quaeghebeur},
  SHORTTITLE = {mcohconstraints},
  TITLE      = {mcohconstraints: Matlab/Octave functions for generating coherence and avoiding sure loss constraints for lower previsions},
  URL        = {http://github.com/equaeghe/mcohconstraints},
}

@MISC{Quaeghebeur-pycohconstraints,
  AUTHOR     = {Erik Quaeghebeur},
  SHORTTITLE = {pycohconstraints},
  TITLE      = {pycohconstraints: Python code for generating coherence constraints for lower previsions},
  URL        = {http://github.com/equaeghe/pycohconstraints},
}

@MISC{Quaeghebeur-murasyp,
  AUTHOR = {Erik Quaeghebeur},
  TITLE  = {murasyp: Python software for accept/reject statement-based uncertainty modeling},
  URL    = {http://equaeghe.github.com/murasyp},
  YEAR   = {in progress},
}

@MISC{2010-Quaeghebeur-SIPTASS,
  AUTHOR       = {Erik Quaeghebeur},
  HOWPUBLISHED = {Lecture at the 4th SIPTA Summer School, Durham, UK},
  TITLE        = {Inference \& Desirability},
  URL          = {http://users.ugent.be/~equaeghe/#EQ-2010-SSS},
  YEAR         = {2010},
}

@BOOK{Raiffa-Schlaifer-1961,
  ANNOTATION = {Boek bij René},
  AUTHOR     = {Howard Raiffa and Robert Schlaifer},
  LOCATION   = {Cambridge, Massachusetts},
  PUBLISHER  = {MIT Press},
  TITLE      = {Applied Statistical Decision Theory},
  YEAR       = {1968},
}

@ARTICLE{Renooij-2001,
  AUTHOR       = {Silja Renooij},
  DOI          = {10.1017/S0269888901000145},
  JOURNALTITLE = {The Knowledge Engineering Review},
  MONTH        = {09},
  NUMBER       = {3},
  PAGES        = {255–269},
  TITLE        = {Probability elicitation for belief networks: Issues to consider},
  VOLUME       = {16},
  YEAR         = {2001},
}

@INPROCEEDINGS{2008-Renooij+Gaag,
  AUTHOR    = {Silja Renooij and Linda C. van der Gaag},
  BOOKTITLE = {Proceedings of the Fourth Workshop on Probabilistic Graphical Models},
  EDITOR    = {M. Jaeger and T. D. Nielsen},
  LOCALFILE = {inproceedings/2008-Renooij+Gaag.pdf},
  PAGES     = {241–248},
  TITLE     = {Discrimination and its sensitivity in probabilistic networks},
  YEAR      = {2008},
}

@ARTICLE{Renooij+Van_der_Gaag-2008,
  ABSTRACT     = {Empirical evidence shows that naive Bayesian classifiers perform quite well compared to more sophisticated classifiers, even in view of inaccuracies in their parameters. In this paper, we study the effects of such parameter inaccuracies by investigating the sensitivity functions of a naive Bayesian network. We show that, as a consequence of the network’s independence properties, these sensitivity functions are highly constrained. We further investigate whether the patterns of sensitivity that follow from these functions support the observed robustness of naive Bayesian classifiers. In addition to standard sensitivities given available evidence, we also study the effect of parameter inaccuracies in view of scenarios of additional evidence. We show that standard sensitivity functions suffice to describe such scenario sensitivities.},
  AUTHOR       = {Silja Renooij and Linda C. van der Gaag},
  DOI          = {10.1016/j.ijar.2008.02.008},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Naive Bayesian classifiers; Sensitivity; Robustness},
  LOCALFILE    = {article/Renooij+Van_der_Gaag-2008.pdf},
  NUMBER       = {2},
  PAGES        = {398–416},
  TITLE        = {Evidence and scenario sensitivities in naive Bayesian classifiers},
  VOLUME       = {49},
  YEAR         = {2008},
}

@ARTICLE{Renooij-VanderGaag-2008-EQPN,
  ABSTRACT     = {Qualitative probabilistic networks were designed to overcome, to at least some extent, the quantification problem known to probabilistic networks. Qualitative networks abstract from the numerical probabilities of their quantitative counterparts by using signs to summarise the probabilistic influences between their variables. One of the major drawbacks of these qualitative abstractions, however, is the coarse level of representation detail that does not provide for indicating strengths of influences. As a result, the trade-offs modelled in a network remain unresolved upon inference. We present an enhanced formalism of qualitative probabilistic networks to provide for a finer level of representation detail. An enhanced qualitative probabilistic network differs from a basic qualitative network in that it distinguishes between strong and weak influences. Now, if a strong influence is combined, upon inference, with a conflicting weak influence, the sign of the net influence may be readily determined. Enhanced qualitative networks are purely qualitative in nature, as basic qualitative networks are, yet allow for resolving some trade-offs upon inference.},
  AUTHOR       = {Silja Renooij and Linda C. van der Gaag},
  DOI          = {10.1016/j.artint.2008.04.001},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Probabilistic reasoning; Qualitative reasoning; Trade-off resolution},
  NUMBER       = {12-13},
  PAGES        = {1470–1494},
  TITLE        = {Enhanced qualitative probabilistic networks for resolving trade-offs},
  VOLUME       = {172},
  YEAR         = {2008},
}

@ARTICLE{Renooij-VanderGaag-2000-IPMU,
  AUTHOR    = {Silja Renooij and Linda C. van der Gaag},
  BOOKTITLE = {IPMU 2000: Proceedings of the Eighth International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
  PAGES     = {1285–1290},
  TITLE     = {Exploiting non-monotonic influences in qualitative belief networks},
  VENUE     = {Madrid},
  YEAR      = {2000},
}

@ARTICLE{Renooij-VanderGaag-Parsons-2002,
  ABSTRACT     = {Qualitative probabilistic networks are qualitative abstractions of probabilistic networks, summarising probabilistic influences by qualitative signs. As qualitative networks model influences at the level of variables, knowledge about probabilistic influences that hold only for specific values cannot be expressed. The results computed from a qualitative network, as a consequence, can be weaker than strictly necessary and may in fact be rather uninformative. We extend the basic formalism of qualitative probabilistic networks by providing for the inclusion of context-specific information about influences and show that exploiting this information upon reasoning has the ability to forestall unnecessarily weak results.},
  AUTHOR       = {Silja Renooij and Linda C. van der Gaag and Simon Parsons},
  DOI          = {10.1016/S0004-3702(02)00247-3},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Probabilistic reasoning; Qualitative reasoning; Context-specific independence; Non-monotonicity},
  NUMBER       = {1-2},
  PAGES        = {207–230},
  TITLE        = {Context-specific sign-propagation in qualitative probabilistic networks},
  VOLUME       = {140},
  YEAR         = {2002},
}

@ARTICLE{Renooij-Witteman-1999,
  ABSTRACT     = {The number of knowledge-based systems that build on Bayesian belief networks is increasing. The construction of such a network however requires a large number of probabilities in numerical form. This is often considered a major obstacle, one of the reasons being that experts are reluctant to provide numerical probabilities. The use of verbal probability expressions as an additional method of eliciting probabilistic information may to some extent remove this obstacle. In this paper, we review studies that address the communication of probabilities in words and/or numbers. We then describe our own experiments concerning the development of a probability scale that contains words as well as numbers. This scale appears to be an aid for researchers and domain experts during the elicitation phase of building a belief network and might help users understand the output of the network.},
  AUTHOR       = {Silja Renooij and Cilia L. M. Witteman},
  DOI          = {10.1016/S0888-613X(99)00027-4},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Communicating probability; Expert systems; Knowledge elicitation; Explanation},
  NUMBER       = {3},
  PAGES        = {169–194},
  TITLE        = {Talking probabilities: communicating probabilistic information with words and numbers},
  VOLUME       = {22},
  YEAR         = {1999},
}

@ARTICLE{Revuz-1956,
  AUTHOR       = {André Revuz},
  JOURNALTITLE = {Annales de l'Institut Fourier},
  LOCALFILE    = {article/Revuz-1956.pdf},
  PAGES        = {187–269},
  TITLE        = {Fonctions croissantes et mesures sur les espaces topologiques ordonnés},
  URL          = {http://www.numdam.org/item?id=AIF_1956__6__187_0},
  VOLUME       = {6},
  YEAR         = {1956},
}

@ARTICLE{Rho-Tang-Ye-2010,
  ABSTRACT     = {The advances of next-generation sequencing technology have facilitated metagenomics research that attempts to determine directly the whole collection of genetic material within an environmental sample (i.e. the metagenome). Identification of genes directly from short reads has become an important yet challenging problem in annotating metagenomes, since the assembly of metagenomes is often not available. Gene predictors developed for whole genomes (e.g. Glimmer) and recently developed for metagenomic sequences (e.g. MetaGene) show a significant decrease in performance as the sequencing error rates increase, or as reads get shorter. We have developed a novel gene prediction method FragGeneScan, which combines sequencing error models and codon usages in a hidden Markov model to improve the prediction of protein-coding region in short reads. The performance of FragGeneScan was comparable to Glimmer and MetaGene for complete genomes. But for short reads, FragGeneScan consistently outperformed MetaGene (accuracy improved ~62\% for reads of 400 bases with 1\% sequencing errors, and ~18\% for short reads of 100 bases that are error free). When applied to metagenomes, FragGeneScan recovered substantially more genes than MetaGene predicted (>90\% of the genes identified by homology search), and many novel genes with no homologs in current protein sequence database.},
  AUTHOR       = {Mina Rho and Haixu Tang and Yuzhen Ye},
  DOI          = {10.1093/nar},
  JOURNALTITLE = {Nucleic Acids Research},
  LOCALFILE    = {article/Rho-Tang-Ye-2010.pdf},
  NUMBER       = {20},
  PAGES        = {e191},
  TITLE        = {FragGeneScan: predicting genes in short and error-prone reads},
  VOLUME       = {38},
  YEAR         = {2010},
}

@INCOLLECTION{Rietbergen-Van_der_Gaag-2011,
  AUTHOR    = {Merel T. Rietbergen and Linda C. van der Gaag},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-642-22152-1_12},
  EDITOR    = {Weiru Liu},
  ISBN      = {978-3-642-22151-4},
  LOCALFILE = {inproceedings/Rietbergen-Van_der_Gaag-2011.pdf},
  PAGES     = {134–145},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Attaining monotonicity for Bayesian networks},
  VOLUME    = {6717},
  YEAR      = {2011},
}

@ARTICLE{Rinaldi-Schoen-Sciandrone-2010,
  ABSTRACT     = {Given a non empty polyhedral set, we consider the problem of finding a vector belonging to it and having the minimum number of nonzero components, i.e., a feasible vector with minimum zero-norm. This combinatorial optimization problem is NP-Hard and arises in various fields such as machine learning, pattern recognition, signal processing. One of the contributions of this paper is to propose two new smooth approximations of the zero-norm function, where the approximating functions are separable and concave. In this paper we first formally prove the equivalence between the approximating problems and the original nonsmooth problem. To this aim, we preliminarily state in a general setting theoretical conditions sufficient to guarantee the equivalence between pairs of problems. Moreover we also define an effective and efficient version of the Frank-Wolfe algorithm for the minimization of concave separable functions over polyhedral sets in which variables which are null at an iteration are eliminated for all the following ones, with significant savings in computational time, and we prove the global convergence of the method. Finally, we report the numerical results on test problems showing both the usefulness of the new concave formulations and the efficiency in terms of computational time of the implemented minimization algorithm.},
  AUTHOR       = {F. Rinaldi and F. Schoen and M. Sciandrone},
  DOI          = {10.1007/s10589-008-9202-9},
  ISSN         = {0926-6003},
  JOURNALTITLE = {Computational Optimization and Applications},
  LOCALFILE    = {article/Rinaldi-Schoen-Sciandrone-2010.pdf},
  NUMBER       = {3},
  PAGES        = {467–486},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Concave programming for minimizing the zero-norm over polyhedral sets},
  VOLUME       = {46},
  YEAR         = {2010},
}

@ARTICLE{Robinson-1951,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Julia Robinson},
  JOURNALTITLE = {The Annals of Mathematics},
  LOCALFILE    = {article/Robinson-1951.pdf},
  NUMBER       = {2},
  PAGES        = {296–301},
  TITLE        = {An iterative method of solving a game},
  URL          = {http://www.jstor.org/stable/1969530},
  VOLUME       = {54},
  YEAR         = {1951},
}

@BOOK{Rockafellar-1970,
  AUTHOR    = {R. Tyrell Rockafellar},
  LOCATION  = {Princeton, New Jersey},
  PUBLISHER = {Princeton University Press},
  SERIES    = {Princeton Landmarks in Mathematics},
  TITLE     = {Convex Analysis},
  YEAR      = {1970},
}

@BOOK{Rohn-2005,
  AUTHOR = {Jiri Rohn},
  TITLE  = {A Handbook of Results on Interval Linear Problems},
  URL    = {http://uivtx.cs.cas.cz/~rohn/handbook},
  YEAR   = {2005},
}

@ARTICLE{Rommelfanger-2004,
  ABSTRACT     = {Classical mathematical programming models require well-defined coefficients and right hand sides. In order to avoid a non satisfying modeling usually a broad information gathering and processing is necessary. In case of real problems some model parameters can be only roughly estimated. While in case of classical models the vague data is replaced by "average data", fuzzy models offer the opportunity to model subjective imaginations of the decision maker as precisely as a decision maker will be able to describe it. Thus the risk of applying a wrong model of the reality and selecting solutions which do not reflect the real problem can be clearly reduced. The modeling of real problems by means of deterministic and stochastic models requires extensive information processing. On the other hand we know that an optimum solution is finally defined only by few restrictions. Especially in case of larger systems we notice afterwards that most of the information is useless. The dilemma of data processing is due to the fact that first we have to calculate the solution in order to define, whether the information must be well-defined or whether vague data may be sufficient. Based on multicriteria programming problems it should be demonstrated that the dilemma of data processing in case of real programming problems can be handled adequately by modeling them as fuzzy system combined with an interactive problem-solving. Describing the real problem by means of a fuzzy system first of all only the available information or such information which can be achieved easily will be considered. Then we try to develop an optimum solution. With reference to the cost-benefit relation further information can be gathered in order to describe the solution more precisely. Furthermore it should be pointed out that some interactive fuzzy solution algorithms, e.g. FULPAL provide the opportunity to solve mixed integer multicriteria programming models as well.},
  AUTHOR       = {Heinrich Rommelfanger},
  DOI          = {10.1007/s10700-004-4200-6},
  ISSN         = {1568-4539},
  JOURNALTITLE = {Fuzzy Optimization and Decision Making},
  LOCALFILE    = {article/Rommelfanger-2004.pdf},
  PAGES        = {295–309},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {The Advantages of Fuzzy Optimization Models in Practical Use},
  VOLUME       = {3},
  YEAR         = {2004},
}

@ARTICLE{Rommelfanger-1996,
  ABSTRACT     = {This paper presents a survey on methods for solving fuzzy linear programs. First LP models with soft constraints are discussed. Then LP problems in which coefficients of constraints and/or of the objective function may be fuzzy are outlined. Pivotal questions are the interpretation of the inequality relation in fuzzy constraints and the meaning of fuzzy objectives. In addition to the commonly applied extended addition, based on the min-operator and used for the aggregation of the left-hand sides of fuzzy constraints and fuzzy objectives, a more flexible procedure, based on Yager's parametrized t-norm Tp, is presented. Finally practical applications of fuzzy linear programs are listed.},
  AUTHOR       = {Heinrich Rommelfanger},
  DOI          = {10.1016/0377-2217(95)00008-9},
  ISSN         = {0377-2217},
  JOURNALTITLE = {European Journal of Operational Research},
  KEYWORDS     = {Compromise solution; Extended addition of fuzzy intervals; Fuzzy sets; Inequality relation in fuzzy conslraints; Mathematical Programming},
  LOCALFILE    = {article/Rommelfanger-1996.pdf},
  NUMBER       = {3},
  PAGES        = {512–527},
  TITLE        = {Fuzzy linear programming and applications},
  VOLUME       = {92},
  YEAR         = {1996},
}

@ARTICLE{Rosenthal-1995-Markov-rate,
  ABSTRACT     = {This is an expository paper that presents various ideas related to nonasymptotic rates of convergence for Markov chains. Such rates are of great importance for stochastic algorithms that are widely used in statistics and in computer science. They also have applications to analysis of card shuffling and other areas.In this paper, we attempt to describe various mathematical techniques that have been used to bound such rates of convergence. In particular, we describe eigenvalue analysis, random walks on groups, coupling, and minorization conditions. Connections are made to modern areas of research wherever possible. Elements of linear algebra, probability theory, group theory, and measure theory are used, but efforts are made to keep the presentation elementary and accessible.},
  AUTHOR       = {Jeffrey S. Rosenthal},
  DOI          = {10.1137/1037083},
  JOURNALTITLE = {SIAM Review},
  KEYWORDS     = {Markov chain; coupling; eigenvalue; random walk on group},
  LOCALFILE    = {article/Rosenthal-1995-Markov-rate.pdf},
  NUMBER       = {3},
  PAGES        = {387–405},
  TITLE        = {Convergence rates for Markov chains},
  URL          = {http://www.jstor.org/stable/2132659},
  VOLUME       = {37},
  YEAR         = {1995},
}

@ARTICLE{Rota-1964-moebius,
  AUTHOR       = {Gian-Carlo Rota},
  DOI          = {10.1007/BF00531932},
  JOURNALTITLE = {Probability Theory and Related Fields},
  LOCALFILE    = {article/Rota-1964-moebius.pdf},
  MONTH        = {01},
  NUMBER       = {4},
  PAGES        = {340–368},
  TITLE        = {On the foundations of combinatorial theory: I. Theory of Möbius functions},
  VOLUME       = {2},
  YEAR         = {1964},
}

@ARTICLE{Roy-1987,
  AUTHOR       = {Nina M. Roy},
  DOI          = {10.2307/2322725},
  JOURNALTITLE = {The American Mathematical Monthly},
  LOCALFILE    = {article/Roy-1987.pdf},
  NUMBER       = {5},
  PAGES        = {409–422},
  TITLE        = {Extreme points of convex sets in infinite dimensional spaces},
  URL          = {http://www.jstor.org/stable/2322725},
  VOLUME       = {94},
  YEAR         = {1987},
}

@ARTICLE{Rudolph-1996-duality,
  ABSTRACT     = {Using elementary module theory, an intrinsic definition of the dual (or adjoint) of a generalized time-varying linear system is given. With this, the duality of controllability and observability is recovered from their intrinsic module theoretical definitions. The duality of state feedback and output injection is discussed both in the static case and for its quasistatic generalization. Related Brunovsky type canonical forms are derived in the quasistatic case. The corresponding controllability indices and their duals the observability indices are defined intrinsically.},
  AUTHOR       = {J. Rudolph},
  DOI          = {10.1016/0024-3795(94)00222-3},
  ISSN         = {0024-3795},
  JOURNALTITLE = {Linear Algebra and its Applications},
  LOCALFILE    = {article/Rudolph-1996-duality.pdf},
  PAGES        = {83–106},
  TITLE        = {Duality in time-varying linear systems: a module theoretic approach},
  VOLUME       = {245},
  YEAR         = {1996},
}

@ARTICLE{Sahinidis-2004,
  ABSTRACT     = {A large number of problems in production planning and scheduling, location, transportation, finance, and engineering design require that decisions be made in the presence of uncertainty. Uncertainty, for instance, governs the prices of fuels, the availability of electricity, and the demand for chemicals. A key difficulty in optimization under uncertainty is in dealing with an uncertainty space that is huge and frequently leads to very large-scale optimization models. Decision-making under uncertainty is often further complicated by the presence of integer decision variables to model logical and other discrete decisions in a multi-period or multi-stage setting. This paper reviews theory and methodology that have been developed to cope with the complexity of optimization problems under uncertainty. We discuss and contrast the classical recourse-based stochastic programming, robust stochastic programming, probabilistic (chance-constraint) programming, fuzzy programming, and stochastic dynamic programming. The advantages and shortcomings of these models are reviewed and illustrated through examples. Applications and the state-of-the-art in computations are also reviewed. Finally, we discuss several main areas for future development in this field. These include development of polynomial-time approximation schemes for multi-stage stochastic programs and the application of global optimization algorithms to two-stage and chance-constraint formulations.},
  AUTHOR       = {Nikolaos V. Sahinidis},
  DOI          = {10.1016/j.compchemeng.2003.09.017},
  ISSN         = {0098-1354},
  JOURNALTITLE = {Computers \& Chemical Engineering},
  KEYWORDS     = {Stochastic programming; Fuzzy programming; Stochastic dynamic programming; Global optimization; Approximation algorithms},
  LOCALFILE    = {article/Sahinidis-2004.pdf},
  NUMBER       = {6–7},
  PAGES        = {971–983},
  TITLE        = {Optimization under uncertainty: state-of-the-art and opportunities},
  VOLUME       = {28},
  YEAR         = {2004},
}

@ARTICLE{Sarukkai-2000-link-prediction,
  ABSTRACT     = {The enormous growth in the number of documents in the World Wide Web increases the need for improved link navigation and path analysis models. Link prediction and path analysis are important problems with a wide range of applications ranging from personalization to Web server request prediction. The sheer size of the World Wide Web coupled with the variation in users' navigation patterns makes this a very difficult sequence modelling problem. In this paper, the notion of probabilistic link prediction and path analysis using Markov chains is proposed and evaluated. Markov chains allow the system to dynamically model the URL access patterns that are observed in navigation logs based on the previous state. Furthermore, the Markov chain model can also be used in a generative mode to automatically obtain tours. The Markov transition matrix can be analysed further using eigenvector decomposition to obtain `personalized hubs/authorities'. The utility of the Markov chain approach is demonstrated in many domains: HTTP request prediction, system-driven adaptive Web navigation, tour generation, and detection of `personalized hubs/authorities' from user navigation profiles. The generality and power of Markov chains is a first step towards the application of powerful probabilistic models to Web path analysis and link prediction.},
  AUTHOR       = {Ramesh R. Sarukkai},
  DOI          = {10.1016/S1389-1286(00)00044-X},
  JOURNALTITLE = {Computer Networks},
  KEYWORDS     = {Adaptive navigation; HTTP request; Hubs=authorities; Link prediction; Markov chains; Tour generation},
  LOCALFILE    = {article/Sarukkai-2000-link-prediction.pdf},
  NUMBER       = {1-6},
  PAGES        = {377–386},
  TITLE        = {Link prediction and path analysis using Markov chains},
  VOLUME       = {33},
  YEAR         = {2000},
}

@ARTICLE{Savage-1971,
  AUTHOR       = {Leonard J. Savage},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Savage-1971.pdf},
  NUMBER       = {336},
  PAGES        = {783–801},
  TITLE        = {Elicitation of personal probabilities and expectations},
  URL          = {http://www.jstor.org/stable/2284229},
  VOLUME       = {66},
  YEAR         = {1971},
}

@BOOK{Schechter-1997-HAF,
  AUTHOR    = {Eric Schechter},
  PUBLISHER = {Academic Press},
  TITLE     = {Handbook of Analysis and Its Foundations},
  YEAR      = {1997},
}

@INCOLLECTION{Schempp-1977-Bernstein,
  AUTHOR    = {Walter Schempp},
  BOOKTITLE = {Constructive Theory of Functions of Several Variables},
  DOI       = {10.1007/BFb0086576},
  EDITOR    = {Walter Schempp and Karl Zeller},
  LOCALFILE = {inbook/Schempp-1977-Bernstein.pdf},
  PAGES     = {212–219},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Mathematics},
  TITLE     = {Bernstein Polynomials in Several Variables},
  VOLUME    = {571},
  YEAR      = {1977},
}

@ARTICLE{Schervish-1989-forecaster,
  AUTHOR       = {Mark J. Schervish},
  DOI          = {10.1214/aos/1176347398},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Schervish-1989-forecaster.pdf},
  NUMBER       = {4},
  PAGES        = {1856–1879},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {A general method for comparing probability assessors},
  VOLUME       = {17},
  YEAR         = {1989},
}

@INPROCEEDINGS{Schervish-Seidenfeld-Kadane-2003-incoherence,
  AUTHOR       = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
  BOOKTITLE    = {Bayesian Statistics 7},
  EDITOR       = {José M. Bernardo and others},
  LOCALFILE    = {inproceedings/Schervish-Seidenfeld-Kadane-2002-incoherence.pdf},
  LOCATION     = {Oxford},
  NOTE         = {Proceedings of the Seventh Valencia International Meeting, 2–6 June 2002},
  ORGANIZATION = {ISBA},
  PAGES        = {385–402},
  PUBLISHER    = {Clarendon Press},
  TITLE        = {Measures of incoherence: How not to gamble if you must},
  YEAR         = {2003},
}

@INPROCEEDINGS{Schervish-Seidenfeld-Kadane-1999-incoherence,
  ABSTRACT  = {We introduce two indices for the degree of incoherence in a set of lower and upper previsions: maximizing the rate of loss the incoherent Bookie experiences in a Dutch Book, or maximizing the rate of profit the Gambler achieves who makes Dutch Book against the incoherent Bookie. We report how efficient bookmaking is achieved against these two indices in the case of incoherent previsions for events on a finite partition, and for incoherent previsions that include also a simple random variable. We relate the epsilon-contamination model to efficient bookmaking in the case of the rate of profit.},
  AUTHOR    = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
  BOOKTITLE = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
  EDITOR    = {Gert de Cooman and Fabio Gagliardi Cozman and Serafín Moral and Peter Walley},
  KEYWORDS  = {Dutch Book; coherence; epsilon-contamination mod},
  LOCALFILE = {inproceedings/Schervish-Seidenfeld-Kadane-1999-incoherence.pdf},
  PAGES     = {319–323},
  TITLE     = {How Sets of Coherent Probabilities may Serve as Models for Degrees of Incoherence},
  URL       = {http://decsai.ugr.es/~smc/isipta99/proc/072.html},
  VENUE     = {Ghent, Belgium},
  YEAR      = {1999},
}

@ARTICLE{Schervish-Seidenfeld-Kadane-2002-incoherence,
  AUTHOR       = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
  JOURNALTITLE = {Sankhya Series A},
  LOCALFILE    = {article/Schervish-Seidenfeld-Kadane-2002-incoherence.pdf},
  NUMBER       = {3},
  PAGES        = {561–587},
  TITLE        = {Measuring Incoherence},
  URL          = {http://repository.cmu.edu/statistics/29},
  VOLUME       = {64},
  YEAR         = {2002},
}

@REPORT{Schervish-Seidenfeld-Kadane-1998,
  AUTHOR      = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
  INSTITUTION = {Carnegie Mellon University},
  LOCALFILE   = {techreport/Schervish-Seidenfeld-Kadane-1998.pdf},
  NUMBER      = {660},
  TITLE       = {Two Measures of Incoherence: How Not to Gamble If You Must},
  TYPE        = {techreport},
  URL         = {http://www.stat.cmu.edu/tr/tr660/tr660.html},
  YEAR        = {1998},
}

@BOOK{Schneier-1996-crypto,
  ANNOTATION = {stukken uit H5,6 op papier},
  AUTHOR     = {Bruce Schneier},
  PUBLISHER  = {John Wiley \& Sons},
  TITLE      = {Applied Cryptography},
  URL        = {http://www.schneier.com/book-applied.html},
  YEAR       = {1996},
}

@INPROCEEDINGS{Schrage-IJzendoorn-VanderGaag-2005,
  AUTHOR    = {Martijn M. Schrage and Arjan van IJzendoorn and Linda C. van der Gaag},
  BOOKTITLE = {Haskell '05: Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
  DOI       = {10.1145/1088348.1088351},
  ISBN      = {1-59593-071-X},
  MONTH     = {09},
  PAGES     = {17–26},
  PUBLISHER = {ACM Press},
  TITLE     = {Haskell ready to Dazzle the real world},
  URL       = {http://www.cs.uu.nl/dazzle},
  VENUE     = {Tallinn, Estonia},
  YEAR      = {2005},
}

@INCOLLECTION{Seidenfeld-2000-reasonsforFA,
  AUTHOR    = {Teddy Seidenfeld},
  BOOKTITLE = {Probability theory: philosophy, recent history and relations to science},
  EDITOR    = {Vincent F. Hendricks and Stig Andur Pedersen and Klaus Frovin Jørgensen},
  KEYWORDS  = {improper regular conditional distributions},
  PUBLISHER = {Kluwer Academic Publishers},
  SERIES    = {Synthese Library},
  TITLE     = {Remarks on the theory of conditional probability: some issues of finite versus countable additivity},
  VOLUME    = {297},
  YEAR      = {2001},
}

@ARTICLE{Seidenfeld-1985,
  ABSTRACT     = {Can there be good reasons for judging one set of probabilistic assertions more reliable than a second? There are many candidates for measuring "goodness" of probabilistic forecasts. Here, I focus on one such aspirant: calibration. Calibration requires an alignment of announced probabilities and observed relative frequency, e.g., 50 percent of forecasts made with the announced probability of .5 occur, 70 percent of forecasts made with probability .7 occur, etc. To summarize the conclusions: (i) Surveys designed to display calibration curves, from which a recalibration is to be calculated, are useless without due consideration for the interconnections between questions (forecasts) in the survey. (ii) Subject to feedback, calibration in the long run is otiose. It gives no ground for validating one coherent opinion over another as each coherent forecaster is (almost) sure of his own long-run calibration. (iii) Calibration in the short run is an inducement to hedge forecasts. A calibration score, in the short run, is improper. It gives the forecaster reason to feign violation of total evidence by enticing him to use the more predictable frequencies in a larger finite reference class than that directly relevant.},
  AUTHOR       = {Teddy Seidenfeld},
  JOURNALTITLE = {Philosophy of Science},
  LOCALFILE    = {article/Seidenfeld-1985.pdf},
  NUMBER       = {2},
  PAGES        = {274–294},
  TITLE        = {Calibration, coherence, and scoring rules},
  URL          = {http://www.jstor.org/stable/187511},
  VOLUME       = {52},
  YEAR         = {1985},
}

@INCOLLECTION{Seidenfeld-Schervish-Kadane-1990-decwoord,
  AUTHOR    = {Teddy Seidenfeld and Mark J. Schervish and Joseph B. Kadane},
  BOOKTITLE = {Acting and Reflecting: The Interdisciplinary Turn in Philosophy},
  EDITOR    = {Wilfried Sieg},
  LOCALFILE = {incollection/Seidenfeld-Shervish-Kadane-1990-decwoord.pdf},
  LOCATION  = {Dordrecht},
  PAGES     = {143–170},
  PUBLISHER = {Kluwer Academic Publishers},
  SERIES    = {Synthese Library},
  TITLE     = {Decisions without ordering},
  VOLUME    = {211},
  YEAR      = {1990},
}

@ARTICLE{Seidenfeld-Schervish-Kadane-2010,
  AUTHOR       = {Teddy Seidenfeld and Mark J. Schervish and Joseph B. Kadane},
  DOI          = {10.1007/s11229-009-9470-7},
  ISSN         = {0039-7857},
  JOURNALTITLE = {Synthese},
  KEYWORDS     = {Choice functions; Coherence; Γ-Maximin; Maximality; Uncertainty; State-independent utility},
  LOCALFILE    = {article/Seidenfeld-Schervish-Kadane-2010.pdf},
  PAGES        = {157–176},
  PUBLISHER    = {Springer Netherlands},
  TITLE        = {Coherent choice functions under uncertainty},
  VOLUME       = {172},
  YEAR         = {2010},
}

@ARTICLE{Seidenfeld-Schervish-Kadane-1995-preference,
  AUTHOR       = {Teddy Seidenfeld and Mark J. Schervish and Joseph B. Kadane},
  DOI          = {10.1214/aos/1034713653},
  JOURNALTITLE = {The Annals of Statistics},
  KEYWORDS     = {Axioms of decision theory; partial order; robust},
  LOCALFILE    = {article/Seidenfeld-Schervish-Kadane-1995-preference.pdf},
  MONTH        = {12},
  NUMBER       = {6},
  PAGES        = {2168–2217},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {A representation of partially ordered preferences},
  VOLUME       = {23},
  YEAR         = {1995},
}

@ARTICLE{Seidenfeld-Wasserman-1993,
  ABSTRACT     = {Suppose that a probability measure P is known to lie in a set of probability measures M. Upper and lower bounds on the probability of any event may then be computed. Sometimes, the bounds on the probability of an event A conditional on an event B may strictly contain the bounds on the unconditional probability of A. Surprisingly, this might happen for every B in a partition \mathscr{B}. If so, we say that dilation has occurred. In addition to being an interesting statistical curiosity, this counterintuitive phenomenon has important implications in robust Bayesian inference and in the theory of upper and lower probabilities. We investigate conditions under which dilation occurs and we study some of its implications. We characterize dilation immune neighborhoods of the uniform measure.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Teddy Seidenfeld and Larry Wasserman},
  DOI          = {10.1214/aos/1176349254},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Seidenfeld-Wasserman-1993.pdf},
  NUMBER       = {3},
  PAGES        = {1139–1154},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {Dilation for sets of probabilities},
  VOLUME       = {21},
  YEAR         = {1993},
}

@INCOLLECTION{2007-Sent+Gaag,
  AUTHOR    = {Danielle Sent and Linda C. van der Gaag},
  BOOKTITLE = {Artificial Intelligence in Medicine},
  DOI       = {10.1007/978-3-540-73599-1_44},
  EDITOR    = {Riccardo Bellazzi and Ameen Abu-Hanna and Jim Hunter},
  ISBN      = {978-3-540-73598-4},
  KEYWORDS  = {diagnostic test selection; probabilistic networks; semi-myopia},
  LOCALFILE = {inproceedings/2007-Sent+Gaag.pdf},
  PAGES     = {331–335},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Enhancing automated test selection in probabilistic networks},
  VOLUME    = {4594},
  YEAR      = {2007},
}

@INCOLLECTION{Sent+Van_der_Gaag-2007,
  AUTHOR    = {Danielle Sent and Linda C. van der Gaag},
  BOOKTITLE = {Artificial Intelligence in Medicine},
  DOI       = {10.1007/978-3-540-73599-1_42},
  EDITOR    = {Riccardo Bellazzi and Ameen Abu-Hanna and Jim Hunter},
  ISBN      = {978-3-540-73598-4},
  KEYWORDS  = {Shannon entropy; Gini index; misclassification error; test selection},
  LOCALFILE = {inproceedings/Sent+Van_der_Gaag-2007.pdf},
  PAGES     = {316–325},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {On the behaviour of information measures for test selection},
  VOLUME    = {4594},
  YEAR      = {2007},
}

@ARTICLE{Sent-etal-2005,
  AUTHOR       = {D. Sent and Linda C. van der Gaag and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
  JOURNALTITLE = {Interdisciplinary Journal of Artificial Intelligence and the Simulation of Behaviour},
  NUMBER       = {6},
  PAGES        = {543–561},
  TITLE        = {Eliciting test-selection strategies for a decision-support system in oncology},
  VOLUME       = {1},
  YEAR         = {2005},
}

@BOOK{Shafer-1976,
  AUTHOR    = {Glenn Shafer},
  PUBLISHER = {Princeton University Press},
  TITLE     = {A mathematical theory of evidence},
  YEAR      = {1976},
}

@ARTICLE{Shannon-1948,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Claude E. Shannon},
  JOURNALTITLE = {The Bell System Technical Journal},
  LOCALFILE    = {article/Shannon-1948.pdf},
  PAGES        = {379–423,623–656},
  TITLE        = {A Mathematical Theory of Communication},
  URL          = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
  VOLUME       = {27},
  YEAR         = {1948},
}

@INCOLLECTION{Shapley-1964-twoperson,
  AUTHOR    = {Lloyd S. Shapley},
  BOOKTITLE = {Advances in game theory},
  EDITOR    = {D. Dresher and L. S. Shapley and A. W. Tucker},
  PUBLISHER = {Princeton University Press},
  TITLE     = {Some topics in two-person games},
  YEAR      = {1964},
}

@ARTICLE{Shapley-1971,
  ABSTRACT     = {The core of an n-person game is the set of feasible outcomes that cannot be improved upon by any coalition of players. A convex game is defined as one that is based on a convex set function. In this paper it is shown that the core of a convex game is not empty and that it has an especially regular structure. It is further shown that certain other cooperative solution concepts are related in a simple way to the core: The value of a convex game is the center of gravity of the extreme points of the core, and the von Neumann-Morgenstern stable set solution of a convex game is unique and coincides with the core.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Lloyd S. Shapley},
  DOI          = {10.1007/BF01753431},
  JOURNALTITLE = {International Journal of Game Theory},
  LOCALFILE    = {article/Shapley-1971.pdf},
  NUMBER       = {1},
  PAGES        = {11–26},
  PUBLISHER    = {Springer},
  TITLE        = {Cores of convex games},
  VOLUME       = {1},
  YEAR         = {1971},
}

@INCOLLECTION{Shenoy-Shafer-2008-axioms,
  ABSTRACT  = {In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible. The primitive objects of the framework are variables and valuations. The primitive operators of the framework are combination and marginalization. These operate on valuations. We state three axioms for these operators and we derive the possibility of local computation from the axioms. Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree. Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework.},
  AUTHOR    = {Prakash Shenoy and Glenn Shafer},
  BOOKTITLE = {Classic Works of the Dempster-Shafer Theory of Belief Functions},
  DOI       = {10.1007/978-3-540-44792-4_20},
  EDITOR    = {Ronald R. Yager and Liping Liu},
  PAGES     = {499–528},
  PUBLISHER = {Springer},
  SERIES    = {Studies in Fuzziness and Soft Computing},
  TITLE     = {Axioms for Probability and Belief-Function Propagation},
  VOLUME    = {219},
  YEAR      = {2008},
}

@ARTICLE{Shimony-1955,
  AUTHOR       = {Abner Shimony},
  ISSN         = {0022-4812},
  JOURNALTITLE = {The Journal of Symbolic Logic},
  LOCALFILE    = {article/Shimony-1955.pdf},
  NUMBER       = {1},
  PAGES        = {1–28},
  PUBLISHER    = {Association for Symbolic Logic},
  TITLE        = {Coherence and the Axioms of Confirmation},
  URL          = {http://www.jstor.org/stable/2268039},
  VOLUME       = {20},
  YEAR         = {1955},
}

@ARTICLE{Sine-1990-Perron-Frobenius,
  ABSTRACT     = {If T is a nonexpansive map on a domain in a finite-dimensional sup-norm space then there is a universal bound on the periods of periodic points. This yields the same result for T nonexpansive on a domain in a finite-dimensional Banach space which has a polyhedral unit ball. Similar results are obtained for certain nonexpansive maps defined on all of an infinite-dimensional L\_p space with 1<p<∞.},
  AUTHOR       = {Robert Sine},
  JOURNALTITLE = {Proceedings of the American Mathematical Society},
  LOCALFILE    = {article/Sine-1990-Perron-Frobenius.pdf},
  NUMBER       = {2},
  PAGES        = {331–336},
  TITLE        = {A nonlinear Perron-Frobenius theorem},
  URL          = {http://www.ams.org/proc/1990-109-02/S0002-9939-1990-0948156-X/S0002-9939-1990-0948156-X.pdf},
  VOLUME       = {109},
  YEAR         = {1990},
}

@ARTICLE{Skulj-2009-impmarkov,
  ABSTRACT     = {The parameters of Markov chain models are often not known precisely. Instead of ignoring this problem, a better way to cope with it is to incorporate the imprecision into the models. This has become possible with the development of models of imprecise probabilities, such as the interval probability model. In this paper we discuss some modelling approaches which range from simple probability intervals to the general interval probability models and further to the models allowing completely general convex sets of probabilities. The basic idea is that precisely known initial distributions and transition matrices are replaced by imprecise ones, which effectively means that sets of possible candidates are considered. Consequently, sets of possible results are obtained and represented using similar imprecise probability models. We first set up the model and then show how to perform calculations of the distributions corresponding to the consecutive steps of a Markov chain. We present several approaches to such calculations and compare them with respect to the accuracy of the results. Next we consider a generalisation of the concept of regularity and study the convergence of regular imprecise Markov chains. We also give some numerical examples to compare different approaches to calculations of the sets of probabilities.},
  AUTHOR       = {Damjan Škulj},
  DOI          = {10.1016/j.ijar.2009.06.007},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Imprecise Markov chains; Imprecise probabilities; Interval probabilities; Markov chains; Regularity},
  LOCALFILE    = {article/Skulj-2009-impmarkov.pdf},
  NUMBER       = {8},
  PAGES        = {1314–1329},
  TITLE        = {Discrete time Markov chains with interval probabilities},
  VOLUME       = {50},
  YEAR         = {2009},
}

@INPROCEEDINGS{Skulj-2007-Markov,
  ABSTRACT = {In Markov chain theory a stochastic matrix P is regular if some matrix power P^n contains only strictly positive elements. Regularity of transition matrix of a Markov chain guarantees the existence of a unique invariant distribution which is also the limiting distribution. In the present paper a similar result is shown for the generalized Markov chain model that replaces classical probabilities with interval probabilities. We generalize the concept of regularity and show that for a regular interval transition matrix sets of probabilities corresponding to consecutive steps of a Markov chain converge to a unique limiting set of distributions that only depends on transition matrix and is independent of the initial distribution. A similar convergence result is also shown for approximations of the invariant set.},
  AUTHOR   = {Damjan Škulj},
  KEYWORDS = {Markov chain; interval probability},
  PAGES    = {405–414},
  TITLE    = {Regular finite Markov chains with interval probabilities},
  YEAR     = {2007},
}

@ARTICLE{Skyrms-1993,
  ABSTRACT     = {Maher (1992b) advances an objection to dynamic Dutch-book arguments, partly inspired by the discussion in Levi (1987; in particular by Levi's case 2, p. 204). Informally, the objection is that the decision maker will "see the dutch book coming" and consequently refuse to bet, thus escaping the Dutch book. Maher makes this explicit by modeling the decision maker's choices as a sequential decision problem. On this basis he claims that there is a mistake in dynamic coherence arguments. There is really no formal mistake in classical dynamic coherence arguments, but the discussions in Maher and Levi do suggest interesting ways in which the definition of dynamic coherence might be strengthened. Such a strengthened "sequentialized" notion of dynamic coherence is explored here. It so happens that even on the strengthened standards for a Dutch book, the classic dynamic coherence argument for conditioning still goes through.},
  AUTHOR       = {Brian Skyrms},
  ISSN         = {0031-8248},
  JOURNALTITLE = {Philosophy of Science},
  LOCALFILE    = {article/Skyrms-1993.pdf},
  NUMBER       = {2},
  PAGES        = {320–328},
  PUBLISHER    = {The University of Chicago Press on behalf of the Philosophy of Science Association},
  TITLE        = {A mistake in dynamic coherence arguments?},
  URL          = {http://www.jstor.org/stable/188357},
  VOLUME       = {60},
  YEAR         = {1993},
}

@ARTICLE{Smith-1961,
  ABSTRACT     = {It is suggested that the strength of a person's beliefs may be tested by finding at what odds he is prepared to bet on them. This leads to a system of numerical "medial personal probabilities" obeying the classical laws of probability. However, these do not have precisely defined values, but are contained within specified intervals. The appropriate method of inference is Bayes's Theorem. This leads to generally accepted statistical procedures in large samples, except that the "weight of evidence" and not significance level is the measure of conviction in a significance test. Under very general conditions decisions are made by maximizing expected utility.},
  ANNOTATION   = {geannoteerde kopie op papier},
  AUTHOR       = {Cedric A. B. Smith},
  ISSN         = {0035-9246},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Smith-1961.pdf},
  NUMBER       = {1},
  PAGES        = {1–37},
  PUBLISHER    = {Blackwell Publishing for the Royal Statistical Society},
  TITLE        = {Consistency in statistical inference and decision},
  URL          = {http://www.jstor.org/stable/2983842},
  VOLUME       = {23},
  YEAR         = {1961},
}

@ARTICLE{Smithson-Segale-2009,
  ABSTRACT     = {On grounds of insufficient reason, a probability of 1/K is assigned to K mutually exclusive possible events when nothing is known about the likelihood of those events. Fox and Rottenstreich (2003) present evidence that subjective probability judgments are typically biased towards this ignorance prior, and therefore depend on the partition K. Results from two studies indicate that lower-upper (imprecise) probability judgments by naive judges also exhibit partition dependence, despite the potential that imprecise probabilities provide for avoiding it. However, beta regression reveals two kinds of priming effects, one of which is modeled by mixture distributions. Another novel finding suggests that when partition primes conflict with a normatively correct partition some judges widen their probability intervals to encompass both partitions. The results indicate that imprecise probability judgments may be better suited than precise probabilities for handling conflicting or ambiguous information about partitions.},
  AUTHOR       = {Michael Smithson and Carl Segale},
  DOI          = {10.1080/15598608.2009.10411918},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Imprecise probability; Judgment; Partition; Subjective probability},
  LOCALFILE    = {article/Smithson-Segale-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {169–181},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Partition Priming in Judgments of Imprecise Probabilities},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Soyster-1973,
  ABSTRACT     = {This note formulates a convex mathematical programming problem in which the usual definition of the feasible region is replaced by a significantly different strategy. Instead of specifying the feasible region by a set of convex inequalities, fi(x)≤ bi, i=1,2,⋯,m, the feasible region is defined via set containment. Here n convex activity sets {Kj, j=1,2,⋯,n} and a convex resource set K are specified and the feasible region is given by X={x ∈ R^{n}|x\_{1}K\_{1}+x\_{2}K\_{2}+⋯ +x\_{n}K\_{n}\subseteq K,\ x\_{j}\geq 0}, where the binary operation + refers to addition of sets. The problem is then to find x∈ X that maximizes the linear function c· x. When the resource set has a special form, this problem is solved via an auxiliary linear-programming problem and application to inexact linear programming is possible.},
  AUTHOR       = {A. L. Soyster},
  ISSN         = {0030-364X},
  JOURNALTITLE = {Operations Research},
  LOCALFILE    = {article/Soyster-1973.pdf},
  NUMBER       = {5},
  PAGES        = {1154–1157},
  PUBLISHER    = {INFORMS},
  TITLE        = {Convex programming with set-inclusive constraints and applications to inexact linear programming},
  URL          = {http://www.jstor.org/stable/168933},
  VOLUME       = {21},
  YEAR         = {1973},
}

@ARTICLE{Steeneveld+al-2010,
  ABSTRACT     = {Based on sensor measurements, an automatic milking system (AMS) generates mastitis alert lists indicating cows which are likely to have clinical mastitis (CM). Because of the general assumption of equal probabilities of developing CM for all cows, all alerts on the list have the same success rate. As a consequence, it is not possible to rank-order the alerts in terms of their likelihood of CM. In practice, the performance of a CM detection system is not only based on the sensitivity (SN) and specificity (SP) of the system, but is also influenced by the prior probability of a cow having CM. This study illustrates the idea of using cow-specific prior probabilities of CM, based on non-AMS information, to provide a rank-order on the alerts from an AMS. A tree-augmented naive Bayesian network was trained from available data to determine these cow-specific prior probabilities for CM. The graphical structure of the network and the probability tables for its variables in the network were based on data from 274 Dutch dairy herds that recorded each case of CM over an 18-month period. The final data set contained information on a total of 5363 CM cases derived from 28,137 lactations and 22,860 cows. The available prior cow information (parity, days in milk, season of the year, somatic cell count history and CM history) was included as variables in the network. By combining the cow-specific prior probabilities of CM with the SN and SP of the detection system of the AMS, the computed success rates can be used to discriminate between CM alerts. Our illustrations indicate that the success rate might range from 3 to 84\%, while assuming an equal overall probability would result in a success rate of 21\%. Using the computed success rates, the CM alerts on an alert list can be rank-ordered, thereby providing the dairy farmer information about which cows have the highest priority for visual inspection for CM.},
  AUTHOR       = {Wilma Steeneveld and Linda C. van der Gaag and H. W. Barkema and Henk Hogeveen},
  DOI          = {10.1016/j.compag.2009.12.011},
  ISSN         = {0168-1699},
  JOURNALTITLE = {Computers and Electronics in Agriculture},
  KEYWORDS     = {Clinical mastitis; Automatic milking systems; Detection},
  LOCALFILE    = {article/Steeneveld+al-2010.pdf},
  NUMBER       = {1},
  PAGES        = {50–56},
  TITLE        = {Simplify the interpretation of alert lists for clinical mastitis in automatic milking systems},
  VOLUME       = {71},
  YEAR         = {2010},
}

@ARTICLE{Steeneveld+al-2009,
  ABSTRACT     = {Clinical mastitis (CM) can be caused by a wide variety of pathogens and farmers must start treatment before the actual causal pathogen is known. By providing a probability distribution for the causal pathogen, naive Bayesian networks (NBN) can serve as a management tool for farmers to decide which treatment to use. The advantage of providing a probability distribution for the causal pathogen, rather than only providing the most likely causal pathogen, is that the uncertainty involved is visible and a more informed treatment decision can be made. The objective of this study was to illustrate provision of probability distributions for the gram status and for the causal pathogen for CM cases. For constructing the NBN, data were used from 274 Dutch dairy herds in which the occurrence of CM was recorded over an 18-mo period. The data set contained information on 3,833 CM cases. Two-thirds of the data set was used for the construction process and one-third was retained for validation. One NBN was constructed with the CM cases classified according to their gram status, and another was built with the CM cases classified into streptococci, Staphylococcus aureus, or Escherichia coli. Information usually available at a dairy farm was included in both NBN (parity, month in lactation, season of the year, quarter position, SCC and CM history, being sick or not, and color and texture of the milk). Accuracy was calculated to obtain insight in the quality of the constructed NBN. The accuracy of classifying CM cases into gram-positive or gram-negative pathogens was 73\%, while the accuracy of classifying CM cases into streptococci, Staph. aureus, or E. coli was 52\%. Because only CM cases with a high probability for a single causal pathogen will be considered for pathogen-specific treatment, accuracies based on only classifying CM cases above a particular probability threshold were determined. For instance, for CM cases in which either gram-negative or gram-positive had a probability >0.90, classification according to the gram status reached an accuracy of 97\%. We found that the greater the probability for a particular pathogen was for a CM case, the more accurate was the classification of this case as being caused by this pathogen. The probability distributions provided by the NBN and the associated accuracies for varying classification thresholds provide the farmer with considerable insight about the most likely causal pathogen for a CM case and the uncertainty involved.},
  AUTHOR       = {Wilma Steeneveld and Linda C. van der Gaag and H. W. Barkema and Henk Hogeveen},
  DOI          = {10.3168/jds.2008-1694},
  JOURNALTITLE = {Journal of Dairy Science},
  LOCALFILE    = {article/Steeneveld+al-2009.pdf},
  MONTH        = {6},
  NUMBER       = {6},
  PAGES        = {2598–2509},
  TITLE        = {Providing probability distributions for the causal pathogen of clinical mastitis using naive Bayesian networks},
  VOLUME       = {92},
  YEAR         = {2009},
}

@ARTICLE{Steeneveld-etal-2010,
  ABSTRACT     = {Automatic milking systems (AMS) generate alert lists reporting cows likely to have clinical mastitis (CM). Dutch farmers indicated that they use non-AMS cow information or the detailed alert information from the AMS to decide whether to check an alerted cow for CM. However, it is not yet known to what extent such information can be used to discriminate between true-positive and false-positive alerts. The overall objective was to investigate whether selection of the alerted cows that need further investigation for CM can be made. For this purpose, non-AMS cow information and detailed alert information were used. During a 2-yr study period, 11,156 alerts for CM, including 159 true-positive alerts, were collected at one farm in the Netherlands. Non-AMS cow information on parity, days in milk, season of the year, somatic cell count history, and CM history was added to each alert. In addition, 6 alert information variables were defined. These were the height of electrical conductivity, the alert origin (electrical conductivity, color, or both), whether or not a color alert for mastitic milk was given, whether or not a color alert for abnormal milk was given, deviation from the expected milk yield, and the number of alerts of the cow in the preceding 12 to 96 h. Subsequently, naive Bayesian networks (NBN) were constructed to compute the posterior probability of an alert being truly positive based only on non-AMS cow information, based on only alert information, or based on both types of information. The NBN including both types of information had the highest area under the receiver operating characteristic curve (AUC; 0.78), followed by the NBN including only alert information (AUC = 0.75) and the NBN including only non-AMS cow information (AUC = 0.62). By combining the 2 types of information and by setting a threshold on the computed probabilities, the number of false-positive alerts on a mastitis alert list was reduced by 35\%, and 10\% of the true-positive alerts would not be identified. To detect CM cases at a farm with an AMS, checking all alerts is still the best option but would result in a high workload. Checking alerts based on a single alert information variable would result in missing too many true-positive cases. Using a combination of alert information variables, however, is the best way to select cows that need further investigation. The effect of adding non-AMS cow information on making a distinction between true-positive and false-positive alerts would be minor.},
  AUTHOR       = {Wilma Steeneveld and Linda C. van der Gaag and W. Ouweltjes and H. Mollenhorst and Henk Hogeveen},
  DOI          = {10.3168/jds.2009-3020},
  ISSN         = {0022-0302},
  JOURNALTITLE = {Journal of Dairy Science},
  KEYWORDS     = {clinical mastitis; detection; automatic milking; dairy cow},
  NUMBER       = {6},
  PAGES        = {2559–2568},
  TITLE        = {Discriminating between true-positive and false-positive clinical mastitis alerts from automatic milking systems},
  VOLUME       = {93},
  YEAR         = {2010},
}

@ARTICLE{Steuer-1981,
  ABSTRACT     = {This paper presents three algorithms for solving linear programming problems in which some or all of the objective function coefficients are specified in terms of intervals. Which algorithm is applicable depends upon (a) the number of interval objective function coefficients, (b) the number of nonzero objective function coefficients, and (c) whether or not the feasible region is bounded. The algorithms output all extreme points and unbounded edge directions that are "multiparametrically optimal" with respect to the ranges placed on the objective function coefficients. The algorithms are most suitable to linear programs in which the objective function coefficients are deterministic but are likely to vary from time period to time period (as for example in blending problems).},
  AUTHOR       = {Ralph E. Steuer},
  ISSN         = {0364-765X},
  JOURNALTITLE = {Mathematics of Operations Research},
  LOCALFILE    = {article/Steuer-1981.pdf},
  NUMBER       = {3},
  PAGES        = {333–348},
  PUBLISHER    = {INFORMS},
  TITLE        = {Algorithms for linear programming problems with interval objective function coefficients},
  URL          = {http://www.jstor.org/stable/3689177},
  VOLUME       = {6},
  YEAR         = {1981},
}

@ARTICLE{Stoye-2009,
  ABSTRACT     = {This paper applies recently developed methods to robust assessment of treatment outcomes and robust treatment choice based on nonexperimental data. The substantive question is whether young offenders should be assigned to residential or nonresidential treatment in order to prevent subsequent recidivism. A large data set on past offenders exists, but treatment assignment was by judges and not by experimenters, hence counterfactual outcomes are not identified unless one imposes strong assumptions. The analysis is carried out in two steps. First, I show how to compute identified bounds on expected outcomes under various assumptions that are too weak to restore conventional identification but may be accordingly credible. The bounds are estimated, and confidence regions that take current theoretical developments into account are computed. I then ask which treatment to assign to future offenders if the identity of the best treatment will not be learned from the data. This is a decision problem under ambiguity. I characterize and compute decision rules that are asymptotically efficient under the minimax regret criterion. The substantive conclusion is that both bounds and recommended decisions vary significantly across the assumptions. The data alone do not permit conclusions or decisions that are globally robust in the sense of holding uniformly over reasonable assumptions.},
  AUTHOR       = {Jörg Stoye},
  DOI          = {10.1080/15598608.2009.10411923},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bounds; Minimax regret; Partial identification; Statistical decision rules; Treatment choice; Treatment evaluation},
  LOCALFILE    = {article/Stoye-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {239–254},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Partial Identification and Robust Treatment Choice: An Application to Young Offenders},
  VOLUME       = {3},
  YEAR         = {2009},
}

@INPROCEEDINGS{Strens-2000,
  AUTHOR    = {Malcolm Strens},
  BOOKTITLE = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
  TITLE     = {A Bayesian Framework for Reinforcement Learning},
  YEAR      = {2000},
}

@ARTICLE{Strijbosch-vanDoorne-Selen-1991,
  ABSTRACT     = {A number of MOLP-algorithms have been developed to establish the set of non-dominated solutions, using a number of different approaches and theorems that may be non-trivial to the non-expert user. This article presents a simplified MOLP-algorithm (MOLP-S), based on a straightforward extension of the simplex-method of linear programming, to trace out the set of non-dominated solutions. The proposed methodology exhibits computational characteristics that may render the method more efficient as compared to other algorithms currently in use. The proposed method is tested on a number of problems from the literature which exhibit varying degrees of complexity.},
  AUTHOR       = {Leo W. G. Strijbosch and Arno G.M. van Doorne and Willem J. Selen},
  DOI          = {10.1016/0305-0548(91)90008-F},
  ISSN         = {0305-0548},
  JOURNALTITLE = {Computers \& Operations Research},
  LOCALFILE    = {article/Strijbosch-vanDoorne-Selen-1991.pdf},
  NUMBER       = {8},
  PAGES        = {709–716},
  TITLE        = {A simplified MOLP algorithm: The MOLP-S procedure},
  VOLUME       = {18},
  YEAR         = {1991},
}

@THESIS{Strobl-2008,
  AUTHOR      = {Carolin Strobl},
  INSTITUTION = {Institut für Statistik, Fakultät für Mathematik, Informatik und Statistik, Ludwig-Maximilians-Universität München},
  TITLE       = {Statistical Issues in Machine Learning – Towards Reliable Split Selection and Variable Importance Measures},
  TYPE        = {phdthesis},
  YEAR        = {2008},
}

@ARTICLE{Strobl-Augustin-2009,
  ABSTRACT     = {Classification and regression trees are a popular and easy to interpret non-parametric regression approach, but are known to be very instable: Small changes in the learning sample can produce completely different trees. Therefore recently it has become state-of-the-art to consider ensembles (i.e. sets) of trees. The present paper contributes to the so-called TWIX approach, which produces ensembles by extra splits in additional cutpoints. This approach can be considered as a compromise between the interpretable but instable single tree models and the stable but no longer interpretable ensemble methods bagging and random forests. Based on the idea to study the sensitivity of a split to some virtual, yet unseen observations, we develop a new, data driven, cutpoint selection criterion, that technically turns out to be closely related to an upper entropy approach based on an Imprecise Dirichlet Model. Our criterion combines several attractive features: By adding extra cutpoints only iff the underlying cutpoint is instable, the tree is robustified parsimoniously and the computational expense of the resulting TWIX ensemble is reduced considerably. As a welcome by-product we moreover obtain a vivid diagnostic measure for the robustness of a single tree model. The rationale and benefit of our new adaptive criterion are illustrated by means of a small data example and a simulation study. Credal classification rules for robust aggregated predictions from sets of trees are briefly sketched in an outlook.},
  AUTHOR       = {Carolin Strobl and Thomas Augustin},
  DOI          = {10.1080/15598608.2009.10411915},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Aggregation; Bagging; C4.5; Classification trees; Credal classification; CART; Cutpoint selection; Imprecise Dirichlet Model; Gini index; Random forests; Shannon entropy; TWIX},
  LOCALFILE    = {article/Strobl-Augustin-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {119–135},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Adaptive Selection of Extra Cutpoints—Towards Reconciling Robustness and Interpretability in Classification Trees},
  VOLUME       = {3},
  YEAR         = {2009},
}

@ARTICLE{Sundberg-Wagner-1992,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl Sundberg and Carl G. Wagner},
  JOURNALTITLE = {Advances in Applied Mathematics of Operations Research},
  PAGES        = {262–272},
  TITLE        = {Generalized Differences and Bayesian Conditioning of Choquet Capacities},
  VOLUME       = {13},
  YEAR         = {1992},
}

@ARTICLE{Sundberg-Wagner-1990-capacities,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl Sundberg and Carl G. Wagner},
  JOURNALTITLE = {Journal of Theoretical Probability},
  NUMBER       = {1},
  PAGES        = {159–167},
  TITLE        = {Characterizations of Monotone and 2-Monotone Capacities},
  VOLUME       = {5},
  YEAR         = {1990},
}

@MISC{Suppes-2003,
  AUTHOR = {Patrick Suppes},
  TITLE  = {Nonmonotonic Upper Probabilities and Quantum Entanglement},
  YEAR   = {2003},
}

@ARTICLE{Suppes-1974,
  ABSTRACT     = {This paper criticizes some of the claims of the standard theories of subjective probability. The criticisms are especially oriented toward the structural axioms that cannot be regarded as axioms of pure rationality and the general results that yield exact measurement of subjective probabilities. Qualitative axioms for upper and lower probability are introduced to provide a theory of inexact measurement of subjective probability. Only minor modifications of de Finetti's qualitative axioms yield the desired theory. The paper concludes with a comparison of the measurement of belief to the measurement results characteristic of Euclidean geometry, and also examines briefly some possibilities for using learning models as simplified abstract processes for constructing belief.},
  AUTHOR       = {Patrick Suppes},
  ISSN         = {0035-9246},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  KEYWORDS     = {belief; lower probability; measurement; upper probability},
  LOCALFILE    = {article/Suppes-1974.pdf},
  NUMBER       = {2},
  PAGES        = {160–191},
  TITLE        = {The measurement of belief},
  URL          = {http://www.jstor.org/stable/2984811},
  VOLUME       = {36},
  YEAR         = {1974},
}

@ARTICLE{Szpilrajn-1930,
  AUTHOR       = {Edward Szpilrajn},
  JOURNALTITLE = {Fundamenta Mathematicae},
  LOCALFILE    = {article/Szpilrajn-1930.pdf},
  NUMBER       = {1},
  PAGES        = {386–389},
  PUBLISHER    = {Institute of Mathematics Polish Academy of Sciences},
  TITLE        = {Sur l'extension de l'ordre partiel},
  URL          = {http://eudml.org/doc/212499},
  VOLUME       = {16},
  YEAR         = {1930},
}

@INPROCEEDINGS{TabachneckSchijf-etal-2008,
  AUTHOR    = {Hermina J. M. Tabachneck-Schijf and Linda C. van der Gaag and Petra L. Geenen and Martijn M. Schrage and Willie L. A. Loeffen and Armin R. W. Elbers},
  BOOKTITLE = {Proceedings of the 20th International Pig Veterinary Science Congress},
  EDITOR    = {P. Evans},
  TITLE     = {Designing a personal digital assistant for early on-site detection of classical swine fever in a pig unit},
  VENUE     = {Durban, South Africa},
  YEAR      = {2008},
}

@ARTICLE{Tanaka-Okuda-Asai-1973,
  ABSTRACT     = {In problems of system analysis, it is customary to treat imprecision by the use of probability theory. It is becoming increasingly clear, however, that in the case of many real world problems involving large scale systems such as economic systems, social systems, mass service systems, etc., the major source of imprecision should more properly be labeled 'fuzziness' rather than 'randomness.' By fuzziness, we mean the type of imprecision which is associated with the lack of sharp transition from membership to nonmembership, as in tall men, small numbers, likely events, etc. In this paper our main concern is with the application of the theory of fuzzy sets to decision problems involving fuzzy goals and strategies, etc., as defined by R. E. Bellman and L. A. Zadeh [1]. However, in our approach, the emphasis is on mathematical programming and the use of the concept of a level set to extend some of the classical results to problems involving fuzzy constraints and objective functions.},
  AUTHOR       = {Hideo Tanaka and Tetsuji Okuda and Kiyoji Asai},
  DOI          = {10.1080/01969727308545912},
  JOURNALTITLE = {Cybernetics and Systems: An International Journal},
  NUMBER       = {4},
  PAGES        = {37–46},
  TITLE        = {On Fuzzy-Mathematical Programming},
  VOLUME       = {3},
  YEAR         = {1973},
}

@ARTICLE{Tanaka-1993,
  AUTHOR       = {Yoshihiro Tanaka},
  EPRINT       = {2115/30499},
  EPRINTTYPE   = {hdl},
  JOURNALTITLE = {Economic Journal of Hokkaido University},
  LOCALFILE    = {article/Tanaka-1993.pdf},
  PAGES        = {159–166},
  TITLE        = {On Convexity of A System of Linear Interval Equations},
  URL          = {http://hdl.handle.net/2115/30499},
  VOLUME       = {22},
  YEAR         = {1993},
}

@ARTICLE{Tessem,
  ABSTRACT     = {Belief networks are tried as a method for propagation of singleton interval probabilities. A convex polytope representation of the interval probabilities is shown to make the problem intractable even for small parameters. A solution to this is to use the interval bounds directly in computations of the propagation algorithm. The algorithm presented leads to approximative results but has the advantage of being polynomial in time. It is shown that the method gives fairly good results.},
  AUTHOR       = {Bjørnar Tessem},
  DOI          = {10.1016/0888-613X(92)90006-L},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  NUMBER       = {3-4},
  PAGES        = {95–120},
  TITLE        = {Interval probability propagation},
  VOLUME       = {7},
  YEAR         = {1992},
}

@ARTICLE{Tatcher-1964,
  ABSTRACT     = {Given the number of successes in a random sample, prediction limits can be determined for the number which will be observed in a second sample, in a way which does not depend on any assumption or inference about the unknown proportion in the population. Such "confidence limits" for the prediction are found to correspond to Bayesian solutions based on two particular prior distributions, and are related to Laplace's rule of succession. The results suggest a possible type of "prediction strategy".},
  AUTHOR       = {A. R. Thatcher},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Tatcher-1964.pdf},
  NUMBER       = {2},
  PAGES        = {176–210},
  TITLE        = {Relationships between Bayesian and confidence limits for predictions},
  URL          = {http://www.jstor.org/stable/2984417},
  VOLUME       = {26},
  YEAR         = {1964},
}

@REPORT{Troffaes-idmfacts,
  AUTHOR = {Matthias C. M. Troffaes},
  TITLE  = {The imprecise Dirichlet model: facts and formulas},
  TYPE   = {techreport},
}

@ARTICLE{Troffaes-2007-decision,
  ABSTRACT     = {Various ways for decision making with imprecise probabilities—admissibility, maximal expected utility, maximality, E-admissibility, $\Gamma$-maximax, $\Gamma$-maximin, all of which are well known from the literature—are discussed and compared. We generalise a well-known sufficient condition for existence of optimal decisions. A simple numerical example shows how these criteria can work in practice, and demonstrates their differences. Finally, we suggest an efficient approach to calculate optimal decisions under these decision criteria.},
  AUTHOR       = {Matthias C. M. Troffaes},
  DOI          = {10.1016/j.ijar.2006.06.001},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Decision; E-admissibility; Lower prevision; Maximality; Maximin; Optimality; Probability; Uncertainty},
  LOCALFILE    = {article/Troffaes-2007-decision.pdf},
  NUMBER       = {1},
  PAGES        = {17–29},
  TITLE        = {Decision making under uncertainty using imprecise probabilities},
  VOLUME       = {45},
  YEAR         = {2007},
}

@THESIS{Troffaes-2005,
  AUTHOR      = {Matthias C. M. Troffaes},
  INSTITUTION = {Universiteit Gent – Ghent University},
  TITLE       = {Optimaliteit, onzekerheid, en dynamisch programmeren met onderprevisies – Optimality, Uncertainty, and Dynamic Programming with Lower Previsions},
  TYPE        = {phdthesis},
  YEAR        = {2005},
}

@INPROCEEDINGS{Troffaes-2004,
  AUTHOR    = {Matthias C. M. Troffaes},
  BOOKTITLE = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
  PAGES     = {571–578},
  TITLE     = {Efficient and robust global amino acid sequence alignment with uncertain evolutionary distances},
  VOLUME    = {1},
  YEAR      = {2004},
}

@INPROCEEDINGS{Troffaes-Hable-2011,
  AUTHOR    = {Matthias C. M. Troffaes and Robert Hable},
  BOOKTITLE = {ISIPTA '11: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications},
  EDITOR    = {Frank P. A. Coolen and Gert de Cooman and Thomas Fetz and Michael Oberguggenberger},
  LOCALFILE = {inproceedings/Troffaes-Hable-2011.pdf},
  PAGES     = {361–370},
  PUBLISHER = {SIPTA},
  TITLE     = {Robustness of natural extension},
  URL       = {http://www.sipta.org/isipta11/proceedings/papers/s044.pdf},
  VENUE     = {Innsbruck},
  YEAR      = {2011},
}

@ARTICLE{Trump-Prautzsch-1996,
  ABSTRACT     = {In this paper we present fast algorithms to raise the degree n of a simplicial Bézier representation of degree n to arbitrarily high degree. Each Bézier point of some (n + r)th degree representation can be computed in a simplicial recursive scheme of depth n. In the case of curves the recurrence relation reveals that the (n + r)th degree Bézier polygon can also be obtained by inserting r knots into some nth degree spline which provides a very fast algorithm. Furthermore, a short new proof is given for the fact that the Bézier nets of a multivariate polynomial converge to the polynomial under repeated degree elevation.},
  AUTHOR       = {Wilfried Trump and Hartmut Prautzsch},
  DOI          = {10.1016/0167-8396(95)00031-3},
  JOURNALTITLE = {Computer Aided Geometric Design},
  KEYWORDS     = {Bézier curves; Bézier simplices; Bézier triangles; b-splines; convergence; elevation; knot insertion; pyramidal schemes; repeated degree; simplicial recursions},
  LOCALFILE    = {article/Trump-Prautzsch-1996.pdf},
  NUMBER       = {5},
  PAGES        = {387–398},
  PUBLISHER    = {Elsevier},
  TITLE        = {Arbitrarily high degree elevation of Bézier representations},
  VOLUME       = {13},
  YEAR         = {1996},
}

@ARTICLE{Tukey-1986-sunset,
  AUTHOR       = {John W. Tukey},
  JOURNALTITLE = {The American Statistician},
  LOCALFILE    = {article/Tukey-1986-sunset.pdf},
  NUMBER       = {1},
  PAGES        = {72–76},
  TITLE        = {Sunset salvo},
  URL          = {http://www.jstor.org/stable/2683137},
  VOLUME       = {40},
  YEAR         = {1986},
}

@ARTICLE{Utkin-Augustin-2007,
  ABSTRACT     = {The paper presents an efficient solution to decision problems where direct partial information on the distribution of the states of nature is available, either by observations of previous repetitions of the decision problem or by direct expert judgements. To process this information we use a recent generalization of Walley's imprecise Dirichlet model, allowing us also to handle incomplete observations or imprecise judgements, including missing data. We derive efficient algorithms and discuss properties of the optimal solutions with respect to several criteria, including Gamma-maximinity and E-admissibility. In the case of precise data and pure actions the former surprisingly leads us to a frequency-based variant of the Hodges–Lehmann criterion, which was developed in classical decision theory as a compromise between Bayesian and minimax procedures.},
  AUTHOR       = {Lev V. Utkin and Thomas Augustin},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Belief functions; Coarse data; Decision making; E-admissibility; Imprecise Dirichlet model (IDM); Imprecise probabilities; Incomplete data; Interval probability; Interval statistical models; Missing or set-valued statistical data},
  LOCALFILE    = {article/Utkin-Augustin-2007.pdf},
  NUMBER       = {3},
  PAGES        = {322–338},
  TITLE        = {Decision making under incomplete data using the imprecise Dirichlet model},
  VOLUME       = {44},
  YEAR         = {2007},
}

@ARTICLE{VantVeer-etal-2002,
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Laura J. {van 't Veer} and Hongyue Dai and Marc J. {Van de Vijver} and Yudong D. He and Augustinus A. M. Hart and Mao Mao and Hans L. Peterse and Karin {Van der Kooy} and Matthew J. Marton and Anke T. Witteveen and George J. Schreiber and Ron M. Kerkhoven and Chris Roberts and Peter S. Linsley and René Bernards and Stephen H. Friend},
  DOI          = {10.1038/415530a},
  JOURNALTITLE = {Nature},
  LOCALFILE    = {article/VantVeer-etal-2002.pdf},
  NUMBER       = {6871},
  PAGES        = {530–536},
  PUBLISHER    = {Nature Publishing Group},
  TITLE        = {Gene expression profiling predicts clinical outcome of breastcancer},
  VOLUME       = {415},
  YEAR         = {2002},
}

@INCOLLECTION{VanDorp-Mazzuchi-2003,
  AUTHOR    = {J. René {Van Dorp} and Thomas A. Mazzuchi},
  BOOKTITLE = {Handbook of the Beta Distribution and its Applications},
  EDITOR    = {A. K. Gupta and S. Nadarajah},
  LOCALFILE = {inbook/VanDorp-Mazzuchi-2003.pdf},
  PAGES     = {283–316},
  PUBLISHER = {Marcel Dekker},
  TITLE     = {Parameter Specification of the Beta Distribution and its Dirichlet Extensions Utilizing Quantiles},
  YEAR      = {2003},
}

@INPROCEEDINGS{Verheest-Hellberg-Mace-1998,
  ANNOTATION   = {reprint},
  AUTHOR       = {Frank Verheest and Manfred A. Hellberg and Richard L. Mace},
  BOOKTITLE    = {AIP Conference Proceedings},
  ORGANIZATION = {American Institute of Physics},
  TITLE        = {New aspects of the Jeans instability in dusty plasmas},
  YEAR         = {1998},
}

@ARTICLE{Vicig-Zaffalon-Cozman-2007-notes,
  ABSTRACT     = {These notes comment on Williams' fundamental essay Notes on Conditional Previsions, written as a research report in 1975 and published in the present issue. Basic aspects of that work are discussed, including historical background and relevance to the foundations of probability; examples are supplied to help understanding.},
  AUTHOR       = {Paolo Vicig and Marco Zaffalon and Fabio Gagliardi Cozman},
  DOI          = {10.1016/j.ijar.2006.07.018},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  LOCALFILE    = {article/Vicig-Zaffalon-Cozman-2007-notes.pdf},
  PAGES        = {358–365},
  TITLE        = {Notes on “Notes on conditional previsions”},
  VOLUME       = {44},
  YEAR         = {2007},
}

@INCOLLECTION{2007-Waal+Gaag,
  AUTHOR    = {Peter R. de Waal and Linda C. van der Gaag},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-540-75256-1_45},
  EDITOR    = {Khaled Mellouli},
  ISBN      = {978-3-540-75255-4},
  LOCALFILE = {inproceedings/2007-Waal+Gaag.pdf},
  PAGES     = {501–511},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Inference and learning in multi-dimensional Bayesian network classifiers},
  VOLUME    = {4724},
  YEAR      = {2007},
}

@INCOLLECTION{Wagner-2003-two-dogmas,
  ANNOTATION = {op papier in Wagnerbundel},
  AUTHOR     = {Carl G. Wagner},
  BOOKTITLE  = {The Epistemology of Keith Lehrer},
  CHAPTER    = {9},
  EDITOR     = {E. J. Olsson},
  PAGES      = {143–152},
  PUBLISHER  = {Kluwer Academic Publishers},
  TITLE      = {Two dogmas of probabilism},
  YEAR       = {2003},
}

@ARTICLE{Wagner-2007-Smith-Walley,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  DOI          = {10.1007/s11225-007-9064-7},
  JOURNALTITLE = {Studia Logica},
  LOCALFILE    = {article/Wagner-2007-Smith-Walley.pdf},
  PAGES        = {343–350},
  TITLE        = {The Smith-Walley Interpretation of Subjective Probability: An Appreciation},
  VOLUME       = {86},
  YEAR         = {2007},
}

@ARTICLE{Wagner-2004-modus-tollens,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {British Journal of the Philosophy of Science},
  PAGES        = {747–753},
  TITLE        = {Modus Tollens probabilized},
  VOLUME       = {55},
  YEAR         = {2004},
}

@ARTICLE{Wagner-2003-uniformity-rule,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Erkenntnis},
  PAGES        = {349–364},
  TITLE        = {Commuting probability revisions: the uniformity rule},
  VOLUME       = {59},
  YEAR         = {2003},
}

@ARTICLE{Wagner-2002-probkin+comm,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Philosophy of Science},
  PAGES        = {266–278},
  TITLE        = {Probability Kinematics and Commutativity},
  VOLUME       = {69},
  YEAR         = {2002},
}

@ARTICLE{Wagner-1997-old+new-III,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Philosophy of Science},
  PAGES        = {165–175},
  TITLE        = {Old Evidence and New Explanation},
  VOLUME       = {68},
  YEAR         = {2001},
}

@ARTICLE{Wagner-1997-old+new-II,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Philosophy of Science},
  PAGES        = {283–288},
  TITLE        = {Old Evidence and New Explanation},
  VOLUME       = {66},
  YEAR         = {1999},
}

@ARTICLE{Wagner-1999-two-envelope,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Erkenntnis},
  PAGES        = {233–241},
  TITLE        = {Misadventures in conditional expectation: the two-envelope problem},
  VOLUME       = {51},
  YEAR         = {1999},
}

@ARTICLE{Wagner-1997-old+new,
  ANNOTATION   = {op papier in Wagnerbundel},
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Philosophy of Science},
  PAGES        = {677–691},
  TITLE        = {Old Evidence and New Explanation},
  VOLUME       = {64},
  YEAR         = {1997},
}

@ARTICLE{Wagner-1992-gen-probkin,
  AUTHOR       = {Carl G. Wagner},
  JOURNALTITLE = {Erkenntnis},
  PAGES        = {245–257},
  TITLE        = {Generalized probability Kinematics},
  VOLUME       = {36},
  YEAR         = {1992},
}

@MISC{Walley-0a,
  ANNOTATION = {written notes},
  AUTHOR     = {Peter Walley},
  LOCALFILE  = {misc/Walley-0a.pdf},
  TITLE      = {The theory of natural extension},
}

@ARTICLE{Walley-2002-reconciling,
  ABSTRACT     = {This paper describes the author's research connecting the empirical analysis of treatment response with the normative analysis of treatment choice under ambiguity. Imagine a planner who must choose a treatment rule assigning a treatment to each member of a heterogeneous population of interest. The planner observes certain covariates for each person. Each member of the population has a response function mapping treatments into a real-valued outcome of interest. Suppose that the planner wants to choose a treatment rule that maximizes the population mean outcome. An optimal rule assigns to each member of the population a treatment that maximizes mean outcome conditional on the person's observed covariates. However, identification problems in the empirical analysis of treatment response commonly prevent planners from knowing the conditional mean outcomes associated with alternative treatments; hence planners commonly face problems of treatment choice under ambiguity. The research surveyed here characterizes this ambiguity in practical settings where the planner may be able to bound but not identify the relevant conditional mean outcomes. The statistical problem of treatment choice using finite-sample data is discussed as well.},
  AUTHOR       = {Peter Walley},
  DOI          = {10.1016/S0378-3758(01)00204-X},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Conditional inference; Consistency function; Contamination neighborhood; Foundations of statistics; Frequentist principle; Imprecise Beta model; Imprecise probability; Upper probability},
  LOCALFILE    = {article/Walley-2002-reconciling.pdf},
  PAGES        = {35–65},
  TITLE        = {Reconciling frequentist properties with the likelihood principle},
  VOLUME       = {105},
  YEAR         = {2002},
}

@ARTICLE{Walley-2000-towards,
  AUTHOR       = {Peter Walley},
  DOI          = {10.1016/S0888-613X(00)00031-1},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Choquet capacity; Comparative probability; coherence; credal sets; desirable gambles; foundations of probability; interval-valued probability; lower prevision; lower probability; partial preference ordering; uncertainty measures},
  LOCALFILE    = {article/Walley-2000-towards.pdf},
  NUMBER       = {2-3},
  PAGES        = {125–148},
  TITLE        = {Towards a unified theory of imprecise probability},
  VOLUME       = {24},
  YEAR         = {2000},
}

@MISC{Walley-1998,
  ANNOTATION = {(used to be?) part of the Imprecise Probabilities Project geannoteerde kopie},
  AUTHOR     = {Peter Walley},
  TITLE      = {Coherent upper and lower previsions},
  YEAR       = {1998},
}

@ARTICLE{Walley-1997-bounded,
  ABSTRACT     = {A new method is proposed for drawing coherent statistical inferences about a real-valued parameter in problems where there is little or no prior information. Prior ignorance about the parameter is modelled by the set of all continuous probability density functions for which the derivative of the log-density is bounded by a positive constant. This set is translation-invariant, it contains density functions with a wide variety of shapes and tail behaviour, and it generates prior probabilities that are highly imprecise. Statistical inferences can be calculated by solving a simple type of optimal control problem whose general solution is characterized. Detailed results are given for the problems of calculating posterior upper and lower means, variances, distribution functions and probabilities of intervals. In general, posterior upper and lower expectations are achieved by prior density functions that are piecewise exponential. The results are illustrated by normal and binomial examples},
  AUTHOR       = {Peter Walley},
  DOI          = {10.1111/1467-9469.00075},
  JOURNALTITLE = {Scandinavian Journal of Statistics},
  LOCALFILE    = {article/Walley-1997-bounded.pdf},
  NUMBER       = {4},
  PAGES        = {463–483},
  TITLE        = {A Bounded Derivative model for Prior Ignorance about a Real-valued Parameter},
  VOLUME       = {24},
  YEAR         = {1997},
}

@ARTICLE{Walley-1996-expert,
  ABSTRACT     = {This paper compares four measures that have been advocated as models for uncertainty in expert systems. The measures are additive probabilities (used in the Bayesian theory), coherent lower (or upper) previsions, belief functions (used in the Dempster-Shafer theory) and possibility measures (fuzzy logic). Special emphasis is given to the theory of coherent lower previsions, in which upper and lower probabilities, expectations and conditional probabilities are constructed from initial assessments through a technique of natural extension. Mathematically, all the measures can be regarded as types of coherent lower or upper previsions, and this perspective gives some insight into the properties of belief functions and possibility measures. The measures are evaluated according to six criteria: clarity of interpretation; ability to model partial information and imprecise assessments, especially judgements expressed in natural language; rules for combining and updating uncertainty, and their justification; consistency of models and inferences; feasibility of assessment; and feasibility of computations. Each of the four measures seems to be useful in special kinds of problems, but only lower and upper previsions appear to be sufficiently general to model the most common types of uncertainty.},
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Peter Walley},
  DOI          = {10.1016/0004-3702(95)00009-7},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Bayesian theory; Belief functions; Conditional probability; Decision; Dempster–Shafer theory; Imprecise probabilities; Inference; Lower probability; Possibility theory; Prevision; Upper probability; lndependence},
  LOCALFILE    = {article/Walley-1996-expert.pdf},
  PAGES        = {1–58},
  TITLE        = {Measures of uncertainty in expert systems},
  VOLUME       = {83},
  YEAR         = {1996},
}

@ARTICLE{Walley-1996-IDM,
  ABSTRACT     = {A new method is proposed for making inferences from multinomial data in cases where there is no prior information. A paradigm is the problem of predicting the colour of the next marble to be drawn from a bag whose contents are (initially) completely unknown. In such problems we may be unable to formulate a sample space because we do not know what outcomes are possible. This suggests an invariance principle: inferences based on observations should not depend on the sample space in which the observations and future events of interest are represented. Objective Bayesian methods do not satisfy this principle. This paper describes a statistical model, called the imprecise Dirichlet model, for drawing coherent inferences from multinomial data. Inferences are expressed in terms of posterior upper and lower probabilities. The probabilities are initially vacuous, reflecting prior ignorance, but they become more precise as the number of observations increases. This model does satisfy the invariance principle. Two sets of data are analysed in detail. In the first example one red marble is observed in six drawings from a bag. Inferences from the imprecise Dirichlet model are compared with objective Bayesian and frequentist inferences. The second example is an analysis of data from medical trials which compared two treatments for cardiorespiratory failure in newborn babies. There are two problems: to draw conclusions about which treatment is more effective and to decide when the randomized trials should be terminated. This example shows how the imprecise Dirichlet model can be used to analyse data in the form of a contingency table.},
  ANNOTATION   = {with discussion geannoteerde kopie},
  AUTHOR       = {Peter Walley},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Walley-1996-IDM.pdf},
  NUMBER       = {1},
  PAGES        = {3–57},
  TITLE        = {Inferences from multinomial data: learning about a bag of marbles},
  URL          = {http://www.jstor.org/stable/2346164},
  VOLUME       = {58},
  YEAR         = {1996},
}

@BOOK{Walley-1991,
  AUTHOR    = {Peter Walley},
  LOCALFILE = {book/Walley-1991-book.pdf; book/Walley-1991.pdf},
  LOCATION  = {London},
  PUBLISHER = {Chapman \& Hall},
  SERIES    = {Monographs on Statistics and Applied Probability},
  TITLE     = {Statistical reasoning with imprecise probabilities},
  VOLUME    = {42},
  YEAR      = {1991},
}

@REPORT{Walley-1981,
  ANNOTATION  = {only the part on 2-monotonicity},
  AUTHOR      = {Peter Walley},
  INSTITUTION = {Department of statistics, University of Warwick},
  LOCALFILE   = {techreport/Walley-1981.pdf},
  TITLE       = {Coherent lower (and upper) probabilities},
  TYPE        = {techreport},
  YEAR        = {1981},
}

@ARTICLE{Walley-DeCooman-2001,
  AUTHOR       = {Peter Walley and Gert de Cooman},
  DOI          = {10.1016/S0020-0255(01)00090-1},
  ISSN         = {0020-0255},
  JOURNALTITLE = {Information Sciences},
  KEYWORDS     = {Imprecise probabilities; Linguistic information; Monotonic predicate; Plausibility ordering; Possibility distribution; Prototype theory; Vagueness},
  LOCALFILE    = {article/Walley-DeCooman-2001.pdf},
  NUMBER       = {1-4},
  PAGES        = {1–37},
  TITLE        = {A behavioral model for linguistic uncertainty},
  VOLUME       = {134},
  YEAR         = {2001},
}

@ARTICLE{Walley-DeCooman-1999,
  ABSTRACT     = {Possibility measures and conditional possibility measures are given a behavioural interpretation as marginal betting rates against events. Under this interpretation, possibility measures should satisfy two consistency criteria, known as ‘avoiding sure loss’ and ‘coherence’. We survey the rules that have been proposed for defining conditional possibilities and investigate which of them satisfy our consistency criteria in two situations of practical interest. Only two of these rules satisfy the criteria in both cases studied, and the conditional possibilities produced by these rules are highly uninformative. We introduce a new rule that is more informative and is also coherent in both cases.},
  ANNOTATION   = {reprint},
  AUTHOR       = {Peter Walley and Gert de Cooman},
  DOI          = {10.1016/S0888-613X(99)00007-9},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Dempster's rule; coherence; conditional possibility; imprecise probabilities; natural extension; possibility measure; possibility theory; upper probability},
  LOCALFILE    = {article/Walley-DeCooman-1999.pdf},
  PAGES        = {63–107},
  TITLE        = {Coherence of rules for defining conditional possibility},
  VOLUME       = {21},
  YEAR         = {1999},
}

@ARTICLE{Walley-Gurrin-Burton-1996,
  ABSTRACT     = {This paper describes a new method, based on the theory of imprecise probabilities, for analysing clinical data in the form of a contingency table. The method is applied to a well-known set of statistical data from randomized clinical trials of two treatments for severe cardiorespiratory failure in newborn babies. Two problems are distinguished. The inference problem is to draw conclusions about which treatment is more effective. The decision problem is to determine whether one treatment should be preferred to another for the next patient, or whether it is ethical to select the treatment by randomization. The two problems are analysed using three possible models for prior ignorance about the statistical parameters, and one of the models is modified to take account of earlier clinical data. In this example the four models produce essentially the same conclusions.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Peter Walley and Lyle Gurrin and Paul Burton},
  JOURNALTITLE = {The Statistician},
  LOCALFILE    = {article/Walley-Gurrin-Burton-1996.pdf},
  NUMBER       = {4},
  PAGES        = {457–485},
  TITLE        = {Analysis of clinical data using imprecise prior probabilities},
  URL          = {http://www.jstor.org/stable/2988546},
  VOLUME       = {45},
  YEAR         = {1996},
}

@ARTICLE{2004-Walley++-algorithms,
  ABSTRACT     = {We solve two fundamental problems of probabilistic reasoning: given finitely many conditional probability assessments, how to determine whether the assessments are mutually consistent, and how to determine what they imply about the conditional probabilities of other events? These problems were posed in 1854 by George Boole, who gave a partial solution using algebraic methods. The two problems are fundamental in applications of the Bayesian theory of probability; Bruno de Finetti solved the second problem for the special case of unconditional probability assessments in what he called ‘the fundamental theorem of probability’. We give examples to show that previous attempts to solve the two problems, using probabilistic logic and similar methods, can produce incorrect answers. Using ideas from the theory of imprecise probability, we show that the general problems have simple, direct solutions which can be implemented using linear programming algorithms. Unlike earlier proposals, our methods are formulated directly in terms of the assessments, without introducing unknown probabilities. Our methods work when any of the conditioning events may have probability zero, and they work when the assessments include imprecise (upper and lower) probabilities or previsions. The main methodological contribution of the paper is to provide general algorithms for making inferences from any finite collection of (possibly imprecise) conditional probabilities.},
  AUTHOR       = {Peter Walley and Renato Pelessoni and Paolo Vicig},
  DOI          = {10.1016/j.jspi.2003.09.005},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Avoiding uniform loss; Bayesian inference; Coherent probabilities; Fundamental theorem of probability; Imprecise probability; Lower probability; Natural extension; Probabilistic logic; Probabilistic reasoning},
  LOCALFILE    = {article/2004-Walley++-algorithms.pdf},
  NUMBER       = {1},
  PAGES        = {119–151},
  TITLE        = {Direct algorithms for checking consistency and making inferences from conditional probability assessments},
  VOLUME       = {126},
  YEAR         = {2004},
}

@ARTICLE{Wallner-2007-extremepoints,
  ABSTRACT     = {Every coherent probability (= F-probability) \mathcal{F} on a finite sample space Ω\_k with k elements defines a set of classical probabilities in accordance with the interval limits. This set, called "structure" of , is a convex polytope having dimension \leq k - 1. We prove that the maximal number of extreme points of structures is exactly k!.},
  AUTHOR       = {Anton Wallner},
  DOI          = {10.1016/j.ijar.2006.07.017},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {0/1-Matrix; Coherent probability; Core; Credal set; Extreme point; F-probability; Interval probability; Polyhedron; Polytope; Structure; Vertex},
  LOCALFILE    = {article/Wallner-2007-extremepoints.pdf},
  NUMBER       = {3},
  PAGES        = {339–357},
  TITLE        = {Extreme points of coherent probabilities in finite spaces},
  VOLUME       = {44},
  YEAR         = {2007},
}

@THESIS{Walter-2006,
  AUTHOR      = {Gero Walter},
  INSTITUTION = {Ludwig-Maximilians-Universität München},
  LOCALFILE   = {mastersthesis/Walter-2006.pdf},
  TITLE       = {Robuste Bayes-Regression mit Mengen von Prioris – Ein Beitrag zur Statistik unter komplexer Unsicherheit},
  TYPE        = {mathesis},
  URL         = {http://www.stat.uni-muenchen.de/~thomas/team/diplomathesis_GeroWalter.pdf},
  YEAR        = {2006},
}

@ARTICLE{Walter-Augustin-2009,
  ABSTRACT     = {A great advantage of imprecise probability models over models based on precise, traditional probabilities is the potential to reflect the amount of knowledge they stand for. Consequently, imprecise probability models promise to offer a vivid tool for handling situations of prior-data conflict in (generalized) Bayesian inference. In this paper we consider a general class of recently studied imprecise probability models, including the Imprecise Dirichlet Model under prior information, and more generally the framework of Quaeghebeur and de Cooman for imprecise inference in canonical exponential families. We demonstrate that such models, in their originally proposed form, prove to be insensitive to the extent of prior-data conflict. We propose an extension reestablishing the natural relationship between knowledge and imprecision: The higher the discrepancy between the observed sample and what was expected from prior knowledge, the higher the imprecision in the posterior, producing cautious inferences if, and only if, caution is needed. Our approach is illustrated by some examples and simulation results.},
  AUTHOR       = {Gero Walter and Thomas Augustin},
  DOI          = {10.1080/15598608.2009.10411924},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Canonical exponential family; Generalized Bayesian inference; Generalized Bayes’ Rule; Imprecise Dirichlet Model (IDM); Imprecise priors; Prior-data conflict; Posterior imprecision; Robust Bayesian analysis},
  LOCALFILE    = {article/Walter-Augustin-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {255–271},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {Imprecision and Prior-Data Conflict in Generalized Bayesian Inference},
  VOLUME       = {3},
  YEAR         = {2009},
}

@INPROCEEDINGS{Walter-Augustin-Peters-2007-regression,
  ABSTRACT     = {Regression is the central concept in applied statistics for analyzing multivariate, heterogenous data: The influence of a group of variables on one other variable is quantified by the regression parameter $\beta$. In this paper, we extend standard Bayesian inference on $\beta$ in linear regression models by considering imprecise conjugated priors. Inspired by a variation and an extension of a method for inference in i.i.d. exponential families presented at ISIPTA'05 by Quaeghebeur and de Cooman, we develop a general framework for handling linear regression models including analysis of variance models, and discuss obstacles in direct implementation of the method. Then properties of the interval-valued point estimates for a two-regressor model are derived and illustrated with simulated data. As a practical example we take a small data set from the AIRGENE study and consider the influence of age and body mass index on the concentration of an inflammation marker.},
  AUTHOR       = {Gero Walter and Thomas Augustin and Annette Peters},
  EDITOR       = {Gert de Cooman and Jiřina Vejnarová and Marco Zaffalon},
  ORGANIZATION = {SIPTA},
  TITLE        = {Linear Regression Analysis under Sets of Conjugate Priors},
  VENUE        = {Prague, Czech Republic},
  YEAR         = {2007},
}

@ARTICLE{Wasserman-Kadane-1992,
  ABSTRACT     = {One method for evaluating the sensitivity of a Bayesian analysis is to embed the prior into a class of priors. Then bounds on prior and posterior quantities of interest must be computed. This approach to inference, often called robust Bayesian inference, has received much attention lately. Implementing robust Bayesian methods entails difficult computations, especially if the parameter space is high dimensional. In this article we develop a Monte Carlo approach to computing these bounds and also explore some interesting theoretical properties of certain classes of priors. The methods can be useful in other situations in which bounds on expectations are required.},
  AUTHOR       = {Larry Wasserman and Joseph B. Kadane},
  ISSN         = {0162-1459},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LOCALFILE    = {article/Wasserman-Kadane-1992.pdf},
  NUMBER       = {418},
  PAGES        = {516–522},
  PUBLISHER    = {American Statistical Association},
  TITLE        = {Computing bounds on expectations},
  URL          = {http://www.jstor.org/stable/2290285},
  VOLUME       = {87},
  YEAR         = {1992},
}

@BOOK{2001-Weichselberger-Grundbegriffe,
  ANNOTATION  = {Unter Mitarbeit von T. Augustin und A. Wallner},
  AUTHOR      = {Kurt Weichselberger},
  HYPHENATION = {ngerman},
  LOCATION    = {Heidelberg},
  PUBLISHER   = {Physica-Verlag},
  TITLE       = {Elementare Grundbegriffe einer allgemeineren Wahrscheinlichkeitsrechnung I: Intervallwahrscheinlichkeit als Umfassendes Konzept},
  YEAR        = {2001},
}

@ARTICLE{2000-Weichselberger-IJAR,
  ABSTRACT     = {The concept of interval-probability is motivated by the goal to generalize classical-probability so that it can be used for describing uncertainty in general. The foundations of the theory are based on a system of three axioms — in addition to Kolmogorov's axioms — and definitions of independence as well as of conditional-probability. The resulting theory does not depend upon interpretations of the probability concept. As an example of generalising classical results Bayes' theorem is described — other theorems are only mentioned.},
  AUTHOR       = {Kurt Weichselberger},
  DOI          = {10.1016/S0888-613X(00)00032-3},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Interval-probability; Uncertainty; Conditional-probability; Theorem of Bayes},
  LOCALFILE    = {article/2000-Weichselberger-IJAR.pdf},
  NUMBER       = {2–3},
  PAGES        = {149–170},
  TITLE        = {The theory of interval-probability as a unifying concept for uncertainty},
  VOLUME       = {24},
  YEAR         = {2000},
}

@ARTICLE{Wellman-1990-QPN,
  ABSTRACT     = {Graphical representations for probabilistic relationships have recently received considerable attention in AI. Qualitative probabilistic networks abstract from the usual numeric representations by encoding only qualitative relationships, which are inequality constraints on the joint probability distribution over the variables. Although these constraints are insufficient to determine probabilities uniquely, they are designed to justify the deduction of a class of relative likelihood conclusions that imply useful decision-making properties. Two types of qualitative relationship are defined, each a probabilistic form of monotonicity constraint over a group of variables. Qualitative influences describe the direction of the relationship between two variables. Qualitative synergies describe interactions among influences. The probabilistic definitions chosen justify sound and efficient inference procedures based on graphical manipulations of the network. These procedures answer queries about qualitative relationships among variables separated in the network and determine structural properties of optimal assignments to decision variables.},
  AUTHOR       = {Michael P. Wellman},
  DOI          = {10.1016/0004-3702(90)90026-V},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  NUMBER       = {3},
  PAGES        = {257–303},
  TITLE        = {Fundamental concepts of qualitative probabilistic networks},
  VOLUME       = {44},
  YEAR         = {1990},
}

@BOOK{Whittle-1992,
  AUTHOR    = {Peter Whittle},
  EDITION   = {3},
  PUBLISHER = {Springer},
  SERIES    = {Springer Texts in Statistics},
  TITLE     = {Probability via Expectation},
  VOLUME    = {XVIII},
  YEAR      = {1992},
}

@ARTICLE{Whittle-1955,
  ABSTRACT     = {An exact formula (8) is derived for the probability distribution of an observed set of transition totals. This expression furnishes asymptotic expressions for the likelihood (24), for the covariances of transition totals (27), and for the distribution of Bartlett's goodness-of-fit statistic (31). Formulae are also derived for the expectations of some sample functions related to the factorial moments of the transition totals (34) and for the lower moments of the estimated transition probabilities (42), (44). It is shown that the Markov chain has properties similar to those of a set of independent multinomial distributions.},
  ANNOTATION   = {geannoteerde kopie},
  AUTHOR       = {Peter Whittle},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  LOCALFILE    = {article/Whittle-1955.pdf},
  NUMBER       = {2},
  PAGES        = {235–242},
  TITLE        = {Some distribution and moment formulae for the Markov chain},
  URL          = {http://www.jstor.org/stable/2983957},
  VOLUME       = {17},
  YEAR         = {1955},
}

@INPROCEEDINGS{Williams-1974,
  AUTHOR    = {Peter M. Williams},
  BOOKTITLE = {Formal methods in the methodology of empirical sciences: Proceedings of the conference for formal methods in the methodology of empirical sciences},
  EDITOR    = {Marian Przełęcki and Klemens Szaniawski and Ryszard Wójcicki},
  LOCALFILE = {inproceedings/Williams-1974.pdf},
  PAGES     = {229–246},
  PUBLISHER = {D. Reidel Publishing Company and Ossolineum Publishing company},
  TITLE     = {Indeterminate probabilities},
  YEAR      = {1974},
}

@ARTICLE{Williams-2007-notes,
  ABSTRACT     = {The personalist conception of probability is often explicated in terms of betting rates acceptable to an individual. A common approach, that of de Finetti for example, assumes that the individual is willing to take either side of the bet, so that the bet is “fair” from the individual's point of view. This can sometimes be unrealistic, and leads to difficulties in the case of conditional probabilities or previsions. An alternative conception is presented in which it is only assumed that the collection of acceptable bets forms a convex cone, rather than a linear space. This leads to the more general conception of an upper conditional prevision. The main concerns of the paper are with the extension of upper conditional previsions. The main result is that any upper conditional prevision is the upper envelope of a family of additive conditional previsions.},
  AUTHOR       = {Peter M. Williams},
  DOI          = {10.1016/j.ijar.2006.07.019},
  INSTITUTION  = {University of Sussex},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Coherence; Conditional prevision; Imprecise probabilities; de Finetti},
  LOCALFILE    = {article/Williams-2007-notes.pdf},
  NOTE         = {Published version of \cite{Williams-1975}},
  PAGES        = {366–383},
  TITLE        = {Notes on conditional previsions},
  VOLUME       = {44},
  YEAR         = {2007},
}

@ARTICLE{Williams-1980,
  AUTHOR       = {Peter M. Williams},
  JOURNALTITLE = {The British Journal for the Philosophy of Science},
  LOCALFILE    = {article/Williams-1980.pdf},
  NUMBER       = {2},
  PAGES        = {131–144},
  TITLE        = {Bayesian conditionalisation and the principle of minimum information},
  URL          = {http://www.jstor.org/stable/687182},
  VOLUME       = {31},
  YEAR         = {1980},
}

@ARTICLE{Williams-1978,
  ANNOTATION   = {Review of Shafer-1976},
  AUTHOR       = {Peter M. Williams},
  JOURNALTITLE = {The British Journal for the Philosophy of Science},
  LOCALFILE    = {article/Williams-1978.pdf},
  NUMBER       = {4},
  PAGES        = {375–387},
  TITLE        = {On a new theory of epistemic probability},
  URL          = {http://www.jstor.org/stable/687102},
  VOLUME       = {29},
  YEAR         = {1978},
}

@REPORT{Williams-1975,
  AUTHOR      = {Peter M. Williams},
  BOOKTITLE   = {International Journal of Approximate Reasoning},
  INSTITUTION = {University of Sussex},
  LOCALFILE   = {techreport/Williams-1975.pdf},
  NOTE        = {Published as \cite{Williams-2007-notes}},
  PAGES       = {29},
  TITLE       = {Notes on conditional previsions},
  TYPE        = {techreport},
  VOLUME      = {44},
  YEAR        = {1975},
}

@ARTICLE{Wilson-Huzurbazar-Sentz-2009,
  ABSTRACT     = {In this paper we expand on recent advances in Bayesian inference for multilevel data in fault trees and Bayesian networks. As a first example, we compare the Bayesian fault tree and incomplete data approaches to statistical inference for multilevel data in fault trees. As a second example, we consider two a priori representations of uncertainty about the parameters of a Bayesian network: A multinomial-Dirichlet model and an extension of the imprecise Dirichlet model. We calculate the a posteriori uncertainty after updating with data using Markov chain Monte Carlo and compare the results.},
  AUTHOR       = {Alyson G. Wilson and Aparna V. Huzurbazar and Kari Sentz},
  DOI          = {10.1080/15598608.2009.10411921},
  ISSN         = {1559-8608},
  JOURNALTITLE = {Journal of Statistical Theory and Practice},
  KEYWORDS     = {Bayesian network; Fault tree; Imprecise Dirichlet model; Multilevel data; Multinomial-Dirichlet model; Reliability},
  LOCALFILE    = {article/Wilson-Huzurbazar-Sentz-2009.pdf},
  MONTH        = {03},
  NUMBER       = {1},
  PAGES        = {211–223},
  PUBLISHER    = {Taylor \& Francis},
  TITLE        = {The Imprecise Dirichlet Model for Multilevel System Reliability},
  VOLUME       = {3},
  YEAR         = {2009},
}

@INPROCEEDINGS{Wilson-Moral-1994,
  AUTHOR    = {Nic Wilson and Serafín Moral},
  BOOKTITLE = {ECAI 94: Proceedings of the 11th European Conference on Artificial Intelligence},
  EDITOR    = {A. Cohn},
  LOCALFILE = {inproceedings/Wilson-Moral-1994.pdf},
  PAGES     = {386–390},
  PUBLISHER = {John Wiley \& Sons},
  TITLE     = {A logical view of probability},
  YEAR      = {1994},
}

@ARTICLE{Witteman-Renooij-2003,
  ABSTRACT     = {For purposes such as the development of decision support systems, the probabilities that model the uncertainties in the domain of application are usually elicited from domain experts. A number of elicitation methods is available. While constructing a real-life system, we however found none of these methods to be quite usable: they turned out to be too time-consuming and difficult for experts. In an earlier paper we described a verbal–numerical response scale we developed to facilitate elicitation of a large number of probabilities. In this paper we describe a study that justifies our claim that use of this verbal–numerical scale generally facilitates the assessment process.},
  AUTHOR       = {Cilia L. M. Witteman and Silja Renooij},
  DOI          = {10.1016/S0888-613X(02)00151-2},
  ISSN         = {0888-613X},
  JOURNALTITLE = {International Journal of Approximate Reasoning},
  KEYWORDS     = {Probability elicitation; Decision support systems; Accuracy of assessments; Response scale},
  NUMBER       = {2},
  PAGES        = {117–131},
  TITLE        = {Evaluation of a verbal–numerical probability scale},
  VOLUME       = {33},
  YEAR         = {2003},
}

@ARTICLE{Witteman-Renooij-Koele-2007,
  ABSTRACT     = {BACKGROUND:In the complex domain of medical decision making, reasoning under uncertainty can benefit from supporting tools. Automated decision support tools often build upon mathematical models, such as Bayesian networks. These networks require probabilities which often have to be assessed by experts in the domain of application. Probability response scales can be used to support the assessment process. We compare assessments obtained with different types of response scale.METHODS:General practitioners (GPs) gave assessments on and preferences for three different probability response scales: a numerical scale, a scale with only verbal labels, and a combined verbal-numerical scale we had designed ourselves. Standard analyses of variance were performed.RESULTS:No differences in assessments over the three response scales were found. Preferences for type of scale differed: the less experienced GPs preferred the verbal scale, the most experienced preferred the numerical scale, with the groups in between having a preference for the combined verbal-numerical scale.CONCLUSION:We conclude that all three response scales are equally suitable for supporting probability assessment. The combined verbal-numerical scale is a good choice for aiding the process, since it offers numerical labels to those who prefer numbers and verbal labels to those who prefer words, and accommodates both more and less experienced professionals.},
  AUTHOR       = {Cilia L. M. Witteman and Silja Renooij and Pieter Koele},
  DOI          = {10.1186/1472-6947-7-13},
  ISSN         = {1472-6947},
  JOURNALTITLE = {BMC Medical Informatics and Decision Making},
  NUMBER       = {1},
  PAGES        = {13},
  TITLE        = {Medicine in words and numbers: a cross-sectional survey comparing probability assessment scales},
  VOLUME       = {7},
  YEAR         = {2007},
}

@ARTICLE{Wong-1998,
  ABSTRACT     = {Generalized Dirichlet distribution has a more general covariance structure than Dirichlet distribution. This makes the generalized Dirichlet distribution to be more practical and useful. The concept of complete neutrality will be used to derive the general moment function for the generalized Dirichlet distribution, and then some properties of the generalized Dirichlet distribution will be established. Similar to the Dirichlet distribution, the generalized Dirichlet distribution will be shown to conjugate to multinominal sampling. Two experiments are designed for studying the differences between the Dirichlet and the generalized Dirichlet distributions in Bayesian analysis. A method for generating samples from a generalized Dirichlet in presented. When a prior distribution is either a Dirichlet or a generalized Dirichlet distribution, the way for constructing such a prior is discussed.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Tzu-Tsung Wong},
  DOI          = {10.1016/S0096-3003(97)10140-0},
  JOURNALTITLE = {Applied Mathematics and Computation},
  KEYWORDS     = {Bayesian analysis; Completely neutral; Conjugate; Generalized Dirichlet distribution; Prior construction},
  LOCALFILE    = {article/Wong-1998.pdf},
  NUMBER       = {2-3},
  PAGES        = {165–181},
  PUBLISHER    = {Elsevier},
  TITLE        = {Generalized Dirichlet distribution in Bayesian analysis},
  VOLUME       = {97},
  YEAR         = {1998},
}

@INCOLLECTION{Woudenberg-Van_der_Gaag-2011,
  AUTHOR    = {Steven P. D. Woudenberg and Linda C. van der Gaag},
  BOOKTITLE = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty},
  DOI       = {10.1007/978-3-642-22152-1_11},
  EDITOR    = {Weiru Liu},
  ISBN      = {978-3-642-22151-4},
  LOCALFILE = {inproceedings/Woudenberg-Van_der_Gaag-2011.pdf},
  PAGES     = {122–133},
  PUBLISHER = {Springer Berlin Heidelberg},
  SERIES    = {Lecture Notes in Computer Science},
  TITLE     = {Using the Noisy-OR model can be harmful … but it often is not},
  VOLUME    = {6717},
  YEAR      = {2011},
}

@ARTICLE{Xie-Beerel-1998-stateclassif,
  ABSTRACT     = {This paper presents an efficient method for state classification of finite-state Markov chains using binary-decision diagram-based symbolic techniques. The method exploits the fundamental properties of a Markov chain and classifies the state space by iteratively applying reachability analysis. We compare our method with the state-of-the-art technique, which requires the transitive closure of the transition relation of a Markov chain. Experiments in over a dozen synchronous and asynchronous systems and queueing networks demonstrate that our method dramatically reduces the CPU time needed and solves much larger problems because of the reduced memory requirements},
  ANNOTATION   = {Geannoteerde versie op papier},
  AUTHOR       = {Aiguo Xie and P. A. Beerel},
  DOI          = {10.1109/43.736573},
  ISSN         = {0278-0070},
  JOURNALTITLE = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  KEYWORDS     = {BDD-based symbolic techniques; CPU time reduction; Markov processes; asynchronous systems; binary decision diagrams; binary-decision diagram; circuit analysis computing; finite-state Markov chains; queueing networks; reachability analysis; state classification; state space; synchronous systems},
  LOCALFILE    = {article/Xie-Beerel-1998-stateclassif.pdf},
  NUMBER       = {12},
  PAGES        = {1334–1339},
  TITLE        = {Efficient state classification of finite-state Markov chains},
  VOLUME       = {17},
  YEAR         = {1998},
}

@ARTICLE{Yu-Zeleny-1975,
  ABSTRACT     = {In this note we are interested in the properties of, and methods for locating the set of all nondominated solutions of multiple linear criteria defined over a polyhedron. We first show that the set of all dominated solutions is convex and that the set of all nondominated solutions is a subset of the convex hull of the nondominated extreme points. When the domination cone is polyhedral, we derive a necessary and sufficient condition for a point to be nondominated. The condition is stronger than that of Ref. [1] and enables us to give a simple proof that the set of all nondominated extreme points indeed is connected. In order to locate the entire set of all nondominated extreme points, we derive a generalized version of simplex method—multicriteria simplex method. In addition to some useful results, a necessary and sufficient condition for an extreme point to be nondominated is derived. Examples and computer experience are also given. Finally, we focus on how to generate the entire set of all nondominated solutions through the set of all nondominated extreme points. A decomposition theorem and some necessary and sufficient conditions for a face to be nondominated are derived. We then describe a systematic way to identify the entire set of all nondominated solutions. Through examples, we show that in fact our procedure is quite efficient.},
  AUTHOR       = {P. L. Yu and M. Zeleny},
  DOI          = {10.1016/0022-247X(75)90189-4},
  ISSN         = {0022-247X},
  JOURNALTITLE = {Journal of Mathematical Analysis and Applications},
  LOCALFILE    = {article/Yu-Zeleny-1975.pdf},
  NUMBER       = {2},
  PAGES        = {430–468},
  TITLE        = {The set of all nondominated solutions in linear cases and a multicriteria simplex method},
  VOLUME       = {49},
  YEAR         = {1975},
}

@BOOK{Zabell-2005,
  AUTHOR    = {Sandy L. Zabell},
  LOCATION  = {Cambridge, United Kingdom},
  PUBLISHER = {Cambridge University Press},
  SERIES    = {Cambridge Studies in Probability, Induction, and Decision Theory},
  TITLE     = {Symmetry and Its Discontents: Essay on the History of Inductive Probability},
  YEAR      = {2005},
}

@ARTICLE{Zabell-1995,
  ABSTRACT     = {In the 1920s the English philosopher W. E. Johnson discovered a simple characterization of the Dirichlet family of conjugate priors for a multinomial distribution having at least three categories. In the present note Johnson's result is extended to the case of a Markov exchangeable sequence.},
  AUTHOR       = {Sandy L. Zabell},
  DOI          = {10.1007/BF02213460},
  JOURNALTITLE = {Journal of Theoretical Probability},
  KEYWORDS     = {Markov exchangeable sequences; W. E. Johnson; conjugate prior; predictive probability},
  LOCALFILE    = {article/Zabell-1995.pdf},
  NUMBER       = {1},
  PAGES        = {175–178},
  TITLE        = {Characterizing Markov exchangeable sequences},
  VOLUME       = {8},
  YEAR         = {1995},
}

@ARTICLE{Zabell-1992,
  ABSTRACT     = {A major difficulty for currently existing theories of inductive inference involves the question of what to do when novel, unknown, or previously unsuspected phenomena occur. In this paper one particular instance of this difficulty is considered, the so-called sampling of species problem. The classical probabilistic theories of inductive inference due to Laplace, Johnson, de Finetti, and Carnap adopt a model of simple enumerative induction in which there are a prespecified number of types or species which may be observed. But, realistically, this is often not the case. In 1838 the English mathematician Augustus De Morgan proposed a modification of the Laplacian model to accommodate situations where the possible types or species to be observed are not assumed to be known in advance; but he did not advance a justification for his solution. In this paper a general philosophical approach to such problems is suggested, drawing on work of the English mathematician J. F. C. Kingman. It then emerges that the solution advanced by De Morgan has a very deep, if not totally unexpected, justification. The key idea is that although lsquoexchangeablersquo random sequences are the right objects to consider when all possible outcome-types are known in advance, exchangeable random partitions are the right objects to consider when they are not. The result turns out to be very satisfying. The classical theory has several basic elements: a representation theorem for the general exchangeable sequence (the de Finetti representation theorem), a distinguished class of sequences (those employing Dirichlet priors), and a corresponding rule of succession (the continuum of inductive methods). The new theory has parallel basic elements: a representation theorem for the general exchangeable random partition (the Kingman representation theorem), a distinguished class of random partitions (the Poisson-Dirichlet process), and a rule of succession which corresponds to De Morgan's rule.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Sandy L. Zabell},
  DOI          = {10.1007/BF00485351},
  JOURNALTITLE = {Synthese},
  LOCALFILE    = {article/Zabell-1992.pdf},
  NUMBER       = {2},
  PAGES        = {205–232},
  PUBLISHER    = {Harvard Business School Publication Corp.},
  TITLE        = {Predicting the unpredictable},
  VOLUME       = {90},
  YEAR         = {1992},
}

@ARTICLE{Zabell-1982,
  ABSTRACT     = {How do Bayesians justify using conjugate priors on grounds other than mathematical convenience? In the 1920's the Cambridge philosopher William Ernest Johnson in effect characterized symmetric Dirichlet priors for multinomial sampling in terms of a natural and easily assessed subjective condition. Johnson's proof can be generalized to include asymmetric Dirichlet priors and those finitely exchangeable sequences with linear posterior expectation of success. Some interesting open problems that Johnson's result raises, and its historical and philosophical background, are also discussed.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Sandy L. Zabell},
  DOI          = {10.1214/aos/1176345975},
  JOURNALTITLE = {The Annals of Statistics},
  LOCALFILE    = {article/Zabell-1982.pdf},
  NUMBER       = {4},
  PAGES        = {1091–1099},
  PUBLISHER    = {Institute of Mathematical Statistics},
  TITLE        = {W. E. Johnson's “sufficientness” postulate},
  VOLUME       = {10},
  YEAR         = {1982},
}

@INPROCEEDINGS{Zaffalon-1999,
  AUTHOR    = {Marco Zaffalon},
  BOOKTITLE = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
  EDITOR    = {Gert de Cooman and Fabio Gagliardi Cozman and Serafín Moral and Peter Walley},
  PAGES     = {405–414},
  TITLE     = {A Credal Approach to Naive Classification},
  VENUE     = {Ghent, Belgium},
  YEAR      = {1999},
}

@INPROCEEDINGS{Zaffalon-2001,
  AUTHOR    = {Marco Zaffalon},
  BOOKTITLE = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
  EDITOR    = {Gert de Cooman and Terrence L. Fine and Teddy Seidenfeld},
  LOCATION  = {Maastricht, the Netherlands},
  PAGES     = {384–393},
  PUBLISHER = {Shaker Publishing},
  TITLE     = {Statistical inference of the naive credal classifier},
  VENUE     = {Ithaca, New York},
  YEAR      = {2001},
}

@ARTICLE{Zaffalon-2005-environmental,
  ABSTRACT     = {Classifiers that aim at doing credible predictions should rely on carefully elicited prior knowledge. Often this is not available so they should start learning from data in condition of near-ignorance. This paper shows empirically, on an agricultural data set, that established methods of classification do not always adhere to this principle. Traditional ways to represent prior ignorance are shown to have an overwhelming weight compared to the information in the data, producing overconfident predictions. This point is crucial for problems, such as environmental ones, where prior knowledge is often scarce and even the data may not be known precisely. Credal classification, and in particular the naive credal classifier, is proposed as more faithful ways to cope with the ignorance problem. With credal classification, conditions of ignorance may limit the power of the inferences, not the credibility of the predictions.},
  AUTHOR       = {Marco Zaffalon},
  DOI          = {10.1016/j.envsoft.2004.10.006},
  JOURNALTITLE = {Environmental Modelling \& Software},
  NUMBER       = {8},
  PAGES        = {1003–1012},
  TITLE        = {Credible classification for environmental problems},
  VOLUME       = {20},
  YEAR         = {2005},
}

@ARTICLE{Zaffalon-2002-missing,
  ABSTRACT     = {This paper proposes an exact, no-assumptions approach to dealing with incomplete sets of multivariate categorical data. An incomplete data set is regarded as a 1nite collection of complete data sets, and a joint distribution is obtained from each of them, at a descriptive level. The tools to simultaneously treat all the possible joint distributions compatible with an incomplete set of data are given. In particular, a linear description of the set of distributions is formulated, and it is shown that the computation of bounds on the expectation of real-valued functions under such distributions is both possible and efficient, by means of linear programming. Specific algorithms are also developed whose complexity grows linearly in the number of observations. An analysis is then carried out to estimate population probabilities from incomplete multinomial samples. The descriptive tool extends in a straightforward way to the inferential problem by exploiting Walley s imprecise Dirichlet model.},
  AUTHOR       = {Marco Zaffalon},
  DOI          = {10.1016/S0378-3758(01)00206-3},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Belief functions; Credal sets; Flow network; Imprecise Dirichlet model; Imprecise probabilities; Incomplete data; Linear optimization},
  LOCALFILE    = {article/Zaffalon-2002-missing.pdf},
  PAGES        = {105–122},
  TITLE        = {Exact credal treatment of missing data},
  VOLUME       = {105},
  YEAR         = {2002},
}

@ARTICLE{Zaffalon-2002-ncc,
  ABSTRACT     = {Convex sets of probability distributions are also called credal sets. They generalize probability theory by relaxing the requirement that probability values be precise. Classification, i.e. assigning class labels to instances described by a set of attributes, is an important domain of application of Bayesian methods, where the naive Bayes classifier has a surprisingly good performance. This paper proposes a new method of classification which involves extending the naive Bayes classifier to credal sets. Exact and effective solution procedures for naive credal classification are derived, and the related dominance criteria are discussed. Credal classiffcation appears as a new method, based on more realistic assumptions and in the direction of more reliable inferences.},
  ANNOTATION   = {ook op papier},
  AUTHOR       = {Marco Zaffalon},
  DOI          = {10.1016/S0378-3758(01)00201-4},
  JOURNALTITLE = {Journal of Statistical Planning and Inference},
  KEYWORDS     = {Classification; Credal sets; Imprecise probabilities; Naive Bayes classifier; Pattern recognition},
  LOCALFILE    = {article/Zaffalon-2002-ncc.pdf},
  NUMBER       = {1},
  PAGES        = {5–21},
  PUBLISHER    = {Elsevier},
  TITLE        = {The naive credal classifier},
  VOLUME       = {105},
  YEAR         = {2002},
}

@ARTICLE{Zaffalon-DeCooman-2005-editorial,
  ANNOTATION   = {reprint},
  AUTHOR       = {Marco Zaffalon and Gert de Cooman},
  DOI          = {10.1007/s10472-005-9009-7},
  JOURNALTITLE = {Annals of Mathematics and Artificial Intelligence},
  PAGES        = {1–4},
  TITLE        = {Editorial: Imprecise probability perspectives on artificial intelligence},
  VOLUME       = {45},
  YEAR         = {2005},
}

@ARTICLE{Zaffalon-Fagiuoli-2003,
  ABSTRACT     = {Bayesian networks are models for uncertain reasoning which are achieving a growing importance also for the data mining task of classification. Credal networks extend Bayesian nets to sets of distributions, or credal sets. This paper extends a state-of-the-art Bayesian net for classification, called tree-augmented naive Bayes classifier, to credal sets originated from probability intervals. This extension is a basis to address the fundamental problem of prior ignorance about the distribution that generates the data, which is a commonplace in data mining applications. This issue is often neglected, but addressing it properly is a key to ultimately draw reliable conclusions from the inferred models. In this paper we formalize the new model, develop an exact linear-time classification algorithm, and evaluate the credal net-based classifier on a number of real data sets. The empirical analysis shows that the new classifier is good and reliable, and raises a problem of excessive caution that is discussed in the paper. Overall, given the favorable trade-off between expressiveness and efficient computation, the newly proposed classifier appears to be a good candidate for the wide-scale application of reliable classifiers based on credal networks, to real and complex tasks.},
  AUTHOR       = {Marco Zaffalon and Enrico Fagiuoli},
  DOI          = {10.1023/A:1025822321743},
  JOURNALTITLE = {Reliable Computing},
  NUMBER       = {6},
  PAGES        = {487–509},
  TITLE        = {Tree-Based Credal Networks for Classification},
  VOLUME       = {9},
  YEAR         = {2003},
}

@ARTICLE{2013-Zaffalon+Miranda-time,
  ABSTRACT     = {Probabilistic reasoning is often attributed a temporal meaning, in which conditioning is regarded as a normative rule to compute future beliefs out of current beliefs and observations. However, the well-established ‘updating interpretation’ of conditioning is not concerned with beliefs that evolve in time, and in particular with future beliefs. On the other hand, a temporal justification of conditioning was proposed already by De Moivre and Bayes, by requiring that current and future beliefs be consistent. We reconsider the latter approach while dealing with a generalised version of the problem, using a behavioural theory of imprecise probability in the form of coherent lower previsions as well as of coherent sets of desirable gambles, and letting the possibility space be finite or infinite. We obtain that using conditioning is normative, in the imprecise case, only if one establishes future behavioural commitments at the same time of current beliefs. In this case it is also normative that present beliefs be conglomerable, which is a result that touches on a long-term controversy at the foundations of probability. In the remaining case, where one commits to some future behaviour after establishing present beliefs, we characterise the several possibilities to define consistent future assessments; this shows in particular that temporal consistency does not preclude changes of mind. And yet, our analysis does not support that rationality requires consistency in general, even though pursuing consistency makes sense and is useful, at least as a way to guide and evaluate the assessment process. These considerations narrow down in the special case of precise probability, because this formalism cannot distinguish the two different situations illustrated above: it turns out that the only consistent rule is conditioning and moreover that it is not rational to be willing to stick to precise probability while using a rule different from conditioning to compute future beliefs; rationality requires in addition the disintegrability of the present-time probability.},
  AUTHOR       = {Marco Zaffalon and Enrique Miranda},
  DOI          = {10.1016/j.artint.2013.02.005},
  ISSN         = {0004-3702},
  JOURNALTITLE = {Artificial Intelligence},
  KEYWORDS     = {Temporal reasoning; Imprecise probabilities; Conditioning; Lower previsions; Sets of desirable gambles; Coherence; Conglomerability},
  LOCALFILE    = {article/2013-Zaffalon+Miranda-time.pdf},
  NUMBER       = {0},
  PAGES        = {1–51},
  TITLE        = {Probability and time},
  VOLUME       = {198},
  YEAR         = {2013},
}

@ARTICLE{Zaffalon-Wesnes-Petrini-2003-dementia,
  ABSTRACT     = {Dementia is a serious personal, medical and social problem. Recent research indicates early and accurate diagnoses as the key to effectively cope with it. No definitive cure is available but in some cases when the impairment is still mild the disease can be contained. This paper describes a diagnostic tool that jointly uses the naive credal classifier and the most widely used computerized system of cognitive tests in dementia research, the Cognitive Drug Research system. The naive credal classifier extends the discrete naive Bayes classifier to imprecise probabilities. The naive credal classifier models both prior ignorance and ignorance about the likelihood by sets of probability distributions. This is a new way to deal with small and incomplete datasets that departs significantly from most established classification methods. In the empirical study presented here, the naive credal classifier provides reliability and unmatched predictive performance. It delivers up to 95\% correct predictions while being very robust with respect to the partial ignorance due to the largely incomplete data. The diagnostic tool also proves to be very effective in discriminating between Alzheimer's disease and dementia with Lewy bodies.},
  AUTHOR       = {Marco Zaffalon and Keith Wesnes and Orlando Petrini},
  DOI          = {10.1016/S0933-3657(03)00046-0},
  JOURNALTITLE = {Artificial Intelligence in Medicine},
  KEYWORDS     = {Cognitive tests; Credal classification; Dementia},
  NUMBER       = {1-2},
  PAGES        = {61–79},
  TITLE        = {Reliable diagnoses of dementia by the naive credal classifier inferred from incomplete cognitive data},
  VOLUME       = {29},
  YEAR         = {2003},
}

@ARTICLE{Zhu-Lomsadze-Borodovsky-2010,
  ABSTRACT     = {We describe an algorithm for gene identification in DNA sequences derived from shotgun sequencing of microbial communities. Accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters. While several machine learning approaches could be proposed to bypass this difficulty, one effective method is to estimate parameters from dependencies, formed in evolution, between frequencies of oligonucleotides in protein-coding regions and genome nucleotide composition. Original version of the method was proposed in 1999 and has been used since for (i) reconstructing codon frequency vector needed for gene finding in viral genomes and (ii) initializing parameters of self-training gene finding algorithms. With advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies, as well as by separating models for bacteria and archaea. These advances have increased the accuracy of model reconstruction and, subsequently, gene prediction. We describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences. Also, we show that as a result of application of the new method, several thousands of new genes could be added to existing annotations of several human and mouse gut metagenomes.},
  AUTHOR       = {Wenhan Zhu and Alexandre Lomsadze and Mark Borodovsky},
  DOI          = {10.1093/nar},
  JOURNALTITLE = {Nucleic Acids Research},
  NUMBER       = {12},
  PAGES        = {e132},
  TITLE        = {Ab initio gene identification in metagenomic sequences},
  VOLUME       = {38},
  YEAR         = {2010},
}

@BOOK{Ziegler-1995,
  AUTHOR    = {Günter M. Ziegler},
  PUBLISHER = {Springer},
  TITLE     = {Lectures on polytopes},
  YEAR      = {1995},
}

@ARTICLE{Zimmermann-1985,
  ABSTRACT     = {Mathematical programming is one of the areas to which fuzzy set theory has been applied extensively. Primarily based on Bellman and Zadeh's model of decision in fuzzy environments, models have been suggested which allow flexibility in constraints and fuzziness in the objective function in traditional linear and nonlinear programming, in integer and fractional programming, and in dynamic programming. These models in turn have been used to offer computationally efficient approaches for solving vector maximum problems. This paper surveys major models and theories in this area and offers some indication on future developments which can be expected.},
  AUTHOR       = {Hans-Jürgen Zimmermann},
  DOI          = {10.1016/0020-0255(85)90025-8},
  ISSN         = {0020-0255},
  JOURNALTITLE = {Information Sciences},
  NUMBER       = {1-2},
  PAGES        = {29–58},
  TITLE        = {Applications of fuzzy set theory to mathematical programming},
  VOLUME       = {36},
  YEAR         = {1985},
}

@ARTICLE{Zimmermann-1983,
  ABSTRACT     = {Fuzzy linear programming (FLP) was originally suggested to solve problems which could be formulated as LP-models, the parameters of which, however, were fuzzy rather than crisp numbers. It has turned out in the meantime that FLP is also well suited to solve LP-problems with several objective functions. FLP belongs to goal programming in the sense that implicitly or explicitly aspiration levels have to be defined at which the membership functions of the fuzzy sets reach their maximum or minimum. Main advantages of FLP are, that the models used are numerically very efficient and that they can in many ways be well adopted to different decision behaviors and contexts.},
  AUTHOR       = {Hans-Jürgen Zimmermann},
  DOI          = {10.1016/0305-0548(83)90004-7},
  ISSN         = {0305-0548},
  JOURNALTITLE = {Computers \& Operations Research},
  NUMBER       = {4},
  PAGES        = {291–298},
  TITLE        = {Fuzzy mathematical programming},
  VOLUME       = {10},
  YEAR         = {1983},
}

@ARTICLE{Zimmermann-1975,
  ABSTRACT     = {The concept of fuzzy sets is presented as a new tool for the formulation and solution of systems and decision problems which contain fuzzy components or fuzzy relationships. After a brief description of the basic theory of fuzzy sets, implications to systems theory and decision making are indicated. Fuzzy set theory is then applied to fuzzy linear programming problems and it is shown how fuzzy linear programming problems can be solved without increasing the computational effort. Some critical remarks concerning the presently existing axioms and necessary future research efforts conclude this introductionary paper.},
  AUTHOR       = {Hans-Jürgen Zimmermann},
  DOI          = {10.1080/03081077508960870},
  JOURNALTITLE = {International Journal of General Systems},
  NUMBER       = {1},
  TITLE        = {Description and optimization of fuzzy systems},
  VOLUME       = {2},
  YEAR         = {1975},
}

@BOOK{Abramowitz-Stegun-1972,
  EDITOR    = {Milton Abramowitz and Irene A Stegun},
  PUBLISHER = {Dover},
  TITLE     = {Handbook of mathematical functions with formulas, graphs, and mathematical tables},
  URL       = {http://frameindex.htm},
  YEAR      = {1972},
}

@PROCEEDINGS{ISIPTA-2009,
  EDITOR       = {Thomas Augustin and Frank P. A. Coolen and Serafín Moral and Matthias C. M. Troffaes},
  ORGANIZATION = {SIPTA},
  TITLE        = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
  VENUE        = {Durham, United Kingdom},
  YEAR         = {2009},
}

@PROCEEDINGS{CIPS-1951,
  BOOKTITLE = {Congrès international de philosophie des sciences. 4: Calcul des probabilités},
  EDITOR    = {Raymond Bayer},
  LOCATION  = {Paris},
  NUMBER    = {1146},
  PUBLISHER = {Hermann},
  SERIES    = {Actualités scientifiques et industrielles},
  TITLE     = {Congrès international de philosophie des sciences},
  YEAR      = {1951},
}

@PROCEEDINGS{ISIPTA-2003,
  BOOKTITLE = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
  EDITOR    = {Jean-Marc Bernard and Teddy Seidenfeld and Marco Zaffalon},
  LOCATION  = {Waterloo, Ontario, Canada},
  PUBLISHER = {Carleton Scientific},
  SERIES    = {Proceedings in Informatics},
  TITLE     = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
  VENUE     = {Lugano, Switzerland},
  VOLUME    = {18},
  YEAR      = {2003},
}

@BOOK{Protein-Atlas-1978,
  EDITOR = {M. O. Dayhoff},
  TITLE  = {Atlas of Protein Sequence and Structure},
  YEAR   = {1978},
}

@PROCEEDINGS{ISIPTA-1999,
  BOOKTITLE = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
  EDITOR    = {Gert de Cooman and Fabio Gagliardi Cozman and Serafín Moral and Peter Walley},
  TITLE     = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise Probabilities and Their Applications},
  VENUE     = {Ghent, Belgium},
  YEAR      = {1999},
}

@PROCEEDINGS{ISIPTA-2001,
  BOOKTITLE = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
  EDITOR    = {Gert de Cooman and Terrence L. Fine and Teddy Seidenfeld},
  LOCATION  = {Maastricht, the Netherlands},
  PUBLISHER = {Shaker Publishing},
  TITLE     = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
  VENUE     = {Ithaca, New York},
  YEAR      = {2001},
}

@PROCEEDINGS{ISIPTA-2007,
  EDITOR       = {Gert de Cooman and Jiřina Vejnarová and Marco Zaffalon},
  LOCALFILE    = {proceedings/ISIPTA-2007.pdf},
  ORGANIZATION = {SIPTA},
  PUBLISHER    = {Action M Agency for SIPTA},
  TITLE        = {ISIPTA '07: Proceedings of the Fifth International Symposium on Imprecise Probabilities: Theories and Applications},
  URL          = {http://www.sipta.org/isipta07/proceedings/proceedings-optimised.pdf},
  VENUE        = {Prague, Czech Republic},
  YEAR         = {2007},
}

@BOOK{Floudas-Pardalos-2009,
  EDITION   = {2},
  EDITOR    = {Christodoulos A. Floudas and Panos M. Pardalos},
  PUBLISHER = {Springer},
  SERIES    = {Springer Reference},
  TITLE     = {Encyclopedia of Optimization},
  YEAR      = {2009},
}

@BOOK{Handbook-Beta-2003,
  EDITOR    = {A. K. Gupta and S. Nadarajah},
  PUBLISHER = {Marcel Dekker},
  TITLE     = {Handbook of the Beta Distribution and its Applications},
  YEAR      = {2003},
}

@BOOK{Acta-Numerica-2004,
  EDITOR    = {A. Iserles},
  PUBLISHER = {Cambridge University Press},
  TITLE     = {Acta Numerica 2004},
  YEAR      = {2004},
}

@PROCEEDINGS{Cowles-1951,
  BOOKTITLE    = {Activity analysis of production and allocation},
  EDITOR       = {Tjalling C. Koopmans},
  NUMBER       = {13},
  ORGANIZATION = {Cowles Commission for Research in Economics},
  SERIES       = {Cowles Commission Monographs},
  TITLE        = {Activity analysis of production and allocation},
  YEAR         = {1951},
}

@PROCEEDINGS{SMPS-2004,
  BOOKTITLE = {Soft Methodology and Random Information Systems},
  EDITOR    = {Miguel Lopéz-Díaz and María Ángeles Gil and Przemysław Grzegorzewski and Olgierd Hryniewicz and Jonathan Lawry},
  PUBLISHER = {Springer},
  SERIES    = {Advances in soft computing},
  TITLE     = {Soft Methodology and Random Information Systems},
  VENUE     = {Oviedo, Spain},
  YEAR      = {2004},
}

@BOOK{Michie-Spiegelhalter-Taylor-1994,
  EDITOR = {D. Michie and D. J. Spiegelhalter and C. C. Taylor},
  TITLE  = {Machine Learning, Neural and Statistical Classification},
  URL    = {http://www.amsta.leeds.ac.uk/~charles/statlog},
  YEAR   = {1994},
}

@BOOK{Munro-1999,
  EDITOR    = {N. Munro},
  PUBLISHER = {The Institution of Electrical Engineers (IEE)},
  TITLE     = {The Use of Symbolic Methods in Control System Analysis and Design},
  YEAR      = {1999},
}

@BOOK{Pourret-Naim-Marcot-2008,
  EDITOR    = {Olivier Pourret and Patrick Naïm and Bruce Marcot},
  ISBN      = {978-0-470-06030-8},
  PUBLISHER = {Wiley},
  TITLE     = {Bayesian networks: a practical guide to applications},
  YEAR      = {2008},
}

@PROCEEDINGS{FMMES-1974,
  BOOKTITLE = {Formal methods in the methodology of empirical sciences},
  EDITOR    = {Marian Przełęcki and Klemens Szaniawski},
  LOCATION  = {Warsaw},
  PUBLISHER = {D. Reidel Publishing Company, Dordrecht, Holland / Boston, U.S.A; Ossolineum Publishing company, Wrocław, Poland},
  TITLE     = {Proceedings of the conference for formal methods in the methodology of empirical sciences},
  YEAR      = {1974},
}

@PROCEEDINGS{NIPS2003,
  BOOKTITLE = {NIPS},
  EDITOR    = {Sebastian Thrun and Lawrence K. Saul and Bernhard Schölkopf},
  ISBN      = {0-262-20152-6},
  PUBLISHER = {MIT Press},
  TITLE     = {Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British Columbia, Canada]},
  YEAR      = {2004},
}

@PROCEEDINGS{AI-2004,
  BOOKTITLE = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
  EDITOR    = {Geoffrey I. Webb and Xinghuo Yu},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in AI},
  TITLE     = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
  YEAR      = {2004},
}

@MISC{AUAI,
  SHORTTITLE = {AUAI},
  SORTTITLE  = {AUAI},
  TITLE      = {Association for Uncertainty in Artificial Intelligence},
  URL        = {http://www.auai.org},
}

@MISC{BNatWork,
  SHORTTITLE = {BN@Work},
  SORTTITLE  = {BN@Work},
  TITLE      = {European Community for Researchers on Probabilistic Graphical Models},
  URL        = {http://www.bnatwork.org},
}

@MISC{ECCAI,
  SHORTTITLE = {ECCAI},
  SORTTITLE  = {ECCAI},
  TITLE      = {European Coordinating Committee for Artificial Intelligence},
  URL        = {http://www.eccai.org},
}

@MISC{IPGatIDSIA-website,
  SHORTTITLE = {IPG@IDSIA},
  SORTTITLE  = {IPG@IDSIA},
  TITLE      = {Imprecise probability group at IDSIA},
  URL        = {http://ipg.idsia.ch/home.php},
}

@MISC{SIKS,
  SHORTTITLE = {SIKS},
  SORTTITLE  = {SIKS},
  TITLE      = {Netherlands research school for Information and Knowledge Systems},
  URL        = {http://www.siks.nl},
}

@MISC{SIPTA,
  SHORTTITLE = {SIPTA},
  SORTTITLE  = {SIPTA},
  TITLE      = {Society for Imprecise Probability: Theories and Applications},
  URL        = {http://www.sipta.org},
}

@PROCEEDINGS{IPMU-2004,
  BOOKTITLE = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
  TITLE     = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
  YEAR      = {2004},
}

@PROCEEDINGS{ICML-2002,
  BOOKTITLE = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
  TITLE     = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
  YEAR      = {2002},
}

@PROCEEDINGS{WWW-2002,
  BOOKTITLE = {Proceedings of the Eleventh International World Wide Web Conference},
  TITLE     = {Proceedings of the Eleventh International World Wide Web Conference},
  YEAR      = {2002},
}

@PROCEEDINGS{ICML-2000,
  BOOKTITLE = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
  TITLE     = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
  YEAR      = {2000},
}

@PROCEEDINGS{MTNS-2000,
  BOOKTITLE = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
  TITLE     = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
  YEAR      = {2000},
}

@PROCEEDINGS{WWW-2000,
  BOOKTITLE = {Proceedings of the Ninth International World Wide Web Conference},
  TITLE     = {Proceedings of the Ninth International World Wide Web Conference},
  YEAR      = {2000},
}
